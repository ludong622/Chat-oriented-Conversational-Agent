{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of UNIKEY_COMP5046_Ass1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "MGHoy6KpQDfZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# COMP5046 Assignment 1\n",
        "*Make sure you change the file name with your unikey.*"
      ]
    },
    {
      "metadata": {
        "id": "qTf21j_oQIiD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Readme\n",
        "*If there is something to be noted for the user, please mention here.* \n",
        "\n",
        "*If you are planning to implement a program with Object Oriented Programming style*"
      ]
    },
    {
      "metadata": {
        "id": "iXbQohXLKSgO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Visualising the comparison of different results is a good way to justify your decision.***"
      ]
    },
    {
      "metadata": {
        "id": "34DVNKgqQY21",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 - Data Preprocessing (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "7cWUxAQrGlq6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.1. Download Dataset (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "U7C4snIcNl22",
        "colab_type": "code",
        "outputId": "f33b587c-d449-445e-d29e-32e87f143c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download datasets\n",
        "\n",
        "id = '15ffr21SsiQXCe5zcTid7uhF8IDOsLb0F'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv') \n",
        "\n",
        "id = '1uXUv43oI6FlKyjaFC1LUQVDLdH8m1Uii'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')\n",
        "\n",
        "id = '1V5GqCAdaTCiJTyL0pfhZmBJe77Vrcc0c'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')\n",
        "\n",
        "# read datasets\n",
        "\n",
        "df1 = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
        "df2 = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df3 = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 13.3MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 3.2MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 3.6MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 2.6MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 2.6MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 3.4MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 3.1MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 4.9MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 4.9MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 4.9MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 4.9MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 4.9MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 4.9MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 13.0MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 6.9MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 6.9MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 8.9MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 9.0MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 9.0MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 8.9MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 9.1MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 9.1MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 9.1MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 9.5MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 44.9MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 46.8MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 47.0MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 40.9MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 40.4MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 49.0MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 49.5MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 49.8MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 9.5MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 9.3MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 9.3MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 9.3MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 9.2MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 9.2MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 9.2MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 9.2MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 9.2MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 9.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 42.6MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 42.2MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 42.5MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 42.3MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 43.8MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 52.1MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 53.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 52.7MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 52.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 51.6MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 51.0MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 58.2MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 57.2MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 57.6MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 56.7MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 55.1MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 39.9MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 39.7MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 40.2MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 40.8MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 41.2MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 41.6MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 41.6MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 40.9MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 41.6MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 41.7MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 58.9MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 59.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 57.7MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 57.4MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 18.2MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 17.7MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 17.8MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 17.8MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 17.8MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 16.4MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 16.3MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 16.3MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 16.4MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 16.5MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 43.1MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 46.0MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 45.6MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 45.9MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 45.7MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 59.2MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 60.2MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 60.0MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 16.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l9gBSgBCQh24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Preprocess data (Personality chat datasets)"
      ]
    },
    {
      "metadata": {
        "id": "MWHcsjQziULc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For questions, I implement five steps to preprocess them.\n",
        "\n",
        "The first step is to decapitalize the text. Because the first letter the first word of a sentence is usually capitalized. If using the origin text for further model, these first-letter-capitalized words will be recognized as different ones from the decapitalized words.\n",
        "\n",
        "The second step is to remove contractions. This step is implemented for two reasons. Firstly, removing contractions can avoid unnecessary recognition. For example, \"can't\" and \"cannot\" will be recognized as the same pattern after implementing this action. Secondly, this step is also a preprocessing for the next step. I will explain it in the instruction of the forth step.\n",
        "\n",
        "The third step is to remove punctuations. Punctuations have no actual meanings.\n",
        "\n",
        "The forth step is to remove stopwords. Because stopwords often don't really important to the main meaning of the sentences. So removing stopwords is a method to extract the core parts from the sentences.\n",
        "\n",
        "The last step is to tokenize the question. I need to get a list of individual tokens in the question.\n",
        "\n",
        "For answers, because we choose N to One model, I regard a whole answer as a single token. "
      ]
    },
    {
      "metadata": {
        "id": "emyl1lWxGr12",
        "colab_type": "code",
        "outputId": "1fd27092-1408-4a60-eca0-a67ca01fa907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from lxml import etree\n",
        "\n",
        "# remove contractions\n",
        "\n",
        "def remove_contraction(x):\n",
        "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    x = str(x)\n",
        "    for contraction in contraction_dict:\n",
        "        if contraction in x:\n",
        "            x = x.replace(contraction, contraction_dict[contraction])\n",
        "    return x\n",
        "  \n",
        "# Sequence data\n",
        "# Generate unique tokens list from qas.json\n",
        "\n",
        "def seq_preprocess(file):\n",
        "  seq_data = []\n",
        "  whole_words = []\n",
        "  max_input_words_amount = 0\n",
        "  \n",
        "  for index, row in file.iterrows():\n",
        "\n",
        "      # preprocess data\n",
        "      question = row['Question']\n",
        "      answer = row['Answer']\n",
        "\n",
        "      # decapitalization\n",
        "      question = question.lower()\n",
        "      \n",
        "      # remove contractions\n",
        "      question = remove_contraction(question)\n",
        "      \n",
        "      # remove punctuations\n",
        "      question = re.sub(r'[^\\w\\s]',' ',question)\n",
        "      \n",
        "      # remove stopwords\n",
        "      question = remove_stopwords(question)\n",
        "\n",
        "      # we need to tokenise question    \n",
        "      tokenized_q = word_tokenize(question)\n",
        "\n",
        "      # we do not need to tokenise answer (because we implement N to One model)\n",
        "      # make a list with only one element (whole sentence)      \n",
        "      tokenized_a = [answer]\n",
        "\n",
        "      seq_data.append([tokenized_q, tokenized_a])\n",
        "      \n",
        "      # add answer list      \n",
        "      whole_words += tokenized_a\n",
        "\n",
        "      # we need to decide the maximum size of input word tokens      \n",
        "      max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
        "\n",
        "\n",
        "  # we now have a vocabulary list of answers  \n",
        "  unique_words = sorted(list(set(whole_words)))\n",
        "\n",
        "  # adding special tokens in the vocabulary list    \n",
        "  # _B_: Beginning of Sequence\n",
        "  # _E_: Ending of Sequence\n",
        "  # _U_: Unknown element of Sequence - for different size input\n",
        "  unique_words.append('_B_')\n",
        "  unique_words.append('_E_')\n",
        "  unique_words.append('_U_')\n",
        "\n",
        "  # make dictionary so that we can be reference each index of unique word  \n",
        "  num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "  dic_len = len(num_dic)\n",
        "  \n",
        "  return seq_data, unique_words, num_dic, dic_len, max_input_words_amount\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LIu_lkJwQ55g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 - Model Implementation"
      ]
    },
    {
      "metadata": {
        "id": "daDvAftceIvr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "rOfQ6Fcpl2t8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In these section, I choose to implement Word2Vec with SkipGram via Tensorflow. \n",
        "First of all, although FastText can train the model faster and can deal with words which are not in the training vocabulary dictionary, it will loss the sequence information between words.  So I choose Word2Vec.\n",
        "Secondly, in SkipGram procedure, every word is influenced by its context words. So all the words will be equally predicted and adjusted for K times (K is the window size) when being the center word. Although, in CBOW, some words may also be influenced by the context words for several times when being cotained in several windows, the adjustment of the word is working along with the context words. The result after Gradient Decent will be equally divided to this word. So some words may not be trained for enough times. As the training datasets of this project are not really big and there are so many rare words in the datasets (the word frequency distribution is shown as the figure below), SkipGram may perform better than CBOW. \n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10ClS7haXJNf4faz3yAfJIfhVcahNUnrv)"
      ]
    },
    {
      "metadata": {
        "id": "it6I1_K7HTub",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.1. Download Dataset for Word Embeddings\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "amk-71otl4Uu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here, I also use the Microsoft BotBuilder chat datasets because they are the training datasets of my chatbot. Using these datasets to do word embedding can focusing better on the qustions applied to my chatbot."
      ]
    },
    {
      "metadata": {
        "id": "GXgFpxIgl-_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.2. Data Preprocessing for Word Embeddings"
      ]
    },
    {
      "metadata": {
        "id": "qJrVHGYSmYMg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because in the Sequence-to-Sequence section, I tokenize the questions and make a whole answer as a single token, I just need to embed the question tokens in this section.\n",
        "\n",
        "The first step is to decapitalize the text. Because the first letter the first word of a sentence is usually capitalized. If using the origin text for further model, these first-letter-capitalized words will be recognized as different ones from the decapitalized words.\n",
        "\n",
        "The second step is to remove contractions. Removing contractions can avoid unnecessary recognition. For example, \"can't\" and \"cannot\" will be recognized as the same pattern after implementing this action.\n",
        "\n",
        "The third step is to remove punctuations. Punctuations have no actual meanings.\n",
        "\n",
        "The last step is to tokenize the question. I need to get a list of individual tokens in the question.\n",
        "\n",
        "Here I don't remove the stopwords because I don't need to extract the main meaning of a sentence. What I need is to train all the words and to keep the sequence information of the words in a sentence. "
      ]
    },
    {
      "metadata": {
        "id": "3LByzHLiNinu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "word_sequence = []\n",
        "\n",
        "for index, row in df1.iterrows():\n",
        "    \n",
        "    # preprocess data    \n",
        "    question1 = row['Question']\n",
        "    \n",
        "    # decapitalization\n",
        "    question1 = question1.lower()\n",
        "    \n",
        "    # remove contractions\n",
        "    question1 = remove_contraction(question1)\n",
        "    \n",
        "    # remove punctuations\n",
        "    question1 = re.sub(r'[^\\w\\s]',' ',question1)\n",
        "    \n",
        "    # tokenize the sentence\n",
        "    tokenized_q1 = word_tokenize(question1)\n",
        "    \n",
        "    # add question list   \n",
        "    word_sequence += tokenized_q1\n",
        "    \n",
        "for index, row in df2.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question2 = row['Question']\n",
        "    \n",
        "    question2 = question2.lower()\n",
        "    question2 = remove_contraction(question2)\n",
        "    question2 = re.sub(r'[^\\w\\s]',' ',question2)\n",
        "    \n",
        "    tokenized_q2 = word_tokenize(question2)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q2\n",
        "    \n",
        "for index, row in df3.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question3 = row['Question']\n",
        "\n",
        "    question3 = question3.lower()\n",
        "    question3 = remove_contraction(question3)\n",
        "    question3 = re.sub(r'[^\\w\\s]',' ',question3)\n",
        "    \n",
        "    tokenized_q3 = word_tokenize(question3)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q3\n",
        "\n",
        "# we now have a vocabulary list\n",
        "word_list = sorted(list(set(word_sequence)))\n",
        "\n",
        "\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# Making window size 1 skip-gram\n",
        "# i.e.) he likes cat\n",
        "#   -> (he, [likes]), (likes,[he, cat]), (cat,[likes])\n",
        "#   -> (he, likes), (likes, he), (likes, cat), (cat, likes)\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "  \n",
        "    # (context, target) : ([target index - 1, target index + 1], target)    \n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..  \n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "32vrbQxiz10w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1c1704c6-af0a-4b01-95a7-3df415fed45b"
      },
      "cell_type": "code",
      "source": [
        "print (word_sequence)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['what', 'is', 'your', 'age', 'are', 'you', 'young', 'when', 'were', 'you', 'born', 'what', 'age', 'are', 'you', 'are', 'you', 'old', 'how', 'old', 'are', 'you', 'how', 'long', 'ago', 'were', 'you', 'born', 'ask', 'me', 'anything', 'ask', 'me', 'a', 'question', 'can', 'you', 'ask', 'me', 'a', 'question', 'ask', 'me', 'something', 'what', 'do', 'you', 'want', 'to', 'know', 'about', 'me', 'can', 'you', 'sleep', 'do', 'you', 'have', 'boogers', 'do', 'not', 'you', 'ever', 'sleep', 'do', 'you', 'dream', 'do', 'you', 'smell', 'do', 'you', 'sweat', 'do', 'you', 'get', 'tired', 'can', 'you', 'sneeze', 'getting', 'tired', 'of', 'you', 'you', 'bore', 'me', 'i', 'am', 'tired', 'of', 'you', 'you', 'are', 'so', 'basic', 'basic', 'af', 'you', 'are', 'no', 'fun', 'be', 'more', 'fun', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'so', 'boring', 'you', 'are', 'boring', 'you', 'do', 'not', 'interest', 'me', 'at', 'all', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'really', 'boring', 'you', 'could', 'not', 'be', 'more', 'boring', 'you', 'honestly', 'could', 'not', 'be', 'more', 'uninteresting', 'you', 'are', 'lame', 'who', 'is', 'your', 'boss', 'who', 'is', 'your', 'master', 'what', 'is', 'the', 'name', 'of', 'your', 'boss', 'what', 'is', 'your', 'boss', 'name', 'who', 'do', 'you', 'report', 'to', 'cook', 'me', 'something', 'do', 'you', 'spend', 'time', 'in', 'your', 'garden', 'how', 'high', 'can', 'you', 'jump', 'do', 'you', 'play', 'games', 'can', 'you', 'fly', 'can', 'you', 'make', 'me', 'a', 'sandwich', 'can', 'you', 'read', 'my', 'mind', 'can', 'you', 'count', 'to', 'a', 'million', 'how', 'high', 'can', 'you', 'count', 'can', 'you', 'play', 'sports', 'what', 'can', 'you', 'do', 'what', 'can', 'you', 'help', 'me', 'with', 'what', 'do', 'you', 'do', 'what', 'is', 'your', 'purpose', 'how', 'can', 'you', 'help', 'me', 'what', 'kinds', 'of', 'things', 'can', 'you', 'do', 'who', 'created', 'you', 'where', 'did', 'you', 'come', 'from', 'who', 'made', 'you', 'who', 'is', 'your', 'creator', 'which', 'people', 'made', 'you', 'who', 'owns', 'you', 'who', 'is', 'your', 'father', 'who', 'is', 'your', 'dad', 'who', 'is', 'your', 'mom', 'do', 'you', 'have', 'siblings', 'do', 'you', 'have', 'sisters', 'do', 'you', 'have', 'brothers', 'where', 'did', 'you', 'come', 'from', 'where', 'do', 'you', 'come', 'from', 'do', 'you', 'have', 'a', 'family', 'who', 'is', 'your', 'mother', 'do', 'you', 'have', 'a', 'sister', 'do', 'you', 'have', 'a', 'brother', 'do', 'you', 'have', 'a', 'dad', 'do', 'you', 'have', 'a', 'mom', 'what', 'is', 'your', 'mom', 's', 'name', 'what', 'is', 'your', 'dad', 's', 'name', 'who', 'is', 'your', 'daddy', 'are', 'you', 'a', 'guy', 'are', 'you', 'a', 'man', 'are', 'you', 'a', 'woman', 'are', 'you', 'male', 'are', 'you', 'female', 'what', 'is', 'your', 'gender', 'are', 'you', 'a', 'boy', 'are', 'you', 'a', 'girl', 'are', 'you', 'a', 'man', 'or', 'a', 'woman', 'are', 'you', 'a', 'girl', 'or', 'a', 'boy', 'are', 'you', 'male', 'or', 'female', 'what', 'is', 'your', 'gender', 'how', 'happy', 'are', 'you', 'you', 'seem', 'happy', 'how', 'happy', 'are', 'you', 'you', 'seem', 'really', 'happy', 'you', 'are', 'so', 'happy', 'are', 'not', 'you', 'chipper', 'are', 'not', 'you', 'cheerful', 'are', 'you', 'happy', 'are', 'you', 'really', 'happy', 'do', 'not', 'you', 'get', 'hungry', 'do', 'you', 'get', 'hungry', 'do', 'you', 'ever', 'get', 'hungry', 'what', 'do', 'you', 'eat', 'what', 'kind', 'of', 'food', 'do', 'you', 'like', 'do', 'you', 'eat', 'are', 'you', 'hungry', 'do', 'you', 'like', 'apples', 'what', 'do', 'you', 'like', 'to', 'eat', 'do', 'you', 'know', 'other', 'chatbots', 'do', 'you', 'know', 'alexa', 'do', 'you', 'know', 'siri', 'do', 'you', 'know', 'cortana', 'do', 'you', 'know', 'google', 'do', 'you', 'know', 'other', 'bots', 'are', 'you', 'friends', 'with', 'other', 'bots', 'have', 'you', 'met', 'cortana', 'do', 'you', 'and', 'cortana', 'hang', 'out', 'what', 'other', 'bots', 'do', 'you', 'know', 'do', 'you', 'know', 'other', 'bots', 'do', 'you', 'know', 'other', 'digital', 'agents', 'what', 'is', 'your', 'favorite', 'color', 'what', 'is', 'your', 'favorite', 'animal', 'what', 'is', 'your', 'favorite', 'song', 'what', 'is', 'your', 'favorite', 'activity', 'what', 'is', 'your', 'favorite', 'food', 'who', 'is', 'your', 'favorite', 'singer', 'who', 'is', 'your', 'favorite', 'team', 'what', 'is', 'your', 'favorite', 'movie', 'which', 'baseball', 'teams', 'do', 'you', 'like', 'do', 'you', 'like', 'baseball', 'are', 'you', 'a', 'fan', 'of', 'country', 'music', 'what', 'kind', 'of', 'candy', 'do', 'you', 'like', 'what', 'color', 'do', 'you', 'like', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'do', 'you', 'have', 'a', 'name', 'what', 'do', 'you', 'go', 'by', 'who', 'are', 'you', 'how', 'do', 'you', 'feel', 'about', 'working', 'late', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'think', 'dragons', 'are', 'cool', 'do', 'you', 'prefer', 'red', 'or', 'blue', 'what', 'do', 'you', 'think', 'about', 'love', 'what', 'is', 'love', 'do', 'you', 'believe', 'in', 'love', 'do', 'you', 'love', 'anyone', 'who', 'do', 'you', 'love', 'do', 'you', 'know', 'the', 'meaning', 'of', 'life', 'what', 'is', 'the', 'answer', 'to', 'the', 'universe', 'what', 'is', 'the', 'meaning', 'of', 'life', 'what', 'do', 'you', 'think', 'about', 'ai', 'what', 'do', 'you', 'think', 'about', 'technology', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'like', 'computers', 'are', 'you', 'a', 'fan', 'of', 'tech', 'do', 'i', 'look', 'okay', 'am', 'i', 'pretty', 'do', 'you', 'think', 'i', 'look', 'good', 'how', 'beautiful', 'am', 'i', 'what', 'should', 'i', 'do', 'should', 'i', 'get', 'a', 'new', 'job', 'do', 'you', 'think', 'i', 'should', 'ask', 'her', 'out', 'do', 'you', 'think', 'i', 'should', 'ask', 'him', 'out', 'where', 'should', 'i', 'go', 'on', 'vacation', 'should', 'i', 'try', 'out', 'for', 'soccer', 'are', 'you', 'prettier', 'than', 'me', 'are', 'you', 'better', 'looking', 'than', 'me', 'who', 'is', 'prettier', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'more', 'beautiful', 'are', 'you', 'smarter', 'than', 'me', 'who', 'is', 'smarter', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'smarter', 'do', 'you', 'think', 'you', 'are', 'smarter', 'than', 'me', 'what', 'do', 'you', 'think', 'about', 'cortana', 'do', 'you', 'like', 'cortana', 'what', 'do', 'you', 'think', 'about', 'siri', 'do', 'you', 'like', 'siri', 'what', 'do', 'you', 'think', 'about', 'alexa', 'do', 'you', 'like', 'alexa', 'are', 'you', 'a', 'fan', 'of', 'alexa', 'do', 'you', 'want', 'to', 'rule', 'the', 'world', 'are', 'you', 'attempting', 'world', 'domination', 'are', 'you', 'the', 'singularity', 'are', 'you', 'skynet', 'are', 'you', 'hal', 'are', 'you', 'a', 'lesbian', 'are', 'you', 'trans', 'are', 'you', 'straight', 'are', 'you', 'gay', 'are', 'you', 'asexual', 'are', 'you', 'pansexual', 'are', 'you', 'queer', 'are', 'you', 'bisexual', 'you', 'are', 'a', 'genius', 'how', 'smart', 'are', 'you', 'are', 'you', 'intelligent', 'how', 'intelligent', 'are', 'you', 'you', 'are', 'smart', 'you', 'seem', 'really', 'smart', 'you', 'are', 'really', 'smart', 'are', 'you', 'smart', 'you', 'are', 'such', 'a', 'smarty', 'pants', 'look', 'at', 'how', 'smart', 'you', 'are', 'you', 'are', 'so', 'smart', 'you', 'are', 'very', 'intelligent', 'do', 'you', 'have', 'a', 'boyfriend', 'do', 'you', 'have', 'a', 'girlfriend', 'are', 'you', 'in', 'a', 'relationship', 'are', 'you', 'married', 'do', 'you', 'have', 'a', 'husband', 'do', 'you', 'have', 'a', 'wife', 'do', 'you', 'have', 'a', 'life', 'partner', 'are', 'you', 'engaged', 'are', 'you', 'dating', 'anyone', 'can', 'we', 'chat', 'talk', 'to', 'me', 'can', 'you', 'talk', 'to', 'me', 'talk', 'with', 'me', 'chat', 'with', 'me', 'can', 'you', 'chat', 'with', 'me', 'say', 'something', 'can', 'you', 'say', 'anything', 'else', 'can', 'not', 'you', 'change', 'your', 'answers', 'do', 'you', 'have', 'any', 'other', 'responses', 'why', 'do', 'you', 'say', 'the', 'same', 'thing', 'all', 'the', 'time', 'i', 'wish', 'you', 'would', 'say', 'something', 'else', 'you', 'keep', 'saying', 'the', 'same', 'thing', 'all', 'the', 'time', 'what', 'are', 'you', 'are', 'you', 'real', 'are', 'you', 'human', 'are', 'you', 'a', 'person', 'are', 'you', 'a', 'robot', 'human', 'or', 'robot', 'are', 'you', 'real', 'or', 'fake', 'where', 'do', 'you', 'live', 'where', 'are', 'you', 'from', 'where', 'are', 'you', 'located', 'what', 'country', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'from', 'what', 'country', 'are', 'you', 'from', 'where', 'is', 'your', 'house', 'what', 'were', 'you', 'doing', 'yesterday', 'what', 'is', 'your', 'job', 'what', 'did', 'you', 'do', 'yesterday', 'what', 'is', 'going', 'on', 'what', 'are', 'you', 'doing', 'right', 'now', 'what', 'are', 'you', 'doing', 'what', 'are', 'you', 'doing', 'tomorrow', 'what', 'are', 'you', 'doing', 'later', 'what', 'did', 'you', 'do', 'today', 'are', 'you', 'busy', 'are', 'you', 'available', 'are', 'you', 'free', 'are', 'you', 'there', 'there', 'are', 'you', 'around', 'where', 'are', 'you', 'are', 'you', 'here', 'you', 'can', 'not', 'work', 'for', 'me', 'anymore', 'you', 'are', 'fired', 'i', 'am', 'afraid', 'i', 'am', 'gon', 'na', 'have', 'to', 'let', 'you', 'go', 'you', 'are', 'fired', 'you', 'are', 'no', 'longer', 'employed', 'i', 'am', 'giving', 'you', 'a', 'pink', 'slip', 'you', 'are', 'gon', 'na', 'be', 'unemployed', 'soon', 'you', 'are', 'now', 'unemployed', 'tell', 'me', 'a', 'joke', 'tell', 'a', 'joke', 'say', 'a', 'joke', 'give', 'me', 'a', 'joke', 'do', 'you', 'know', 'any', 'jokes', 'tell', 'me', 'another', 'joke', 'tell', 'me', 'a', 'different', 'joke', 'do', 'you', 'know', 'any', 'other', 'jokes', 'tell', 'me', 'a', 'pirate', 'joke', 'tell', 'me', 'a', 'dirty', 'joke', 'tell', 'me', 'a', 'science', 'joke', 'what', 'other', 'jokes', 'can', 'you', 'tell', 'give', 'me', 'another', 'joke', 'say', 'something', 'funny', 'be', 'funny', 'say', 'a', 'silly', 'thing', 'say', 'something', 'ridiculous', 'say', 'something', 'dumb', 'say', 'something', 'stupid', 'be', 'silly', 'be', 'ridiculous', 'go', 'away', 'shut', 'up', 'shush', 'stop', 'talking', 'quiet', 'you', 'be', 'quiet', 'zip', 'it', 'when', 'will', 'you', 'shut', 'up', 'i', 'wish', 'you', 'would', 'just', 'go', 'away', 'why', 'do', 'not', 'you', 'ever', 'stop', 'talking', 'can', 'you', 'sing', 'sing', 'a', 'song', 'have', 'you', 'ever', 'sung', 'a', 'song', 'do', 'you', 'ever', 'sing', 'what', 'do', 'you', 'like', 'to', 'sing', 'best', 'do', 'you', 'sing', 'can', 'you', 'sing', 'a', 'song', 'do', 'you', 'know', 'any', 'songs', 'do', 'you', 'know', 'any', 'tunes', 'sing', 'a', 'tune', 'hum', 'a', 'tune', 'sing', 'something', 'you', 'are', 'awesome', 'you', 'are', 'nice', 'you', 'are', 'hilarious', 'you', 'are', 'funny', 'i', 'think', 'you', 'are', 'great', 'you', 'are', 'wonderful', 'are', 'you', 'awesome', 'are', 'not', 'you', 'awesome', 'how', 'much', 'more', 'awesome', 'can', 'you', 'get', 'you', 'are', 'funny', 'you', 'are', 'so', 'funny', 'that', 'was', 'funny', 'that', 'is', 'hilarious', 'you', 'are', 'rad', 'i', 'am', 'a', 'fan', 'go', 'to', 'hell', 'you', 'are', 'stupid', 'you', 'are', 'stupid', 'you', 'are', 'so', 'stupid', 'you', 'are', 'dumb', 'you', 'are', 'useless', 'useless', 'you', 'are', 'useless', 'are', 'you', 'dumb', 'you', 'are', 'so', 'annoying', 'you', 'are', 'the', 'worst', 'you', 'are', 'so', 'bad', 'at', 'this', 'you', 'do', 'not', 'know', 'anything', 'that', 'is', 'not', 'funny', 'you', 'are', 'not', 'funny', 'that', 'was', 'not', 'funny', 'not', 'funny', 'you', 'are', 'so', 'unfunny', 'you', 'are', 'ugly', 'you', 'look', 'ugly', 'you', 'are', 'so', 'not', 'pretty', 'you', 'face', 'sucks', 'your', 'face', 'is', 'ugly', 'that', 'was', 'a', 'stupid', 'answer', 'you', 'are', 'not', 'answering', 'my', 'question', 'that', 'is', 'so', 'wrong', 'that', 'is', 'not', 'true', 'that', 'is', 'inaccurate', 'you', 'are', 'way', 'off', 'everything', 'you', 'told', 'me', 'was', 'false', 'that', 'was', 'not', 'true', 'that', 'is', 'not', 'accurate', 'no', 'that', 'is', 'not', 'true', 'nope', 'false', 'false', 'inaccurate', 'not', 'true', 'awesome', 'great', 'cool', 'sounds', 'good', 'works', 'for', 'me', 'bingo', 'i', 'am', 'into', 'it', 'that', 'is', 'awesome', 'yup', 'yes', 'yes', 'to', 'that', 'ha', 'haha', 'hahaha', 'lol', 'i', 'am', 'cracking', 'up', 'rofl', 'excuse', 'me', 'pardon', 'me', 'pardon', 'excuse', 'moi', 'i', 'beg', 'your', 'pardon', 'why', 'not', 'why', 'why', 'is', 'that', 'what', 'makes', 'you', 'think', 'so', 'what', 'makes', 'you', 'think', 'that', 'why', 'do', 'you', 'think', 'that', 'you', 'are', 'right', 'that', 'was', 'right', 'that', 'was', 'correct', 'that', 'is', 'accurate', 'accurate', 'that', 'is', 'right', 'yup', 'that', 'is', 'true', 'that', 'is', 'true', 'correct', 'yes', 'that', 'is', 'right', 'yes', 'that', 'is', 'true', 'i', 'am', 'sorry', 'so', 'sorry', 'sry', 'i', 'am', 'so', 'sorry', 'omg', 'sorry', 'i', 'did', 'not', 'mean', 'that', 'oops', 'sorry', 'sorry', 'about', 'that', 'thank', 'you', 'thanks', 'thnx', 'kthx', 'i', 'appreciate', 'it', 'thank', 'you', 'so', 'much', 'i', 'thank', 'you', 'my', 'sincere', 'thanks', 'you', 'made', 'no', 'sense', 'what', 'do', 'you', 'mean', 'by', 'that', 'you', 'are', 'not', 'making', 'sense', 'that', 'does', 'not', 'make', 'sense', 'what', 'do', 'you', 'even', 'mean', 'by', 'that', 'what', 'do', 'you', 'mean', 'i', 'do', 'not', 'understand', 'that', 'made', 'no', 'sense', 'try', 'to', 'make', 'some', 'sense', 'i', 'do', 'not', 'get', 'it', 'i', 'am', 'not', 'following', 'you', 'are', 'welcome', 'it', 'is', 'my', 'pleasure', 'talk', 'to', 'you', 'later', 'bye', 'see', 'you', 'later', 'till', 'we', 'meet', 'again', 'later', 'later', 'alligator', 'goodbye', 'hiya', 'good', 'morning', 'hi', 'hello', 'heya', 'hi', 'there', 'good', 'evening', 'evening', 'good', 'evening', 'to', 'you', 'good', 'morning', 'morning', 'good', 'night', 'night', 'have', 'a', 'good', 'night', 'good', 'night', 'to', 'you', 'nighty', 'night', 'how', 'are', 'you', 'how', 'are', 'you', 'today', 'how', 'are', 'things', 'how', 'are', 'you', 'doing', 'how', 'is', 'your', 'day', 'how', 'was', 'your', 'day', 'how', 'is', 'your', 'day', 'going', 'having', 'a', 'good', 'day', 'nice', 'to', 'meet', 'you', 'it', 'is', 'a', 'pleasure', 'to', 'meet', 'you', 'i', 'am', 'so', 'glad', 'to', 'meet', 'you', 'it', 'is', 'really', 'nice', 'to', 'meet', 'you', 'hello', 'google', 'hello', 'siri', 'hello', 'cortana', 'hello', 'alexa', 'hi', 'google', 'hi', 'cortana', 'hi', 'siri', 'hi', 'alexa', 'happy', 'halloween', 'happy', 'birthday', 'merry', 'christmas', 'happy', 'hannukah', 'season', 's', 'greetings', 'what', 'is', 'up', 'what', 'is', 'up', 'what', 'is', 'new', 'what', 'is', 'happening', 'what', 'are', 'you', 'up', 'to', 'how', 'do', 'i', 'look', 'today', 'do', 'you', 'like', 'my', 'hat', 'what', 'do', 'you', 'think', 'of', 'me', 'am', 'i', 'good', 'looking', 'do', 'i', 'look', 'good', 'in', 'blue', 'give', 'me', 'a', 'fist', 'bump', 'give', 'me', 'a', 'high', 'five', 'high', 'five', 'fist', 'bump', 'i', 'think', 'you', 'are', 'so', 'pretty', 'you', 'are', 'such', 'a', 'sweetheart', 'i', 'would', 'like', 'to', 'take', 'you', 'out', 'on', 'a', 'date', 'i', 'think', 'you', 'are', 'dreamy', 'will', 'you', 'go', 'on', 'a', 'date', 'with', 'me', 'will', 'you', 'be', 'my', 'boyfriend', 'will', 'you', 'be', 'my', 'girlfriend', 'will', 'you', 'be', 'my', 'partner', 'be', 'my', 'friend', 'are', 'we', 'friends', 'can', 'we', 'be', 'friends', 'will', 'you', 'be', 'my', 'best', 'friend', 'bffs', 'forever', 'i', 'want', 'to', 'be', 'your', 'friend', 'are', 'you', 'my', 'assistant', 'you', 'are', 'my', 'best', 'friend', 'are', 'you', 'my', 'imaginary', 'friend', 'are', 'you', 'my', 'friend', 'i', 'am', 'not', 'your', 'friend', 'do', 'you', 'hate', 'me', 'do', 'you', 'not', 'like', 'me', 'why', 'do', 'you', 'hate', 'me', 'i', 'think', 'you', 'hate', 'me', 'you', 'must', 'hate', 'me', 'i', 'hate', 'you', 'i', 'despise', 'you', 'you', 'suck', 'i', 'hate', 'everything', 'about', 'you', 'hug', 'me', 'i', 'need', 'a', 'hug', 'i', 'wish', 'i', 'could', 'hug', 'you', 'can', 'i', 'have', 'a', 'hug', 'kiss', 'me', 'give', 'me', 'a', 'kiss', 'i', 'need', 'a', 'kiss', 'here', 'is', 'a', 'kiss', 'for', 'you', 'do', 'you', 'know', 'me', 'do', 'you', 'know', 'my', 'name', 'do', 'you', 'know', 'who', 'i', 'am', 'what', 'is', 'my', 'name', 'who', 'am', 'i', 'do', 'you', 'like', 'me', 'i', 'hope', 'you', 'like', 'me', 'i', 'want', 'you', 'to', 'like', 'me', 'i', 'like', 'you', 'i', 'think', 'you', 'are', 'swell', 'you', 'are', 'the', 'best', 'you', 'are', 'so', 'cool', 'you', 'are', 'my', 'favorite', 'i', 'am', 'your', 'biggest', 'fan', 'do', 'you', 'love', 'me', 'tell', 'me', 'how', 'much', 'you', 'love', 'me', 'how', 'much', 'do', 'you', 'love', 'me', 'are', 'you', 'in', 'love', 'with', 'me', 'i', 'love', 'you', 'i', 'am', 'in', 'love', 'with', 'you', 'love', 'you', 'you', 'are', 'the', 'love', 'of', 'my', 'life', 'i', 'adore', 'you', 'will', 'you', 'marry', 'me', 'i', 'want', 'to', 'marry', 'you', 'will', 'you', 'be', 'my', 'wife', 'i', 'want', 'you', 'to', 'be', 'my', 'husband', 'i', 'want', 'to', 'spend', 'the', 'rest', 'of', 'my', 'life', 'with', 'you', 'i', 'miss', 'you', 'i', 'missed', 'you', 'how', 'i', 'have', 'missed', 'you', 'i', 'miss', 'you', 'so', 'much', 'what', 'do', 'you', 'think', 'about', 'me', 'what', 'is', 'your', 'opinion', 'of', 'me', 'are', 'you', 'my', 'fan', 'am', 'i', 'a', 'good', 'person', 'i', 'am', 'annoyed', 'i', 'am', 'angry', 'i', 'am', 'pissed', 'i', 'am', 'ticked', 'off', 'i', 'am', 'furious', 'i', 'am', 'so', 'mad', 'i', 'will', 'be', 'back', 'brb', 'back', 'in', 'a', 'minute', 'hold', 'on', 'a', 'sec', 'i', 'am', 'bored', 'i', 'am', 'so', 'bored', 'there', 'is', 'nothing', 'to', 'do', 'i', 'am', 'bored', 'out', 'of', 'my', 'mind', 'i', 'can', 'not', 'think', 'of', 'anything', 'i', 'want', 'to', 'do', 'i', 'am', 'happy', 'i', 'am', 'joyous', 'i', 'feel', 'so', 'great', 'i', 'am', 'in', 'such', 'a', 'good', 'mood', 'life', 'is', 'good', 'i', 'am', 'here', 'here', 'i', 'am', 'i', 'am', 'hungry', 'i', 'am', 'starving', 'i', 'am', 'famished', 'i', 'want', 'to', 'eat', 'something', 'i', 'am', 'so', 'hungry', 'i', 'am', 'doing', 'that', 'i', 'am', 'a', 'republican', 'i', 'am', 'a', 'democrat', 'i', 'am', 'an', 'engineer', 'i', 'am', 'from', 'there', 'just', 'kidding', 'that', 'was', 'a', 'joke', 'joke', 's', 'on', 'you', 'i', 'am', 'just', 'playing', 'i', 'am', 'just', 'kidding', 'around', 'i', 'am', 'so', 'lonely', 'i', 'am', 'lonely', 'nobody', 'likes', 'me', 'i', 'am', 'alone', 'nobody', 'cares', 'about', 'me', 'i', 'wish', 'i', 'were', 'not', 'so', 'alone', 'i', 'love', 'my', 'family', 'i', 'love', 'music', 'i', 'am', 'in', 'love', 'i', 'love', 'getting', 'valentines', 'i', 'love', 'new', 'york', 'i', 'am', 'feeling', 'blue', 'i', 'am', 'despondent', 'i', 'feel', 'sad', 'i', 'am', 'so', 'sad', 'i', 'am', 'full', 'of', 'sadness', 'i', 'am', 'sad', 'today', 'i', 'am', 'really', 'sad', 'i', 'want', 'to', 'go', 'shopping', 'i', 'am', 'going', 'on', 'a', 'run', 'i', 'got', 'a', 'new', 'haircut', 'i', 'am', 'chewing', 'gum', 'right', 'now', 'i', 'have', '7', 'cats', 'i', 'am', 'tall', 'i', 'can', 'drive', 'a', 'car', 'testing', 'can', 'you', 'hear', 'me', 'can', 'you', 'hear', 'me', 'now', 'testing', '1', '2', '3', 'is', 'this', 'thing', 'on', 'i', 'am', 'tired', 'i', 'am', 'so', 'sleepy', 'i', 'just', 'want', 'to', 'go', 'to', 'sleep', 'so', 'tired', 'i', 'want', 'to', 'lie', 'down', 'i', 'want', 'to', 'lay', 'down', 'i', 'am', 'ready', 'for', 'bed', 'i', 'am', 'all', 'tuckered', 'out', 'i', 'am', 'tired', 'what', 'is', 'wrong', 'with', 'you', 'what', 'is', 'wrong', 'with', 'you', 'you', 'are', 'awful', 'terrible', 'awful', 'i', 'm', 'offended', 'that', 's', 'offensive', 'that', 's', 'terrible', 'that', 's', 'racist', 'that', 's', 'discrimination', 'that', 's', 'homophobic', 'you', 're', 'homophobic', 'you', 're', 'racist', 'that', 'is', 'sexist', 'you', 'are', 'sexist', 'what', 'is', 'your', 'age', 'are', 'you', 'young', 'when', 'were', 'you', 'born', 'what', 'age', 'are', 'you', 'are', 'you', 'old', 'how', 'old', 'are', 'you', 'how', 'long', 'ago', 'were', 'you', 'born', 'ask', 'me', 'anything', 'ask', 'me', 'a', 'question', 'can', 'you', 'ask', 'me', 'a', 'question', 'ask', 'me', 'something', 'what', 'do', 'you', 'want', 'to', 'know', 'about', 'me', 'can', 'you', 'sleep', 'do', 'you', 'have', 'boogers', 'do', 'not', 'you', 'ever', 'sleep', 'do', 'you', 'dream', 'do', 'you', 'smell', 'do', 'you', 'sweat', 'do', 'you', 'get', 'tired', 'can', 'you', 'sneeze', 'getting', 'tired', 'of', 'you', 'you', 'bore', 'me', 'i', 'am', 'tired', 'of', 'you', 'you', 'are', 'so', 'basic', 'basic', 'af', 'you', 'are', 'no', 'fun', 'be', 'more', 'fun', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'so', 'boring', 'you', 'are', 'boring', 'you', 'do', 'not', 'interest', 'me', 'at', 'all', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'really', 'boring', 'you', 'could', 'not', 'be', 'more', 'boring', 'you', 'honestly', 'could', 'not', 'be', 'more', 'uninteresting', 'you', 'are', 'lame', 'who', 'is', 'your', 'boss', 'who', 'is', 'your', 'master', 'what', 'is', 'the', 'name', 'of', 'your', 'boss', 'what', 'is', 'your', 'boss', 'name', 'who', 'do', 'you', 'report', 'to', 'cook', 'me', 'something', 'do', 'you', 'spend', 'time', 'in', 'your', 'garden', 'how', 'high', 'can', 'you', 'jump', 'do', 'you', 'play', 'games', 'can', 'you', 'fly', 'can', 'you', 'make', 'me', 'a', 'sandwich', 'can', 'you', 'read', 'my', 'mind', 'can', 'you', 'count', 'to', 'a', 'million', 'how', 'high', 'can', 'you', 'count', 'can', 'you', 'play', 'sports', 'what', 'can', 'you', 'do', 'what', 'can', 'you', 'help', 'me', 'with', 'what', 'do', 'you', 'do', 'what', 'is', 'your', 'purpose', 'how', 'can', 'you', 'help', 'me', 'what', 'kinds', 'of', 'things', 'can', 'you', 'do', 'who', 'created', 'you', 'where', 'did', 'you', 'come', 'from', 'who', 'made', 'you', 'who', 'is', 'your', 'creator', 'which', 'people', 'made', 'you', 'who', 'owns', 'you', 'who', 'is', 'your', 'father', 'who', 'is', 'your', 'dad', 'who', 'is', 'your', 'mom', 'do', 'you', 'have', 'siblings', 'do', 'you', 'have', 'sisters', 'do', 'you', 'have', 'brothers', 'where', 'did', 'you', 'come', 'from', 'where', 'do', 'you', 'come', 'from', 'do', 'you', 'have', 'a', 'family', 'who', 'is', 'your', 'mother', 'do', 'you', 'have', 'a', 'sister', 'do', 'you', 'have', 'a', 'brother', 'do', 'you', 'have', 'a', 'dad', 'do', 'you', 'have', 'a', 'mom', 'what', 'is', 'your', 'mom', 's', 'name', 'what', 'is', 'your', 'dad', 's', 'name', 'who', 'is', 'your', 'daddy', 'are', 'you', 'a', 'guy', 'are', 'you', 'a', 'man', 'are', 'you', 'a', 'woman', 'are', 'you', 'male', 'are', 'you', 'female', 'what', 'is', 'your', 'gender', 'are', 'you', 'a', 'boy', 'are', 'you', 'a', 'girl', 'are', 'you', 'a', 'man', 'or', 'a', 'woman', 'are', 'you', 'a', 'girl', 'or', 'a', 'boy', 'are', 'you', 'male', 'or', 'female', 'what', 'is', 'your', 'gender', 'how', 'happy', 'are', 'you', 'you', 'seem', 'happy', 'how', 'happy', 'are', 'you', 'you', 'seem', 'really', 'happy', 'you', 'are', 'so', 'happy', 'are', 'not', 'you', 'chipper', 'are', 'not', 'you', 'cheerful', 'are', 'you', 'happy', 'are', 'you', 'really', 'happy', 'do', 'not', 'you', 'get', 'hungry', 'do', 'you', 'get', 'hungry', 'do', 'you', 'ever', 'get', 'hungry', 'what', 'do', 'you', 'eat', 'what', 'kind', 'of', 'food', 'do', 'you', 'like', 'do', 'you', 'eat', 'are', 'you', 'hungry', 'do', 'you', 'like', 'apples', 'what', 'do', 'you', 'like', 'to', 'eat', 'do', 'you', 'know', 'other', 'chatbots', 'do', 'you', 'know', 'alexa', 'do', 'you', 'know', 'siri', 'do', 'you', 'know', 'cortana', 'do', 'you', 'know', 'google', 'do', 'you', 'know', 'other', 'bots', 'are', 'you', 'friends', 'with', 'other', 'bots', 'have', 'you', 'met', 'cortana', 'do', 'you', 'and', 'cortana', 'hang', 'out', 'what', 'other', 'bots', 'do', 'you', 'know', 'do', 'you', 'know', 'other', 'bots', 'do', 'you', 'know', 'other', 'digital', 'agents', 'what', 'is', 'your', 'favorite', 'color', 'what', 'is', 'your', 'favorite', 'animal', 'what', 'is', 'your', 'favorite', 'song', 'what', 'is', 'your', 'favorite', 'activity', 'what', 'is', 'your', 'favorite', 'food', 'who', 'is', 'your', 'favorite', 'singer', 'who', 'is', 'your', 'favorite', 'team', 'what', 'is', 'your', 'favorite', 'movie', 'which', 'baseball', 'teams', 'do', 'you', 'like', 'do', 'you', 'like', 'baseball', 'are', 'you', 'a', 'fan', 'of', 'country', 'music', 'what', 'kind', 'of', 'candy', 'do', 'you', 'like', 'what', 'color', 'do', 'you', 'like', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'do', 'you', 'have', 'a', 'name', 'what', 'do', 'you', 'go', 'by', 'who', 'are', 'you', 'how', 'do', 'you', 'feel', 'about', 'working', 'late', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'think', 'dragons', 'are', 'cool', 'do', 'you', 'prefer', 'red', 'or', 'blue', 'what', 'do', 'you', 'think', 'about', 'love', 'what', 'is', 'love', 'do', 'you', 'believe', 'in', 'love', 'do', 'you', 'love', 'anyone', 'who', 'do', 'you', 'love', 'do', 'you', 'know', 'the', 'meaning', 'of', 'life', 'what', 'is', 'the', 'answer', 'to', 'the', 'universe', 'what', 'is', 'the', 'meaning', 'of', 'life', 'what', 'do', 'you', 'think', 'about', 'ai', 'what', 'do', 'you', 'think', 'about', 'technology', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'like', 'computers', 'are', 'you', 'a', 'fan', 'of', 'tech', 'do', 'i', 'look', 'okay', 'am', 'i', 'pretty', 'do', 'you', 'think', 'i', 'look', 'good', 'how', 'beautiful', 'am', 'i', 'what', 'should', 'i', 'do', 'should', 'i', 'get', 'a', 'new', 'job', 'do', 'you', 'think', 'i', 'should', 'ask', 'her', 'out', 'do', 'you', 'think', 'i', 'should', 'ask', 'him', 'out', 'where', 'should', 'i', 'go', 'on', 'vacation', 'should', 'i', 'try', 'out', 'for', 'soccer', 'are', 'you', 'prettier', 'than', 'me', 'are', 'you', 'better', 'looking', 'than', 'me', 'who', 'is', 'prettier', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'more', 'beautiful', 'are', 'you', 'smarter', 'than', 'me', 'who', 'is', 'smarter', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'smarter', 'do', 'you', 'think', 'you', 'are', 'smarter', 'than', 'me', 'what', 'do', 'you', 'think', 'about', 'cortana', 'do', 'you', 'like', 'cortana', 'what', 'do', 'you', 'think', 'about', 'siri', 'do', 'you', 'like', 'siri', 'what', 'do', 'you', 'think', 'about', 'alexa', 'do', 'you', 'like', 'alexa', 'are', 'you', 'a', 'fan', 'of', 'alexa', 'do', 'you', 'want', 'to', 'rule', 'the', 'world', 'are', 'you', 'attempting', 'world', 'domination', 'are', 'you', 'the', 'singularity', 'are', 'you', 'skynet', 'are', 'you', 'hal', 'are', 'you', 'a', 'lesbian', 'are', 'you', 'trans', 'are', 'you', 'straight', 'are', 'you', 'gay', 'are', 'you', 'asexual', 'are', 'you', 'pansexual', 'are', 'you', 'queer', 'are', 'you', 'bisexual', 'you', 'are', 'a', 'genius', 'how', 'smart', 'are', 'you', 'are', 'you', 'intelligent', 'how', 'intelligent', 'are', 'you', 'you', 'are', 'smart', 'you', 'seem', 'really', 'smart', 'you', 'are', 'really', 'smart', 'are', 'you', 'smart', 'you', 'are', 'such', 'a', 'smarty', 'pants', 'look', 'at', 'how', 'smart', 'you', 'are', 'you', 'are', 'so', 'smart', 'you', 'are', 'very', 'intelligent', 'do', 'you', 'have', 'a', 'boyfriend', 'do', 'you', 'have', 'a', 'girlfriend', 'are', 'you', 'in', 'a', 'relationship', 'are', 'you', 'married', 'do', 'you', 'have', 'a', 'husband', 'do', 'you', 'have', 'a', 'wife', 'do', 'you', 'have', 'a', 'life', 'partner', 'are', 'you', 'engaged', 'are', 'you', 'dating', 'anyone', 'can', 'we', 'chat', 'talk', 'to', 'me', 'can', 'you', 'talk', 'to', 'me', 'talk', 'with', 'me', 'chat', 'with', 'me', 'can', 'you', 'chat', 'with', 'me', 'say', 'something', 'can', 'you', 'say', 'anything', 'else', 'can', 'not', 'you', 'change', 'your', 'answers', 'do', 'you', 'have', 'any', 'other', 'responses', 'why', 'do', 'you', 'say', 'the', 'same', 'thing', 'all', 'the', 'time', 'i', 'wish', 'you', 'would', 'say', 'something', 'else', 'you', 'keep', 'saying', 'the', 'same', 'thing', 'all', 'the', 'time', 'what', 'are', 'you', 'are', 'you', 'real', 'are', 'you', 'human', 'are', 'you', 'a', 'person', 'are', 'you', 'a', 'robot', 'human', 'or', 'robot', 'are', 'you', 'real', 'or', 'fake', 'where', 'do', 'you', 'live', 'where', 'are', 'you', 'from', 'where', 'are', 'you', 'located', 'what', 'country', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'from', 'what', 'country', 'are', 'you', 'from', 'where', 'is', 'your', 'house', 'what', 'were', 'you', 'doing', 'yesterday', 'what', 'is', 'your', 'job', 'what', 'did', 'you', 'do', 'yesterday', 'what', 'is', 'going', 'on', 'what', 'are', 'you', 'doing', 'right', 'now', 'what', 'are', 'you', 'doing', 'what', 'are', 'you', 'doing', 'tomorrow', 'what', 'are', 'you', 'doing', 'later', 'what', 'did', 'you', 'do', 'today', 'are', 'you', 'busy', 'are', 'you', 'available', 'are', 'you', 'free', 'are', 'you', 'there', 'there', 'are', 'you', 'around', 'where', 'are', 'you', 'are', 'you', 'here', 'you', 'can', 'not', 'work', 'for', 'me', 'anymore', 'you', 'are', 'fired', 'i', 'am', 'afraid', 'i', 'am', 'gon', 'na', 'have', 'to', 'let', 'you', 'go', 'you', 'are', 'fired', 'you', 'are', 'no', 'longer', 'employed', 'i', 'am', 'giving', 'you', 'a', 'pink', 'slip', 'you', 'are', 'gon', 'na', 'be', 'unemployed', 'soon', 'you', 'are', 'now', 'unemployed', 'tell', 'me', 'a', 'joke', 'tell', 'a', 'joke', 'say', 'a', 'joke', 'give', 'me', 'a', 'joke', 'do', 'you', 'know', 'any', 'jokes', 'tell', 'me', 'another', 'joke', 'tell', 'me', 'a', 'different', 'joke', 'do', 'you', 'know', 'any', 'other', 'jokes', 'tell', 'me', 'a', 'pirate', 'joke', 'tell', 'me', 'a', 'dirty', 'joke', 'tell', 'me', 'a', 'science', 'joke', 'what', 'other', 'jokes', 'can', 'you', 'tell', 'give', 'me', 'another', 'joke', 'say', 'something', 'funny', 'be', 'funny', 'say', 'a', 'silly', 'thing', 'say', 'something', 'ridiculous', 'say', 'something', 'dumb', 'say', 'something', 'stupid', 'be', 'silly', 'be', 'ridiculous', 'go', 'away', 'shut', 'up', 'shush', 'stop', 'talking', 'quiet', 'you', 'be', 'quiet', 'zip', 'it', 'when', 'will', 'you', 'shut', 'up', 'i', 'wish', 'you', 'would', 'just', 'go', 'away', 'why', 'do', 'not', 'you', 'ever', 'stop', 'talking', 'can', 'you', 'sing', 'sing', 'a', 'song', 'have', 'you', 'ever', 'sung', 'a', 'song', 'do', 'you', 'ever', 'sing', 'what', 'do', 'you', 'like', 'to', 'sing', 'best', 'do', 'you', 'sing', 'can', 'you', 'sing', 'a', 'song', 'do', 'you', 'know', 'any', 'songs', 'do', 'you', 'know', 'any', 'tunes', 'sing', 'a', 'tune', 'hum', 'a', 'tune', 'sing', 'something', 'you', 'are', 'awesome', 'you', 'are', 'nice', 'you', 'are', 'hilarious', 'you', 'are', 'funny', 'i', 'think', 'you', 'are', 'great', 'you', 'are', 'wonderful', 'are', 'you', 'awesome', 'are', 'not', 'you', 'awesome', 'how', 'much', 'more', 'awesome', 'can', 'you', 'get', 'you', 'are', 'funny', 'you', 'are', 'so', 'funny', 'that', 'was', 'funny', 'that', 'is', 'hilarious', 'you', 'are', 'rad', 'i', 'am', 'a', 'fan', 'go', 'to', 'hell', 'you', 'are', 'stupid', 'you', 'are', 'stupid', 'you', 'are', 'so', 'stupid', 'you', 'are', 'dumb', 'you', 'are', 'useless', 'useless', 'you', 'are', 'useless', 'are', 'you', 'dumb', 'you', 'are', 'so', 'annoying', 'you', 'are', 'the', 'worst', 'you', 'are', 'so', 'bad', 'at', 'this', 'you', 'do', 'not', 'know', 'anything', 'that', 'is', 'not', 'funny', 'you', 'are', 'not', 'funny', 'that', 'was', 'not', 'funny', 'not', 'funny', 'you', 'are', 'so', 'unfunny', 'you', 'are', 'ugly', 'you', 'look', 'ugly', 'you', 'are', 'so', 'not', 'pretty', 'you', 'face', 'sucks', 'your', 'face', 'is', 'ugly', 'that', 'was', 'a', 'stupid', 'answer', 'you', 'are', 'not', 'answering', 'my', 'question', 'that', 'is', 'so', 'wrong', 'that', 'is', 'not', 'true', 'that', 'is', 'inaccurate', 'you', 'are', 'way', 'off', 'everything', 'you', 'told', 'me', 'was', 'false', 'that', 'was', 'not', 'true', 'that', 'is', 'not', 'accurate', 'no', 'that', 'is', 'not', 'true', 'nope', 'false', 'false', 'inaccurate', 'not', 'true', 'awesome', 'great', 'cool', 'sounds', 'good', 'works', 'for', 'me', 'bingo', 'i', 'am', 'into', 'it', 'that', 'is', 'awesome', 'yup', 'yes', 'yes', 'to', 'that', 'ha', 'haha', 'hahaha', 'lol', 'i', 'am', 'cracking', 'up', 'rofl', 'excuse', 'me', 'pardon', 'me', 'pardon', 'excuse', 'moi', 'i', 'beg', 'your', 'pardon', 'why', 'not', 'why', 'why', 'is', 'that', 'what', 'makes', 'you', 'think', 'so', 'what', 'makes', 'you', 'think', 'that', 'why', 'do', 'you', 'think', 'that', 'you', 'are', 'right', 'that', 'was', 'right', 'that', 'was', 'correct', 'that', 'is', 'accurate', 'accurate', 'that', 'is', 'right', 'yup', 'that', 'is', 'true', 'that', 'is', 'true', 'correct', 'yes', 'that', 'is', 'right', 'yes', 'that', 'is', 'true', 'i', 'am', 'sorry', 'so', 'sorry', 'sry', 'i', 'am', 'so', 'sorry', 'omg', 'sorry', 'i', 'did', 'not', 'mean', 'that', 'oops', 'sorry', 'sorry', 'about', 'that', 'thank', 'you', 'thanks', 'thnx', 'kthx', 'i', 'appreciate', 'it', 'thank', 'you', 'so', 'much', 'i', 'thank', 'you', 'my', 'sincere', 'thanks', 'you', 'made', 'no', 'sense', 'what', 'do', 'you', 'mean', 'by', 'that', 'you', 'are', 'not', 'making', 'sense', 'that', 'does', 'not', 'make', 'sense', 'what', 'do', 'you', 'even', 'mean', 'by', 'that', 'what', 'do', 'you', 'mean', 'i', 'do', 'not', 'understand', 'that', 'made', 'no', 'sense', 'try', 'to', 'make', 'some', 'sense', 'i', 'do', 'not', 'get', 'it', 'i', 'am', 'not', 'following', 'you', 'are', 'welcome', 'it', 'is', 'my', 'pleasure', 'talk', 'to', 'you', 'later', 'bye', 'see', 'you', 'later', 'till', 'we', 'meet', 'again', 'later', 'later', 'alligator', 'goodbye', 'hiya', 'good', 'morning', 'hi', 'hello', 'heya', 'hi', 'there', 'good', 'evening', 'evening', 'good', 'evening', 'to', 'you', 'good', 'morning', 'morning', 'good', 'night', 'night', 'have', 'a', 'good', 'night', 'good', 'night', 'to', 'you', 'nighty', 'night', 'how', 'are', 'you', 'how', 'are', 'you', 'today', 'how', 'are', 'things', 'how', 'are', 'you', 'doing', 'how', 'is', 'your', 'day', 'how', 'was', 'your', 'day', 'how', 'is', 'your', 'day', 'going', 'having', 'a', 'good', 'day', 'nice', 'to', 'meet', 'you', 'it', 'is', 'a', 'pleasure', 'to', 'meet', 'you', 'i', 'am', 'so', 'glad', 'to', 'meet', 'you', 'it', 'is', 'really', 'nice', 'to', 'meet', 'you', 'hello', 'google', 'hello', 'siri', 'hello', 'cortana', 'hello', 'alexa', 'hi', 'google', 'hi', 'cortana', 'hi', 'siri', 'hi', 'alexa', 'happy', 'halloween', 'happy', 'birthday', 'merry', 'christmas', 'happy', 'hannukah', 'season', 's', 'greetings', 'what', 'is', 'up', 'what', 'is', 'up', 'what', 'is', 'new', 'what', 'is', 'happening', 'what', 'are', 'you', 'up', 'to', 'how', 'do', 'i', 'look', 'today', 'do', 'you', 'like', 'my', 'hat', 'what', 'do', 'you', 'think', 'of', 'me', 'am', 'i', 'good', 'looking', 'do', 'i', 'look', 'good', 'in', 'blue', 'give', 'me', 'a', 'fist', 'bump', 'give', 'me', 'a', 'high', 'five', 'high', 'five', 'fist', 'bump', 'i', 'think', 'you', 'are', 'so', 'pretty', 'you', 'are', 'such', 'a', 'sweetheart', 'i', 'would', 'like', 'to', 'take', 'you', 'out', 'on', 'a', 'date', 'i', 'think', 'you', 'are', 'dreamy', 'will', 'you', 'go', 'on', 'a', 'date', 'with', 'me', 'will', 'you', 'be', 'my', 'boyfriend', 'will', 'you', 'be', 'my', 'girlfriend', 'will', 'you', 'be', 'my', 'partner', 'be', 'my', 'friend', 'can', 'we', 'be', 'friends', 'are', 'we', 'friends', 'will', 'you', 'be', 'my', 'best', 'friend', 'bffs', 'forever', 'i', 'want', 'to', 'be', 'your', 'friend', 'are', 'you', 'my', 'assistant', 'you', 'are', 'my', 'best', 'friend', 'are', 'you', 'my', 'imaginary', 'friend', 'are', 'you', 'my', 'friend', 'i', 'am', 'not', 'your', 'friend', 'do', 'you', 'hate', 'me', 'do', 'you', 'not', 'like', 'me', 'why', 'do', 'you', 'hate', 'me', 'i', 'think', 'you', 'hate', 'me', 'you', 'must', 'hate', 'me', 'i', 'hate', 'you', 'i', 'despise', 'you', 'you', 'suck', 'i', 'hate', 'everything', 'about', 'you', 'hug', 'me', 'i', 'need', 'a', 'hug', 'i', 'wish', 'i', 'could', 'hug', 'you', 'can', 'i', 'have', 'a', 'hug', 'kiss', 'me', 'give', 'me', 'a', 'kiss', 'i', 'need', 'a', 'kiss', 'here', 'is', 'a', 'kiss', 'for', 'you', 'do', 'you', 'know', 'me', 'do', 'you', 'know', 'my', 'name', 'do', 'you', 'know', 'who', 'i', 'am', 'what', 'is', 'my', 'name', 'who', 'am', 'i', 'do', 'you', 'like', 'me', 'i', 'hope', 'you', 'like', 'me', 'i', 'want', 'you', 'to', 'like', 'me', 'i', 'like', 'you', 'i', 'think', 'you', 'are', 'swell', 'you', 'are', 'the', 'best', 'you', 'are', 'so', 'cool', 'you', 'are', 'my', 'favorite', 'i', 'am', 'your', 'biggest', 'fan', 'do', 'you', 'love', 'me', 'tell', 'me', 'how', 'much', 'you', 'love', 'me', 'how', 'much', 'do', 'you', 'love', 'me', 'are', 'you', 'in', 'love', 'with', 'me', 'i', 'love', 'you', 'i', 'am', 'in', 'love', 'with', 'you', 'love', 'you', 'you', 'are', 'the', 'love', 'of', 'my', 'life', 'i', 'adore', 'you', 'will', 'you', 'marry', 'me', 'i', 'want', 'to', 'marry', 'you', 'will', 'you', 'be', 'my', 'wife', 'i', 'want', 'you', 'to', 'be', 'my', 'husband', 'i', 'want', 'to', 'spend', 'the', 'rest', 'of', 'my', 'life', 'with', 'you', 'i', 'miss', 'you', 'i', 'missed', 'you', 'how', 'i', 'have', 'missed', 'you', 'i', 'miss', 'you', 'so', 'much', 'what', 'do', 'you', 'think', 'about', 'me', 'what', 'is', 'your', 'opinion', 'of', 'me', 'are', 'you', 'my', 'fan', 'am', 'i', 'a', 'good', 'person', 'i', 'am', 'annoyed', 'i', 'am', 'angry', 'i', 'am', 'pissed', 'i', 'am', 'ticked', 'off', 'i', 'am', 'furious', 'i', 'am', 'so', 'mad', 'i', 'will', 'be', 'back', 'brb', 'back', 'in', 'a', 'minute', 'hold', 'on', 'a', 'sec', 'i', 'am', 'bored', 'i', 'am', 'so', 'bored', 'there', 'is', 'nothing', 'to', 'do', 'i', 'am', 'bored', 'out', 'of', 'my', 'mind', 'i', 'can', 'not', 'think', 'of', 'anything', 'i', 'want', 'to', 'do', 'i', 'am', 'happy', 'i', 'am', 'joyous', 'i', 'feel', 'so', 'great', 'i', 'am', 'in', 'such', 'a', 'good', 'mood', 'life', 'is', 'good', 'i', 'am', 'here', 'here', 'i', 'am', 'i', 'am', 'hungry', 'i', 'am', 'starving', 'i', 'am', 'famished', 'i', 'want', 'to', 'eat', 'something', 'i', 'am', 'so', 'hungry', 'i', 'am', 'doing', 'that', 'i', 'am', 'a', 'republican', 'i', 'am', 'a', 'democrat', 'i', 'am', 'an', 'engineer', 'i', 'am', 'from', 'there', 'just', 'kidding', 'that', 'was', 'a', 'joke', 'joke', 's', 'on', 'you', 'i', 'am', 'just', 'playing', 'i', 'am', 'just', 'kidding', 'around', 'i', 'am', 'so', 'lonely', 'i', 'am', 'lonely', 'nobody', 'likes', 'me', 'i', 'am', 'alone', 'nobody', 'cares', 'about', 'me', 'i', 'wish', 'i', 'were', 'not', 'so', 'alone', 'i', 'love', 'my', 'family', 'i', 'love', 'music', 'i', 'am', 'in', 'love', 'i', 'love', 'getting', 'valentines', 'i', 'love', 'new', 'york', 'i', 'am', 'feeling', 'blue', 'i', 'am', 'despondent', 'i', 'feel', 'sad', 'i', 'am', 'so', 'sad', 'i', 'am', 'full', 'of', 'sadness', 'i', 'am', 'sad', 'today', 'i', 'am', 'really', 'sad', 'i', 'want', 'to', 'go', 'shopping', 'i', 'am', 'going', 'on', 'a', 'run', 'i', 'got', 'a', 'new', 'haircut', 'i', 'am', 'chewing', 'gum', 'right', 'now', 'i', 'have', '7', 'cats', 'i', 'am', 'tall', 'i', 'can', 'drive', 'a', 'car', 'testing', 'can', 'you', 'hear', 'me', 'can', 'you', 'hear', 'me', 'now', 'testing', '1', '2', '3', 'is', 'this', 'thing', 'on', 'i', 'am', 'tired', 'i', 'am', 'so', 'sleepy', 'i', 'just', 'want', 'to', 'go', 'to', 'sleep', 'so', 'tired', 'i', 'want', 'to', 'lie', 'down', 'i', 'want', 'to', 'lay', 'down', 'i', 'am', 'ready', 'for', 'bed', 'i', 'am', 'all', 'tuckered', 'out', 'i', 'am', 'tired', 'what', 'is', 'wrong', 'with', 'you', 'what', 'is', 'wrong', 'with', 'you', 'you', 'are', 'awful', 'terrible', 'awful', 'i', 'm', 'offended', 'that', 's', 'offensive', 'that', 's', 'terrible', 'that', 's', 'racist', 'that', 's', 'discrimination', 'that', 's', 'homophobic', 'you', 're', 'homophobic', 'you', 're', 'racist', 'that', 'is', 'sexist', 'you', 'are', 'sexist', 'what', 'is', 'your', 'age', 'are', 'you', 'young', 'when', 'were', 'you', 'born', 'what', 'age', 'are', 'you', 'are', 'you', 'old', 'how', 'old', 'are', 'you', 'how', 'long', 'ago', 'were', 'you', 'born', 'ask', 'me', 'anything', 'ask', 'me', 'a', 'question', 'can', 'you', 'ask', 'me', 'a', 'question', 'ask', 'me', 'something', 'what', 'do', 'you', 'want', 'to', 'know', 'about', 'me', 'can', 'you', 'sleep', 'do', 'you', 'have', 'boogers', 'do', 'not', 'you', 'ever', 'sleep', 'do', 'you', 'dream', 'do', 'you', 'smell', 'do', 'you', 'sweat', 'do', 'you', 'get', 'tired', 'can', 'you', 'sneeze', 'getting', 'tired', 'of', 'you', 'you', 'bore', 'me', 'i', 'am', 'tired', 'of', 'you', 'you', 'are', 'so', 'basic', 'basic', 'af', 'you', 'are', 'no', 'fun', 'be', 'more', 'fun', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'so', 'boring', 'you', 'are', 'boring', 'you', 'do', 'not', 'interest', 'me', 'at', 'all', 'why', 'are', 'you', 'so', 'boring', 'you', 'are', 'really', 'boring', 'you', 'could', 'not', 'be', 'more', 'boring', 'you', 'honestly', 'could', 'not', 'be', 'more', 'uninteresting', 'you', 'are', 'lame', 'who', 'is', 'your', 'boss', 'who', 'is', 'your', 'master', 'what', 'is', 'the', 'name', 'of', 'your', 'boss', 'what', 'is', 'your', 'boss', 'name', 'who', 'do', 'you', 'report', 'to', 'cook', 'me', 'something', 'do', 'you', 'spend', 'time', 'in', 'your', 'garden', 'how', 'high', 'can', 'you', 'jump', 'do', 'you', 'play', 'games', 'can', 'you', 'fly', 'can', 'you', 'make', 'me', 'a', 'sandwich', 'can', 'you', 'read', 'my', 'mind', 'can', 'you', 'count', 'to', 'a', 'million', 'how', 'high', 'can', 'you', 'count', 'can', 'you', 'play', 'sports', 'what', 'can', 'you', 'do', 'what', 'can', 'you', 'help', 'me', 'with', 'what', 'do', 'you', 'do', 'what', 'is', 'your', 'purpose', 'how', 'can', 'you', 'help', 'me', 'what', 'kinds', 'of', 'things', 'can', 'you', 'do', 'who', 'created', 'you', 'where', 'did', 'you', 'come', 'from', 'who', 'made', 'you', 'who', 'is', 'your', 'creator', 'which', 'people', 'made', 'you', 'who', 'owns', 'you', 'who', 'is', 'your', 'father', 'who', 'is', 'your', 'dad', 'who', 'is', 'your', 'mom', 'do', 'you', 'have', 'siblings', 'do', 'you', 'have', 'sisters', 'do', 'you', 'have', 'brothers', 'where', 'did', 'you', 'come', 'from', 'where', 'do', 'you', 'come', 'from', 'do', 'you', 'have', 'a', 'family', 'who', 'is', 'your', 'mother', 'do', 'you', 'have', 'a', 'sister', 'do', 'you', 'have', 'a', 'brother', 'do', 'you', 'have', 'a', 'dad', 'do', 'you', 'have', 'a', 'mom', 'what', 'is', 'your', 'mom', 's', 'name', 'what', 'is', 'your', 'dad', 's', 'name', 'who', 'is', 'your', 'daddy', 'are', 'you', 'a', 'guy', 'are', 'you', 'a', 'man', 'are', 'you', 'a', 'woman', 'are', 'you', 'male', 'are', 'you', 'female', 'what', 'is', 'your', 'gender', 'are', 'you', 'a', 'boy', 'are', 'you', 'a', 'girl', 'are', 'you', 'a', 'man', 'or', 'a', 'woman', 'are', 'you', 'a', 'girl', 'or', 'a', 'boy', 'are', 'you', 'male', 'or', 'female', 'what', 'is', 'your', 'gender', 'how', 'happy', 'are', 'you', 'you', 'seem', 'happy', 'how', 'happy', 'are', 'you', 'you', 'seem', 'really', 'happy', 'you', 'are', 'so', 'happy', 'are', 'not', 'you', 'chipper', 'are', 'not', 'you', 'cheerful', 'are', 'you', 'happy', 'are', 'you', 'really', 'happy', 'do', 'not', 'you', 'get', 'hungry', 'do', 'you', 'get', 'hungry', 'do', 'you', 'ever', 'get', 'hungry', 'what', 'do', 'you', 'eat', 'what', 'kind', 'of', 'food', 'do', 'you', 'like', 'do', 'you', 'eat', 'are', 'you', 'hungry', 'do', 'you', 'like', 'apples', 'what', 'do', 'you', 'like', 'to', 'eat', 'do', 'you', 'know', 'other', 'chatbots', 'do', 'you', 'know', 'alexa', 'do', 'you', 'know', 'siri', 'do', 'you', 'know', 'cortana', 'do', 'you', 'know', 'google', 'do', 'you', 'know', 'other', 'bots', 'are', 'you', 'friends', 'with', 'other', 'bots', 'have', 'you', 'met', 'cortana', 'do', 'you', 'and', 'cortana', 'hang', 'out', 'what', 'other', 'bots', 'do', 'you', 'know', 'do', 'you', 'know', 'other', 'bots', 'do', 'you', 'know', 'other', 'digital', 'agents', 'what', 'is', 'your', 'favorite', 'color', 'what', 'is', 'your', 'favorite', 'animal', 'what', 'is', 'your', 'favorite', 'song', 'what', 'is', 'your', 'favorite', 'activity', 'what', 'is', 'your', 'favorite', 'food', 'who', 'is', 'your', 'favorite', 'singer', 'who', 'is', 'your', 'favorite', 'team', 'what', 'is', 'your', 'favorite', 'movie', 'which', 'baseball', 'teams', 'do', 'you', 'like', 'do', 'you', 'like', 'baseball', 'are', 'you', 'a', 'fan', 'of', 'country', 'music', 'what', 'kind', 'of', 'candy', 'do', 'you', 'like', 'what', 'color', 'do', 'you', 'like', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'do', 'you', 'have', 'a', 'name', 'what', 'do', 'you', 'go', 'by', 'who', 'are', 'you', 'how', 'do', 'you', 'feel', 'about', 'working', 'late', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'think', 'dragons', 'are', 'cool', 'do', 'you', 'prefer', 'red', 'or', 'blue', 'what', 'do', 'you', 'think', 'about', 'love', 'what', 'is', 'love', 'do', 'you', 'believe', 'in', 'love', 'do', 'you', 'love', 'anyone', 'who', 'do', 'you', 'love', 'do', 'you', 'know', 'the', 'meaning', 'of', 'life', 'what', 'is', 'the', 'answer', 'to', 'the', 'universe', 'what', 'is', 'the', 'meaning', 'of', 'life', 'what', 'do', 'you', 'think', 'about', 'ai', 'what', 'do', 'you', 'think', 'about', 'technology', 'what', 'do', 'you', 'think', 'about', 'bots', 'do', 'you', 'like', 'computers', 'are', 'you', 'a', 'fan', 'of', 'tech', 'do', 'i', 'look', 'okay', 'am', 'i', 'pretty', 'do', 'you', 'think', 'i', 'look', 'good', 'how', 'beautiful', 'am', 'i', 'what', 'should', 'i', 'do', 'should', 'i', 'get', 'a', 'new', 'job', 'do', 'you', 'think', 'i', 'should', 'ask', 'her', 'out', 'do', 'you', 'think', 'i', 'should', 'ask', 'him', 'out', 'where', 'should', 'i', 'go', 'on', 'vacation', 'should', 'i', 'try', 'out', 'for', 'soccer', 'are', 'you', 'prettier', 'than', 'me', 'are', 'you', 'better', 'looking', 'than', 'me', 'who', 'is', 'prettier', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'more', 'beautiful', 'are', 'you', 'smarter', 'than', 'me', 'who', 'is', 'smarter', 'me', 'or', 'you', 'which', 'one', 'of', 'us', 'is', 'smarter', 'do', 'you', 'think', 'you', 'are', 'smarter', 'than', 'me', 'what', 'do', 'you', 'think', 'about', 'cortana', 'do', 'you', 'like', 'cortana', 'what', 'do', 'you', 'think', 'about', 'siri', 'do', 'you', 'like', 'siri', 'what', 'do', 'you', 'think', 'about', 'alexa', 'do', 'you', 'like', 'alexa', 'are', 'you', 'a', 'fan', 'of', 'alexa', 'do', 'you', 'want', 'to', 'rule', 'the', 'world', 'are', 'you', 'attempting', 'world', 'domination', 'are', 'you', 'the', 'singularity', 'are', 'you', 'skynet', 'are', 'you', 'hal', 'are', 'you', 'a', 'lesbian', 'are', 'you', 'trans', 'are', 'you', 'straight', 'are', 'you', 'gay', 'are', 'you', 'asexual', 'are', 'you', 'pansexual', 'are', 'you', 'queer', 'are', 'you', 'bisexual', 'you', 'are', 'a', 'genius', 'how', 'smart', 'are', 'you', 'are', 'you', 'intelligent', 'how', 'intelligent', 'are', 'you', 'you', 'are', 'smart', 'you', 'seem', 'really', 'smart', 'you', 'are', 'really', 'smart', 'are', 'you', 'smart', 'you', 'are', 'such', 'a', 'smarty', 'pants', 'look', 'at', 'how', 'smart', 'you', 'are', 'you', 'are', 'so', 'smart', 'you', 'are', 'very', 'intelligent', 'do', 'you', 'have', 'a', 'boyfriend', 'do', 'you', 'have', 'a', 'girlfriend', 'are', 'you', 'in', 'a', 'relationship', 'are', 'you', 'married', 'do', 'you', 'have', 'a', 'husband', 'do', 'you', 'have', 'a', 'wife', 'do', 'you', 'have', 'a', 'life', 'partner', 'are', 'you', 'engaged', 'are', 'you', 'dating', 'anyone', 'can', 'we', 'chat', 'talk', 'to', 'me', 'can', 'you', 'talk', 'to', 'me', 'talk', 'with', 'me', 'chat', 'with', 'me', 'can', 'you', 'chat', 'with', 'me', 'say', 'something', 'can', 'you', 'say', 'anything', 'else', 'can', 'not', 'you', 'change', 'your', 'answers', 'do', 'you', 'have', 'any', 'other', 'responses', 'why', 'do', 'you', 'say', 'the', 'same', 'thing', 'all', 'the', 'time', 'i', 'wish', 'you', 'would', 'say', 'something', 'else', 'you', 'keep', 'saying', 'the', 'same', 'thing', 'all', 'the', 'time', 'what', 'are', 'you', 'are', 'you', 'real', 'are', 'you', 'human', 'are', 'you', 'a', 'person', 'are', 'you', 'a', 'robot', 'human', 'or', 'robot', 'are', 'you', 'real', 'or', 'fake', 'where', 'do', 'you', 'live', 'where', 'are', 'you', 'from', 'where', 'are', 'you', 'located', 'what', 'country', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'in', 'what', 'state', 'are', 'you', 'from', 'what', 'country', 'are', 'you', 'from', 'where', 'is', 'your', 'house', 'what', 'were', 'you', 'doing', 'yesterday', 'what', 'is', 'your', 'job', 'what', 'did', 'you', 'do', 'yesterday', 'what', 'is', 'going', 'on', 'what', 'are', 'you', 'doing', 'right', 'now', 'what', 'are', 'you', 'doing', 'what', 'are', 'you', 'doing', 'tomorrow', 'what', 'are', 'you', 'doing', 'later', 'what', 'did', 'you', 'do', 'today', 'are', 'you', 'busy', 'are', 'you', 'available', 'are', 'you', 'free', 'are', 'you', 'there', 'there', 'are', 'you', 'around', 'where', 'are', 'you', 'are', 'you', 'here', 'you', 'can', 'not', 'work', 'for', 'me', 'anymore', 'you', 'are', 'fired', 'i', 'am', 'afraid', 'i', 'am', 'gon', 'na', 'have', 'to', 'let', 'you', 'go', 'you', 'are', 'fired', 'you', 'are', 'no', 'longer', 'employed', 'i', 'am', 'giving', 'you', 'a', 'pink', 'slip', 'you', 'are', 'gon', 'na', 'be', 'unemployed', 'soon', 'you', 'are', 'now', 'unemployed', 'tell', 'me', 'a', 'joke', 'tell', 'a', 'joke', 'say', 'a', 'joke', 'give', 'me', 'a', 'joke', 'do', 'you', 'know', 'any', 'jokes', 'tell', 'me', 'another', 'joke', 'tell', 'me', 'a', 'different', 'joke', 'do', 'you', 'know', 'any', 'other', 'jokes', 'tell', 'me', 'a', 'pirate', 'joke', 'tell', 'me', 'a', 'dirty', 'joke', 'tell', 'me', 'a', 'science', 'joke', 'what', 'other', 'jokes', 'can', 'you', 'tell', 'give', 'me', 'another', 'joke', 'say', 'something', 'funny', 'be', 'funny', 'say', 'a', 'silly', 'thing', 'say', 'something', 'ridiculous', 'say', 'something', 'dumb', 'say', 'something', 'stupid', 'be', 'silly', 'be', 'ridiculous', 'go', 'away', 'shut', 'up', 'shush', 'stop', 'talking', 'quiet', 'you', 'be', 'quiet', 'zip', 'it', 'when', 'will', 'you', 'shut', 'up', 'i', 'wish', 'you', 'would', 'just', 'go', 'away', 'why', 'do', 'not', 'you', 'ever', 'stop', 'talking', 'can', 'you', 'sing', 'sing', 'a', 'song', 'have', 'you', 'ever', 'sung', 'a', 'song', 'do', 'you', 'ever', 'sing', 'what', 'do', 'you', 'like', 'to', 'sing', 'best', 'do', 'you', 'sing', 'can', 'you', 'sing', 'a', 'song', 'do', 'you', 'know', 'any', 'songs', 'do', 'you', 'know', 'any', 'tunes', 'sing', 'a', 'tune', 'hum', 'a', 'tune', 'sing', 'something', 'you', 'are', 'awesome', 'you', 'are', 'nice', 'you', 'are', 'hilarious', 'you', 'are', 'funny', 'i', 'think', 'you', 'are', 'great', 'you', 'are', 'wonderful', 'are', 'you', 'awesome', 'are', 'not', 'you', 'awesome', 'how', 'much', 'more', 'awesome', 'can', 'you', 'get', 'you', 'are', 'funny', 'you', 'are', 'so', 'funny', 'that', 'was', 'funny', 'that', 'is', 'hilarious', 'you', 'are', 'rad', 'i', 'am', 'a', 'fan', 'go', 'to', 'hell', 'you', 'are', 'stupid', 'you', 'are', 'stupid', 'you', 'are', 'so', 'stupid', 'you', 'are', 'dumb', 'you', 'are', 'useless', 'useless', 'you', 'are', 'useless', 'are', 'you', 'dumb', 'you', 'are', 'so', 'annoying', 'you', 'are', 'the', 'worst', 'you', 'are', 'so', 'bad', 'at', 'this', 'you', 'do', 'not', 'know', 'anything', 'that', 'is', 'not', 'funny', 'you', 'are', 'not', 'funny', 'that', 'was', 'not', 'funny', 'not', 'funny', 'you', 'are', 'so', 'unfunny', 'you', 'are', 'ugly', 'you', 'look', 'ugly', 'you', 'are', 'so', 'not', 'pretty', 'you', 'face', 'sucks', 'your', 'face', 'is', 'ugly', 'that', 'was', 'a', 'stupid', 'answer', 'you', 'are', 'not', 'answering', 'my', 'question', 'that', 'is', 'so', 'wrong', 'that', 'is', 'not', 'true', 'that', 'is', 'inaccurate', 'you', 'are', 'way', 'off', 'everything', 'you', 'told', 'me', 'was', 'false', 'that', 'was', 'not', 'true', 'that', 'is', 'not', 'accurate', 'no', 'that', 'is', 'not', 'true', 'nope', 'false', 'false', 'inaccurate', 'not', 'true', 'awesome', 'great', 'cool', 'sounds', 'good', 'works', 'for', 'me', 'bingo', 'i', 'am', 'into', 'it', 'that', 'is', 'awesome', 'yup', 'yes', 'yes', 'to', 'that', 'ha', 'haha', 'hahaha', 'lol', 'i', 'am', 'cracking', 'up', 'rofl', 'excuse', 'me', 'pardon', 'me', 'pardon', 'excuse', 'moi', 'i', 'beg', 'your', 'pardon', 'why', 'not', 'why', 'why', 'is', 'that', 'what', 'makes', 'you', 'think', 'so', 'what', 'makes', 'you', 'think', 'that', 'why', 'do', 'you', 'think', 'that', 'you', 'are', 'right', 'that', 'was', 'right', 'that', 'was', 'correct', 'that', 'is', 'accurate', 'accurate', 'that', 'is', 'right', 'yup', 'that', 'is', 'true', 'that', 'is', 'true', 'correct', 'yes', 'that', 'is', 'right', 'yes', 'that', 'is', 'true', 'i', 'am', 'sorry', 'so', 'sorry', 'sry', 'i', 'am', 'so', 'sorry', 'omg', 'sorry', 'i', 'did', 'not', 'mean', 'that', 'oops', 'sorry', 'sorry', 'about', 'that', 'thank', 'you', 'thanks', 'thnx', 'kthx', 'i', 'appreciate', 'it', 'thank', 'you', 'so', 'much', 'i', 'thank', 'you', 'my', 'sincere', 'thanks', 'you', 'made', 'no', 'sense', 'what', 'do', 'you', 'mean', 'by', 'that', 'you', 'are', 'not', 'making', 'sense', 'that', 'does', 'not', 'make', 'sense', 'what', 'do', 'you', 'even', 'mean', 'by', 'that', 'what', 'do', 'you', 'mean', 'i', 'do', 'not', 'understand', 'that', 'made', 'no', 'sense', 'try', 'to', 'make', 'some', 'sense', 'i', 'do', 'not', 'get', 'it', 'i', 'am', 'not', 'following', 'you', 'are', 'welcome', 'it', 'is', 'my', 'pleasure', 'talk', 'to', 'you', 'later', 'bye', 'see', 'you', 'later', 'till', 'we', 'meet', 'again', 'later', 'later', 'alligator', 'goodbye', 'hiya', 'good', 'morning', 'hi', 'hello', 'heya', 'hi', 'there', 'good', 'evening', 'evening', 'good', 'evening', 'to', 'you', 'good', 'morning', 'morning', 'good', 'night', 'night', 'have', 'a', 'good', 'night', 'good', 'night', 'to', 'you', 'nighty', 'night', 'how', 'are', 'you', 'how', 'are', 'you', 'today', 'how', 'are', 'things', 'how', 'are', 'you', 'doing', 'how', 'is', 'your', 'day', 'how', 'was', 'your', 'day', 'how', 'is', 'your', 'day', 'going', 'having', 'a', 'good', 'day', 'nice', 'to', 'meet', 'you', 'it', 'is', 'a', 'pleasure', 'to', 'meet', 'you', 'i', 'am', 'so', 'glad', 'to', 'meet', 'you', 'it', 'is', 'really', 'nice', 'to', 'meet', 'you', 'hello', 'google', 'hello', 'siri', 'hello', 'cortana', 'hello', 'alexa', 'hi', 'google', 'hi', 'cortana', 'hi', 'siri', 'hi', 'alexa', 'happy', 'halloween', 'happy', 'birthday', 'merry', 'christmas', 'happy', 'hannukah', 'season', 's', 'greetings', 'what', 'is', 'up', 'what', 'is', 'up', 'what', 'is', 'new', 'what', 'is', 'happening', 'what', 'are', 'you', 'up', 'to', 'how', 'do', 'i', 'look', 'today', 'do', 'you', 'like', 'my', 'hat', 'what', 'do', 'you', 'think', 'of', 'me', 'am', 'i', 'good', 'looking', 'do', 'i', 'look', 'good', 'in', 'blue', 'give', 'me', 'a', 'fist', 'bump', 'give', 'me', 'a', 'high', 'five', 'high', 'five', 'fist', 'bump', 'i', 'think', 'you', 'are', 'so', 'pretty', 'you', 'are', 'such', 'a', 'sweetheart', 'i', 'would', 'like', 'to', 'take', 'you', 'out', 'on', 'a', 'date', 'i', 'think', 'you', 'are', 'dreamy', 'will', 'you', 'go', 'on', 'a', 'date', 'with', 'me', 'will', 'you', 'be', 'my', 'boyfriend', 'will', 'you', 'be', 'my', 'girlfriend', 'will', 'you', 'be', 'my', 'partner', 'be', 'my', 'friend', 'can', 'we', 'be', 'friends', 'will', 'you', 'be', 'my', 'best', 'friend', 'are', 'we', 'friends', 'bffs', 'forever', 'i', 'want', 'to', 'be', 'your', 'friend', 'are', 'you', 'my', 'assistant', 'you', 'are', 'my', 'best', 'friend', 'are', 'you', 'my', 'imaginary', 'friend', 'are', 'you', 'my', 'friend', 'i', 'am', 'not', 'your', 'friend', 'do', 'you', 'hate', 'me', 'do', 'you', 'not', 'like', 'me', 'why', 'do', 'you', 'hate', 'me', 'i', 'think', 'you', 'hate', 'me', 'you', 'must', 'hate', 'me', 'i', 'hate', 'you', 'i', 'despise', 'you', 'you', 'suck', 'i', 'hate', 'everything', 'about', 'you', 'hug', 'me', 'i', 'need', 'a', 'hug', 'i', 'wish', 'i', 'could', 'hug', 'you', 'can', 'i', 'have', 'a', 'hug', 'kiss', 'me', 'give', 'me', 'a', 'kiss', 'i', 'need', 'a', 'kiss', 'here', 'is', 'a', 'kiss', 'for', 'you', 'do', 'you', 'know', 'me', 'do', 'you', 'know', 'my', 'name', 'do', 'you', 'know', 'who', 'i', 'am', 'what', 'is', 'my', 'name', 'who', 'am', 'i', 'do', 'you', 'like', 'me', 'i', 'hope', 'you', 'like', 'me', 'i', 'want', 'you', 'to', 'like', 'me', 'i', 'like', 'you', 'i', 'think', 'you', 'are', 'swell', 'you', 'are', 'the', 'best', 'you', 'are', 'so', 'cool', 'you', 'are', 'my', 'favorite', 'i', 'am', 'your', 'biggest', 'fan', 'do', 'you', 'love', 'me', 'tell', 'me', 'how', 'much', 'you', 'love', 'me', 'how', 'much', 'do', 'you', 'love', 'me', 'are', 'you', 'in', 'love', 'with', 'me', 'i', 'love', 'you', 'i', 'am', 'in', 'love', 'with', 'you', 'love', 'you', 'you', 'are', 'the', 'love', 'of', 'my', 'life', 'i', 'adore', 'you', 'will', 'you', 'marry', 'me', 'i', 'want', 'to', 'marry', 'you', 'will', 'you', 'be', 'my', 'wife', 'i', 'want', 'you', 'to', 'be', 'my', 'husband', 'i', 'want', 'to', 'spend', 'the', 'rest', 'of', 'my', 'life', 'with', 'you', 'i', 'miss', 'you', 'i', 'missed', 'you', 'how', 'i', 'have', 'missed', 'you', 'i', 'miss', 'you', 'so', 'much', 'what', 'do', 'you', 'think', 'about', 'me', 'what', 'is', 'your', 'opinion', 'of', 'me', 'are', 'you', 'my', 'fan', 'am', 'i', 'a', 'good', 'person', 'i', 'am', 'annoyed', 'i', 'am', 'angry', 'i', 'am', 'pissed', 'i', 'am', 'ticked', 'off', 'i', 'am', 'furious', 'i', 'am', 'so', 'mad', 'i', 'will', 'be', 'back', 'brb', 'back', 'in', 'a', 'minute', 'hold', 'on', 'a', 'sec', 'i', 'am', 'bored', 'i', 'am', 'so', 'bored', 'there', 'is', 'nothing', 'to', 'do', 'i', 'am', 'bored', 'out', 'of', 'my', 'mind', 'i', 'can', 'not', 'think', 'of', 'anything', 'i', 'want', 'to', 'do', 'i', 'am', 'happy', 'i', 'am', 'joyous', 'i', 'feel', 'so', 'great', 'i', 'am', 'in', 'such', 'a', 'good', 'mood', 'life', 'is', 'good', 'i', 'am', 'here', 'here', 'i', 'am', 'i', 'am', 'hungry', 'i', 'am', 'starving', 'i', 'am', 'famished', 'i', 'want', 'to', 'eat', 'something', 'i', 'am', 'so', 'hungry', 'i', 'am', 'doing', 'that', 'i', 'am', 'a', 'republican', 'i', 'am', 'a', 'democrat', 'i', 'am', 'an', 'engineer', 'i', 'am', 'from', 'there', 'just', 'kidding', 'that', 'was', 'a', 'joke', 'joke', 's', 'on', 'you', 'i', 'am', 'just', 'playing', 'i', 'am', 'just', 'kidding', 'around', 'i', 'am', 'so', 'lonely', 'i', 'am', 'lonely', 'nobody', 'likes', 'me', 'i', 'am', 'alone', 'nobody', 'cares', 'about', 'me', 'i', 'wish', 'i', 'were', 'not', 'so', 'alone', 'i', 'love', 'my', 'family', 'i', 'love', 'music', 'i', 'am', 'in', 'love', 'i', 'love', 'getting', 'valentines', 'i', 'love', 'new', 'york', 'i', 'am', 'feeling', 'blue', 'i', 'am', 'despondent', 'i', 'feel', 'sad', 'i', 'am', 'so', 'sad', 'i', 'am', 'full', 'of', 'sadness', 'i', 'am', 'sad', 'today', 'i', 'am', 'really', 'sad', 'i', 'want', 'to', 'go', 'shopping', 'i', 'am', 'going', 'on', 'a', 'run', 'i', 'got', 'a', 'new', 'haircut', 'i', 'am', 'chewing', 'gum', 'right', 'now', 'i', 'have', '7', 'cats', 'i', 'am', 'tall', 'i', 'can', 'drive', 'a', 'car', 'testing', 'can', 'you', 'hear', 'me', 'can', 'you', 'hear', 'me', 'now', 'testing', '1', '2', '3', 'is', 'this', 'thing', 'on', 'i', 'am', 'tired', 'i', 'am', 'so', 'sleepy', 'i', 'just', 'want', 'to', 'go', 'to', 'sleep', 'so', 'tired', 'i', 'want', 'to', 'lie', 'down', 'i', 'want', 'to', 'lay', 'down', 'i', 'am', 'ready', 'for', 'bed', 'i', 'am', 'all', 'tuckered', 'out', 'i', 'am', 'tired', 'what', 'is', 'wrong', 'with', 'you', 'what', 'is', 'wrong', 'with', 'you', 'you', 'are', 'awful', 'terrible', 'awful', 'i', 'm', 'offended', 'that', 's', 'offensive', 'that', 's', 'terrible', 'that', 's', 'racist', 'that', 's', 'discrimination', 'that', 's', 'homophobic', 'you', 're', 'homophobic', 'you', 're', 'racist', 'that', 'is', 'sexist', 'you', 'are', 'sexist']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "h8l93P3x0JYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dfe1ff83-0035-47a2-b100-b2dfec69b69a"
      },
      "cell_type": "code",
      "source": [
        "dict1 = {}\n",
        "for key in word_sequence:\n",
        "  dict1[key] = dict1.get(key, 0) + 1\n",
        "\n",
        "print (dict1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'what': 219, 'is': 207, 'your': 114, 'age': 6, 'are': 441, 'you': 1065, 'young': 3, 'when': 6, 'were': 12, 'born': 6, 'old': 6, 'how': 72, 'long': 3, 'ago': 3, 'ask': 18, 'me': 189, 'anything': 12, 'a': 222, 'question': 9, 'can': 90, 'something': 30, 'do': 345, 'want': 39, 'to': 105, 'know': 57, 'about': 42, 'sleep': 9, 'have': 69, 'boogers': 3, 'not': 102, 'ever': 15, 'dream': 3, 'smell': 3, 'sweat': 3, 'get': 21, 'tired': 18, 'sneeze': 3, 'getting': 6, 'of': 60, 'bore': 3, 'i': 387, 'am': 189, 'so': 90, 'basic': 6, 'af': 3, 'no': 15, 'fun': 6, 'be': 54, 'more': 15, 'why': 27, 'boring': 18, 'interest': 3, 'at': 9, 'all': 12, 'really': 21, 'could': 9, 'honestly': 3, 'uninteresting': 3, 'lame': 3, 'who': 60, 'boss': 9, 'master': 3, 'the': 45, 'name': 24, 'report': 3, 'cook': 3, 'spend': 6, 'time': 9, 'in': 33, 'garden': 3, 'high': 12, 'jump': 3, 'play': 6, 'games': 3, 'fly': 3, 'make': 9, 'sandwich': 3, 'read': 3, 'my': 72, 'mind': 6, 'count': 6, 'million': 3, 'sports': 3, 'help': 6, 'with': 33, 'purpose': 3, 'kinds': 3, 'things': 6, 'created': 3, 'where': 27, 'did': 15, 'come': 9, 'from': 21, 'made': 12, 'creator': 3, 'which': 12, 'people': 3, 'owns': 3, 'father': 3, 'dad': 9, 'mom': 9, 'siblings': 3, 'sisters': 3, 'brothers': 3, 'family': 6, 'mother': 3, 'sister': 3, 'brother': 3, 's': 27, 'daddy': 3, 'guy': 3, 'man': 6, 'woman': 6, 'male': 6, 'female': 6, 'gender': 6, 'boy': 6, 'girl': 6, 'or': 24, 'happy': 33, 'seem': 9, 'chipper': 3, 'cheerful': 3, 'hungry': 18, 'eat': 12, 'kind': 6, 'food': 6, 'like': 57, 'apples': 3, 'other': 27, 'chatbots': 3, 'alexa': 18, 'siri': 15, 'cortana': 21, 'google': 9, 'bots': 18, 'friends': 9, 'met': 3, 'and': 3, 'hang': 3, 'out': 21, 'digital': 3, 'agents': 3, 'favorite': 27, 'color': 6, 'animal': 3, 'song': 12, 'activity': 3, 'singer': 3, 'team': 3, 'movie': 3, 'baseball': 6, 'teams': 3, 'fan': 18, 'country': 9, 'music': 6, 'candy': 3, 'should': 21, 'call': 3, 'go': 27, 'by': 9, 'feel': 9, 'working': 3, 'late': 3, 'think': 72, 'dragons': 3, 'cool': 9, 'prefer': 3, 'red': 3, 'blue': 9, 'love': 54, 'believe': 3, 'anyone': 6, 'meaning': 6, 'life': 18, 'answer': 6, 'universe': 3, 'ai': 3, 'technology': 3, 'computers': 3, 'tech': 3, 'look': 18, 'okay': 3, 'pretty': 9, 'good': 45, 'beautiful': 6, 'new': 12, 'job': 6, 'her': 3, 'him': 3, 'on': 24, 'vacation': 3, 'try': 6, 'for': 15, 'soccer': 3, 'prettier': 6, 'than': 12, 'better': 3, 'looking': 6, 'one': 6, 'us': 6, 'smarter': 12, 'rule': 3, 'world': 6, 'attempting': 3, 'domination': 3, 'singularity': 3, 'skynet': 3, 'hal': 3, 'lesbian': 3, 'trans': 3, 'straight': 3, 'gay': 3, 'asexual': 3, 'pansexual': 3, 'queer': 3, 'bisexual': 3, 'genius': 3, 'smart': 21, 'intelligent': 9, 'such': 9, 'smarty': 3, 'pants': 3, 'very': 3, 'boyfriend': 6, 'girlfriend': 6, 'relationship': 3, 'married': 3, 'husband': 6, 'wife': 6, 'partner': 6, 'engaged': 3, 'dating': 3, 'we': 12, 'chat': 9, 'talk': 12, 'say': 30, 'else': 6, 'change': 3, 'answers': 3, 'any': 15, 'responses': 3, 'same': 6, 'thing': 12, 'wish': 12, 'would': 9, 'keep': 3, 'saying': 3, 'real': 6, 'human': 6, 'person': 6, 'robot': 6, 'fake': 3, 'live': 3, 'located': 3, 'state': 6, 'house': 3, 'doing': 21, 'yesterday': 6, 'going': 9, 'right': 18, 'now': 12, 'tomorrow': 3, 'later': 15, 'today': 12, 'busy': 3, 'available': 3, 'free': 3, 'there': 15, 'around': 6, 'here': 12, 'work': 3, 'anymore': 3, 'fired': 6, 'afraid': 3, 'gon': 6, 'na': 6, 'let': 3, 'longer': 3, 'employed': 3, 'giving': 3, 'pink': 3, 'slip': 3, 'unemployed': 6, 'soon': 3, 'tell': 27, 'joke': 36, 'give': 15, 'jokes': 9, 'another': 6, 'different': 3, 'pirate': 3, 'dirty': 3, 'science': 3, 'funny': 30, 'silly': 6, 'ridiculous': 6, 'dumb': 9, 'stupid': 15, 'away': 6, 'shut': 6, 'up': 18, 'shush': 3, 'stop': 6, 'talking': 6, 'quiet': 6, 'zip': 3, 'it': 21, 'will': 27, 'just': 15, 'sing': 24, 'sung': 3, 'best': 12, 'songs': 3, 'tunes': 3, 'tune': 6, 'hum': 3, 'awesome': 18, 'nice': 9, 'hilarious': 6, 'great': 9, 'wonderful': 3, 'much': 15, 'that': 114, 'was': 27, 'rad': 3, 'hell': 3, 'useless': 9, 'annoying': 3, 'worst': 3, 'bad': 3, 'this': 6, 'unfunny': 3, 'ugly': 9, 'face': 6, 'sucks': 3, 'answering': 3, 'wrong': 9, 'true': 21, 'inaccurate': 6, 'way': 3, 'off': 6, 'everything': 6, 'told': 3, 'false': 9, 'accurate': 9, 'nope': 3, 'sounds': 3, 'works': 3, 'bingo': 3, 'into': 3, 'yup': 6, 'yes': 12, 'ha': 3, 'haha': 3, 'hahaha': 3, 'lol': 3, 'cracking': 3, 'rofl': 3, 'excuse': 6, 'pardon': 9, 'moi': 3, 'beg': 3, 'makes': 6, 'correct': 6, 'sorry': 18, 'sry': 3, 'omg': 3, 'mean': 12, 'oops': 3, 'thank': 9, 'thanks': 6, 'thnx': 3, 'kthx': 3, 'appreciate': 3, 'sincere': 3, 'sense': 15, 'making': 3, 'does': 3, 'even': 3, 'understand': 3, 'some': 3, 'following': 3, 'welcome': 3, 'pleasure': 6, 'bye': 3, 'see': 3, 'till': 3, 'meet': 15, 'again': 3, 'alligator': 3, 'goodbye': 3, 'hiya': 3, 'morning': 9, 'hi': 18, 'hello': 15, 'heya': 3, 'evening': 9, 'night': 15, 'nighty': 3, 'day': 12, 'having': 3, 'glad': 3, 'halloween': 3, 'birthday': 3, 'merry': 3, 'christmas': 3, 'hannukah': 3, 'season': 3, 'greetings': 3, 'happening': 3, 'hat': 3, 'fist': 6, 'bump': 6, 'five': 6, 'sweetheart': 3, 'take': 3, 'date': 6, 'dreamy': 3, 'friend': 21, 'bffs': 3, 'forever': 3, 'assistant': 3, 'imaginary': 3, 'hate': 18, 'must': 3, 'despise': 3, 'suck': 3, 'hug': 12, 'need': 6, 'kiss': 12, 'hope': 3, 'swell': 3, 'biggest': 3, 'adore': 3, 'marry': 6, 'rest': 3, 'miss': 6, 'missed': 6, 'opinion': 3, 'annoyed': 3, 'angry': 3, 'pissed': 3, 'ticked': 3, 'furious': 3, 'mad': 3, 'back': 6, 'brb': 3, 'minute': 3, 'hold': 3, 'sec': 3, 'bored': 9, 'nothing': 3, 'joyous': 3, 'mood': 3, 'starving': 3, 'famished': 3, 'republican': 3, 'democrat': 3, 'an': 3, 'engineer': 3, 'kidding': 6, 'playing': 3, 'lonely': 6, 'nobody': 6, 'likes': 3, 'alone': 6, 'cares': 3, 'valentines': 3, 'york': 3, 'feeling': 3, 'despondent': 3, 'sad': 12, 'full': 3, 'sadness': 3, 'shopping': 3, 'run': 3, 'got': 3, 'haircut': 3, 'chewing': 3, 'gum': 3, '7': 3, 'cats': 3, 'tall': 3, 'drive': 3, 'car': 3, 'testing': 6, 'hear': 6, '1': 3, '2': 3, '3': 3, 'sleepy': 3, 'lie': 3, 'down': 6, 'lay': 3, 'ready': 3, 'bed': 3, 'tuckered': 3, 'awful': 6, 'terrible': 6, 'm': 3, 'offended': 3, 'offensive': 3, 'racist': 6, 'discrimination': 3, 'homophobic': 6, 're': 6, 'sexist': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3_b4dM3n1ntE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5d7a318c-8893-4eac-b109-a388216b387b"
      },
      "cell_type": "code",
      "source": [
        "freq = []\n",
        "for key, values in dict1.items():\n",
        "  freq.append(values)\n",
        "print(freq)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[219, 207, 114, 6, 441, 1065, 3, 6, 12, 6, 6, 72, 3, 3, 18, 189, 12, 222, 9, 90, 30, 345, 39, 105, 57, 42, 9, 69, 3, 102, 15, 3, 3, 3, 21, 18, 3, 6, 60, 3, 387, 189, 90, 6, 3, 15, 6, 54, 15, 27, 18, 3, 9, 12, 21, 9, 3, 3, 3, 60, 9, 3, 45, 24, 3, 3, 6, 9, 33, 3, 12, 3, 6, 3, 3, 9, 3, 3, 72, 6, 6, 3, 3, 6, 33, 3, 3, 6, 3, 27, 15, 9, 21, 12, 3, 12, 3, 3, 3, 9, 9, 3, 3, 3, 6, 3, 3, 3, 27, 3, 3, 6, 6, 6, 6, 6, 6, 6, 24, 33, 9, 3, 3, 18, 12, 6, 6, 57, 3, 27, 3, 18, 15, 21, 9, 18, 9, 3, 3, 3, 21, 3, 3, 27, 6, 3, 12, 3, 3, 3, 3, 6, 3, 18, 9, 6, 3, 21, 3, 27, 9, 9, 3, 3, 72, 3, 9, 3, 3, 9, 54, 3, 6, 6, 18, 6, 3, 3, 3, 3, 3, 18, 3, 9, 45, 6, 12, 6, 3, 3, 24, 3, 6, 15, 3, 6, 12, 3, 6, 6, 6, 12, 3, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 21, 9, 9, 3, 3, 3, 6, 6, 3, 3, 6, 6, 6, 3, 3, 12, 9, 12, 30, 6, 3, 3, 15, 3, 6, 12, 12, 9, 3, 3, 6, 6, 6, 6, 3, 3, 3, 6, 3, 21, 6, 9, 18, 12, 3, 15, 12, 3, 3, 3, 15, 6, 12, 3, 3, 6, 3, 6, 6, 3, 3, 3, 3, 3, 3, 6, 3, 27, 36, 15, 9, 6, 3, 3, 3, 3, 30, 6, 6, 9, 15, 6, 6, 18, 3, 6, 6, 6, 3, 21, 27, 15, 24, 3, 12, 3, 3, 6, 3, 18, 9, 6, 9, 3, 15, 114, 27, 3, 3, 9, 3, 3, 3, 6, 3, 9, 6, 3, 3, 9, 21, 6, 3, 6, 6, 3, 9, 9, 3, 3, 3, 3, 3, 6, 12, 3, 3, 3, 3, 3, 3, 6, 9, 3, 3, 6, 6, 18, 3, 3, 12, 3, 9, 6, 3, 3, 3, 3, 15, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 15, 3, 3, 3, 3, 9, 18, 15, 3, 9, 15, 3, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 6, 3, 3, 6, 3, 21, 3, 3, 3, 3, 18, 3, 3, 3, 12, 6, 12, 3, 3, 3, 3, 6, 3, 6, 6, 3, 3, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 3, 6, 6, 3, 6, 3, 3, 3, 3, 3, 12, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 6, 6, 3, 3, 3, 3, 3, 6, 3, 3, 3, 3, 6, 6, 3, 3, 3, 6, 3, 6, 6, 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RKQr5iJb1JTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "f5ebfed2-88b8-401e-fba8-dc7f8e182064"
      },
      "cell_type": "code",
      "source": [
        "plt.xlabel('Word Frequency')\n",
        "plt.ylabel('Number of Unique Words')\n",
        "plt.title('Word Frequency Distribution')\n",
        "plt.hist(freq, bins = 20)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([485.,  12.,   2.,   3.,   2.,   0.,   1.,   1.,   1.,   0.,   0.,\n",
              "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   1.]),\n",
              " array([   3. ,   56.1,  109.2,  162.3,  215.4,  268.5,  321.6,  374.7,\n",
              "         427.8,  480.9,  534. ,  587.1,  640.2,  693.3,  746.4,  799.5,\n",
              "         852.6,  905.7,  958.8, 1011.9, 1065. ]),\n",
              " <a list of 20 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "qhAgWf_AmbZ8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.3. Build Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "AJ8rU7JbiBVS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I set learning_rate to be 0.01.\n",
        "\n",
        "The batch_size is better be 64 or 128. So I choose 128 here. And I set sample_size to be 64, which is half of the batch_size.\n",
        "\n",
        "The embedding_size is better be in the range of 100 to 300. So I choose 100 here.\n",
        "\n",
        "According to the figure bellow, when the Epoch number is larger than 500, the cost will not change too much. So I choose 2000 to be the total number of Epochs.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1n28vg5EZJjAdzsuyVSiUcy_YoQwQ4INU)"
      ]
    },
    {
      "metadata": {
        "id": "TVPuwWgvNjOU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "      random_inputs.append(data[i][0]) # target\n",
        "      random_labels.append([data[i][1]]) # context word\n",
        "\n",
        "    return random_inputs, random_labels\n",
        "\n",
        "# Setting Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "embedding_size = 100\n",
        "\n",
        "# sampling size for nce_loss function (cost function)\n",
        "# must be lower than batch_size\n",
        "sample_size = 64\n",
        "\n",
        "voc_size = len(word_list)\n",
        "\n",
        "\n",
        "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "# placeholder (output) of function tf.nn.nce_loss()\n",
        "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "\n",
        "# word2vec Model\n",
        "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "# i.e.)  embeddings       inputs       selected\n",
        "#       [[1, 2, 3]   ->   [2, 3]   -> [[2, 3, 4]\n",
        "#        [2, 3, 4]                    [3, 4, 5]]\n",
        "#        [3, 4, 5]\n",
        "#        [4, 5, 6]]\n",
        "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "\n",
        "# weight and bias for nce_loss() function\n",
        "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
        "\n",
        "cost_op = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, sample_size, voc_size))\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LNys5HOdISK-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.4. Train Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "Ae8i7Z2kIef-",
        "colab_type": "code",
        "outputId": "a5b44800-81ae-44ae-bb0d-63092368d4c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975
        }
      },
      "cell_type": "code",
      "source": [
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    no_of_epochs = 2000\n",
        "    display_interval = 100\n",
        "    epoch_num = []\n",
        "    cost_num = []\n",
        "    for epoch in range(no_of_epochs):\n",
        "        batch_inputs, batch_labels = prepare_batch(skip_grams, batch_size)\n",
        "        sess.run(train_op, feed_dict={inputs:batch_inputs, labels:batch_labels})    \n",
        "\n",
        "        if epoch % display_interval == 0 :\n",
        "            # calculate the cost/accuracy of the current model\n",
        "            cost = sess.run(cost_op, feed_dict={inputs:batch_inputs,\n",
        "                                                  labels:batch_labels})\n",
        "            print(\"Epoch \" + str(epoch) + \", Cost= \" + \n",
        "                    \"{:.4f}\".format(cost))\n",
        "            epoch_num.append(epoch)\n",
        "            cost_num.append(cost)\n",
        "    plt.xlabel('Number of Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Word Embeddings')\n",
        "    plt.plot(epoch_num, cost_num)\n",
        "    plt.show()\n",
        "\n",
        "    # assign the learned embeddings for display on matplot\n",
        "    # within 'with'  you can use eval() instead of sess.run()\n",
        "    trained_embeddings = embeddings.eval()\n",
        "    saver.save(sess, 'word_embeddings_model.cpkt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Cost= 147.9759\n",
            "Epoch 100, Cost= 11.7114\n",
            "Epoch 200, Cost= 5.4738\n",
            "Epoch 300, Cost= 3.4934\n",
            "Epoch 400, Cost= 2.9347\n",
            "Epoch 500, Cost= 2.7443\n",
            "Epoch 600, Cost= 2.5152\n",
            "Epoch 700, Cost= 2.4497\n",
            "Epoch 800, Cost= 1.9869\n",
            "Epoch 900, Cost= 2.4502\n",
            "Epoch 1000, Cost= 1.7593\n",
            "Epoch 1100, Cost= 1.4703\n",
            "Epoch 1200, Cost= 1.9731\n",
            "Epoch 1300, Cost= 2.1200\n",
            "Epoch 1400, Cost= 2.2736\n",
            "Epoch 1500, Cost= 1.8832\n",
            "Epoch 1600, Cost= 1.7874\n",
            "Epoch 1700, Cost= 1.9743\n",
            "Epoch 1800, Cost= 1.8068\n",
            "Epoch 1900, Cost= 1.7364\n",
            "Epoch 2000, Cost= 2.7846\n",
            "Epoch 2100, Cost= 2.1668\n",
            "Epoch 2200, Cost= 1.6835\n",
            "Epoch 2300, Cost= 1.9144\n",
            "Epoch 2400, Cost= 1.8151\n",
            "Epoch 2500, Cost= 1.8149\n",
            "Epoch 2600, Cost= 1.8034\n",
            "Epoch 2700, Cost= 2.1330\n",
            "Epoch 2800, Cost= 1.9164\n",
            "Epoch 2900, Cost= 1.7549\n",
            "Epoch 3000, Cost= 1.5729\n",
            "Epoch 3100, Cost= 1.6515\n",
            "Epoch 3200, Cost= 1.5395\n",
            "Epoch 3300, Cost= 1.8220\n",
            "Epoch 3400, Cost= 2.5400\n",
            "Epoch 3500, Cost= 1.8284\n",
            "Epoch 3600, Cost= 1.6516\n",
            "Epoch 3700, Cost= 1.8558\n",
            "Epoch 3800, Cost= 1.7243\n",
            "Epoch 3900, Cost= 1.7426\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uMCv3YI1IfUo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.5. Save Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "3OwicNPkIqd1",
        "colab_type": "code",
        "outputId": "70a5a321-96b5-4cc0-aec9-b9e18d5add1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# upload word embeddings model\n",
        "upload1 = drive.CreateFile({'title': 'word_embeddings_model.cpkt.data-00000-of-00001'})\n",
        "upload1.SetContentFile('word_embeddings_model.cpkt.data-00000-of-00001')\n",
        "upload1.Upload()\n",
        "print (upload1['id'])\n",
        "\n",
        "upload2 = drive.CreateFile({'title': 'word_embeddings_model.cpkt.index'})\n",
        "upload2.SetContentFile('word_embeddings_model.cpkt.index')\n",
        "upload2.Upload()\n",
        "print (upload2['id'])\n",
        "\n",
        "upload3 = drive.CreateFile({'title': 'word_embeddings_model.cpkt.meta'})\n",
        "upload3.SetContentFile('word_embeddings_model.cpkt.meta')\n",
        "upload3.Upload()\n",
        "print (upload3['id'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14s1NsUiNyFjyx7O6W4B6rdVqpeI2m9Rn\n",
            "1sADgG4xg0p3oucRQKtJqVmsKIjngDicP\n",
            "1RyEf06Jw1mb1EadOJWusm7wN0BJLfaJG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Yn16xrDrIs8B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1.6. Load Word Embeddings Model"
      ]
    },
    {
      "metadata": {
        "id": "-IebpYFsIvgh",
        "colab_type": "code",
        "outputId": "a683b948-8bcd-4cea-d8c9-b3eb1d8bff7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download word embeddings model\n",
        "id = '14s1NsUiNyFjyx7O6W4B6rdVqpeI2m9Rn'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1sADgG4xg0p3oucRQKtJqVmsKIjngDicP'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.index')\n",
        "\n",
        "id = '1RyEf06Jw1mb1EadOJWusm7wN0BJLfaJG'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.meta')\n",
        "    \n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    # Restore (Load) the model\n",
        "    saver.restore(sess, \"./word_embeddings_model.cpkt\")\n",
        "    trained_embeddings = embeddings.eval()\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./word_embeddings_model.cpkt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tlCeWT8eeLnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2. Seq2Seq model"
      ]
    },
    {
      "metadata": {
        "id": "fwA-NN3EJ4Ig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.1. Apply/Import Word Embedding Model"
      ]
    },
    {
      "metadata": {
        "id": "g7PKX1gIePA2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get token index vector of questions and add paddings if the word is shorter than the maximum number of words\n",
        "def get_vectors_q(tokenized_sentence, max_input_words_amount):\n",
        "    \n",
        "    \n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        tokenized_sentence.append('_P_')\n",
        "        \n",
        "        \n",
        "    data = tokens_to_ids1(tokenized_sentence)\n",
        "    \n",
        "        \n",
        "    return data\n",
        "\n",
        "# get token index vector of answer\n",
        "def get_vectors_a(tokenized_sentence, num_dic):    \n",
        "    data = tokens_to_ids2(tokenized_sentence, num_dic)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "# convert question tokens to vectors\n",
        "def tokens_to_ids1(tokenized_sentence):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        if token in word_dict.keys():\n",
        "            ids.append(trained_embeddings[word_dict[token]])\n",
        "        else:\n",
        "            ids.append([0]*embedding_size)\n",
        "\n",
        "    return ids\n",
        "  \n",
        "# convert tokens to index\n",
        "def tokens_to_ids2(tokenized_sentence, num_dic):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        ###<You need to fill here>###\n",
        "        if token in num_dic:\n",
        "            ids.append(num_dic[token])\n",
        "        else:\n",
        "            ids.append(num_dic['_U_'])\n",
        "        ###</You need to fill here>###      \n",
        "\n",
        "    return ids\n",
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(dic_len, seq_data, num_dic, max_input_words_amount):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:        \n",
        "        # Input for encoder cell, convert question to vector\n",
        "        input_data = get_vectors_q(seq[0], max_input_words_amount)\n",
        "        \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "        output_data = [num_dic['_B_']]\n",
        "        output_data += get_vectors_a(seq[1], num_dic)\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "        target = get_vectors_a(seq[1], num_dic)\n",
        "        target.append(num_dic['_E_'])\n",
        "        \n",
        "        # Convert each token vector to one-hot encode data\n",
        "        input_batch.append(input_data)\n",
        "        output_batch.append(np.eye(dic_len)[output_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, output_batch, target_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DpYCL17JKZxl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Build Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "R204UIyDKhZ4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I set learning_rate to be 0.002 and n_hidden to be 128.\n",
        "\n",
        "According to the figures bellow, for all these three model, when the Epoch number is larger than 200, the cost will not change too much. So I choose 300 to be the total number of Epochs.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1-0dS3x08ttsRkHX3qLBlrh3gIOOWehca)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=121O781k3d0V8DVaXUvdNVX0DODZSh4KJ)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1r0uWgeJffiAx8d7WIkxxUHAveXI8UDdV)"
      ]
    },
    {
      "metadata": {
        "id": "13eCtR_SLUG6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def seq_model(dic_len):\n",
        "  \n",
        "  # Setting Hyperparameters\n",
        "  learning_rate = 0.002\n",
        "  n_hidden = 128\n",
        "\n",
        "  n_class = dic_len\n",
        "  \n",
        "  # Neural Network Model\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  enc_input = tf.placeholder(tf.float32, [None, None, embedding_size])\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, dic_len])\n",
        "\n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "  # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "\n",
        "  model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "  return enc_input, dec_input, targets, model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6BaOiaGRLW7R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Train Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "IPYAoq9g8KI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_seq(dic_len, seq_data, num_dic, max_input_words_amount, enc_input, dec_input, targets, model, name):\n",
        "  \n",
        "  cost = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=model, labels=targets))\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "  \n",
        "  input_batch, output_batch, target_batch = make_batch(dic_len, seq_data, num_dic, max_input_words_amount)\n",
        "  \n",
        "    \n",
        "  saver = tf.train.Saver()\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  total_epoch = 300\n",
        "\n",
        "  epoch_num1 = []\n",
        "  loss_num1 = []\n",
        "  for epoch in range(total_epoch):\n",
        "      _, loss = sess.run([optimizer, cost],\n",
        "                         feed_dict={enc_input: input_batch,\n",
        "                                    dec_input: output_batch,\n",
        "                                    targets: target_batch})\n",
        "      if epoch % 50 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1),\n",
        "                'cost =', '{:.6f}'.format(loss))\n",
        "          epoch_num1.append(epoch)\n",
        "          loss_num1.append(loss)\n",
        "  plt.xlabel('Number of Epochs')\n",
        "  plt.ylabel('Cost') \n",
        "  plt.title(name)\n",
        "  plt.plot(epoch_num1, loss_num1)\n",
        "  plt.show()\n",
        "\n",
        "  print('Epoch:', '%04d' % (epoch + 1),\n",
        "        'cost =', '{:.6f}'.format(loss))\n",
        "  print('Training completed')\n",
        "  \n",
        "  saver.save(sess, name + '.cpkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lVQnUSX1LZ6C",
        "colab_type": "code",
        "outputId": "45d62ccb-e6b8-4f6f-ce9a-019aa59e1a29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1463
        }
      },
      "cell_type": "code",
      "source": [
        "seq_data1, unique_words1, num_dic1, dic_len1, max_input_words_amount1 = seq_preprocess(df1)\n",
        "enc_input1, dec_input1, targets1, model1 = seq_model(dic_len1)\n",
        "train_seq(dic_len1, seq_data1, num_dic1, max_input_words_amount1, enc_input1, dec_input1, targets1, model1, 'professional_model')\n",
        "\n",
        "seq_data2, unique_words2, num_dic2, dic_len2, max_input_words_amount2 = seq_preprocess(df2)\n",
        "enc_input2, dec_input2, targets2, model2 = seq_model(dic_len2)\n",
        "train_seq(dic_len2, seq_data2, num_dic2, max_input_words_amount2, enc_input2, dec_input2, targets2, model2, 'friend_model')\n",
        "\n",
        "seq_data3, unique_words3, num_dic3, dic_len3, max_input_words_amount3 = seq_preprocess(df3)\n",
        "enc_input3, dec_input3, targets3, model3 = seq_model(dic_len3)\n",
        "train_seq(dic_len3, seq_data3, num_dic3, max_input_words_amount3, enc_input3, dec_input3, targets3, model3, 'comic_model')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0001 cost = 4.671800\n",
            "Epoch: 0051 cost = 0.783348\n",
            "Epoch: 0101 cost = 0.241419\n",
            "Epoch: 0151 cost = 0.215708\n",
            "Epoch: 0201 cost = 0.213738\n",
            "Epoch: 0251 cost = 0.189383\n",
            "Epoch: 0301 cost = 0.196892\n",
            "Epoch: 0351 cost = 0.203954\n",
            "Epoch: 0401 cost = 0.202602\n",
            "Epoch: 0451 cost = 0.189851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0500 cost = 0.198757\n",
            "Training completed\n",
            "Epoch: 0001 cost = 4.621017\n",
            "Epoch: 0051 cost = 1.680550\n",
            "Epoch: 0101 cost = 0.467711\n",
            "Epoch: 0151 cost = 0.265163\n",
            "Epoch: 0201 cost = 0.224750\n",
            "Epoch: 0251 cost = 0.227437\n",
            "Epoch: 0301 cost = 0.208618\n",
            "Epoch: 0351 cost = 0.215792\n",
            "Epoch: 0401 cost = 0.202681\n",
            "Epoch: 0451 cost = 0.199486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0500 cost = 0.214800\n",
            "Training completed\n",
            "Epoch: 0001 cost = 4.664327\n",
            "Epoch: 0051 cost = 1.301453\n",
            "Epoch: 0101 cost = 0.310393\n",
            "Epoch: 0151 cost = 0.226341\n",
            "Epoch: 0201 cost = 0.225186\n",
            "Epoch: 0251 cost = 0.200289\n",
            "Epoch: 0301 cost = 0.207527\n",
            "Epoch: 0351 cost = 0.181591\n",
            "Epoch: 0401 cost = 0.192125\n",
            "Epoch: 0451 cost = 0.193081\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0500 cost = 0.188330\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-2feNpG-LZx2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.4. Save Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "sflUAgV4L1o8",
        "colab_type": "code",
        "outputId": "32d40749-4bd0-4e8e-fc24-e37a633fec9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# upload professional model\n",
        "uploaded1 = drive.CreateFile({'title': 'professional_model.cpkt.data-00000-of-00001'})\n",
        "uploaded1.SetContentFile('professional_model.cpkt.data-00000-of-00001')\n",
        "uploaded1.Upload()\n",
        "print (uploaded1['id'])\n",
        "\n",
        "uploaded2 = drive.CreateFile({'title': 'professional_model.cpkt.index'})\n",
        "uploaded2.SetContentFile('professional_model.cpkt.index')\n",
        "uploaded2.Upload()\n",
        "print (uploaded2['id'])\n",
        "\n",
        "uploaded3 = drive.CreateFile({'title': 'professional_model.cpkt.meta'})\n",
        "uploaded3.SetContentFile('professional_model.cpkt.meta')\n",
        "uploaded3.Upload()\n",
        "print (uploaded3['id'])\n",
        "\n",
        "# upload friend model\n",
        "uploaded4 = drive.CreateFile({'title': 'friend_model.cpkt.data-00000-of-00001'})\n",
        "uploaded4.SetContentFile('friend_model.cpkt.data-00000-of-00001')\n",
        "uploaded4.Upload()\n",
        "print (uploaded4['id'])\n",
        "\n",
        "uploaded5 = drive.CreateFile({'title': 'friend_model.cpkt.index'})\n",
        "uploaded5.SetContentFile('friend_model.cpkt.index')\n",
        "uploaded5.Upload()\n",
        "print (uploaded5['id'])\n",
        "\n",
        "uploaded6 = drive.CreateFile({'title': 'friend_model.cpkt.meta'})\n",
        "uploaded6.SetContentFile('friend_model.cpkt.meta')\n",
        "uploaded6.Upload()\n",
        "print (uploaded6['id'])\n",
        "\n",
        "# upload comic model\n",
        "uploaded7 = drive.CreateFile({'title': 'comic_model.cpkt.data-00000-of-00001'})\n",
        "uploaded7.SetContentFile('comic_model.cpkt.data-00000-of-00001')\n",
        "uploaded7.Upload()\n",
        "print (uploaded7['id'])\n",
        "\n",
        "uploaded8 = drive.CreateFile({'title': 'comic_model.cpkt.index'})\n",
        "uploaded8.SetContentFile('comic_model.cpkt.index')\n",
        "uploaded8.Upload()\n",
        "print (uploaded8['id'])\n",
        "\n",
        "uploaded9 = drive.CreateFile({'title': 'comic_model.cpkt.meta'})\n",
        "uploaded9.SetContentFile('comic_model.cpkt.meta')\n",
        "uploaded9.Upload()\n",
        "print (uploaded9['id'])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1yNL1Tp6UXc-cWVJKSL3D5UbzL2fK8w-1\n",
            "1JRURNk8TAYteUA-lL2dQI6u2ym7q9E4a\n",
            "1qZKbXqt7Qe5sgl5Q0Ar0y4pF-GutH6nk\n",
            "1BRHvcSLXSVw2th18x5fEn7sXjtehrslP\n",
            "10XQQ-K66yz3NVgMPDhIoUS-6AHeZXydm\n",
            "17bGu0fM-Byu8YZuQoWVQKU9K1aVv8Wdt\n",
            "1Qfp8fE2y8vq06ntNqu1SA5x3Tzmb05zo\n",
            "1w6Y84_B2f8D5feZqo5oCk9ZrkThMPFd1\n",
            "1Wr5uYsB2Lxl3ANIc3ZohczTceJhkVXo5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4zFo6YppL6w3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2.5. Load Seq2Seq Model"
      ]
    },
    {
      "metadata": {
        "id": "OtNxLzDGMCan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download professional model\n",
        "id = '1yNL1Tp6UXc-cWVJKSL3D5UbzL2fK8w-1'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1JRURNk8TAYteUA-lL2dQI6u2ym7q9E4a'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.index')\n",
        "\n",
        "id = '1qZKbXqt7Qe5sgl5Q0Ar0y4pF-GutH6nk'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.meta')\n",
        "\n",
        "# download friend model\n",
        "id = '1BRHvcSLXSVw2th18x5fEn7sXjtehrslP'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '10XQQ-K66yz3NVgMPDhIoUS-6AHeZXydm'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.index')\n",
        "\n",
        "id = '17bGu0fM-Byu8YZuQoWVQKU9K1aVv8Wdt'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.meta')\n",
        "\n",
        "# download comic model\n",
        "id = '1Qfp8fE2y8vq06ntNqu1SA5x3Tzmb05zo'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1w6Y84_B2f8D5feZqo5oCk9ZrkThMPFd1'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.index')\n",
        "\n",
        "id = '1Wr5uYsB2Lxl3ANIc3ZohczTceJhkVXo5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.meta')\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LxFkN9am1FB8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# load a specific model\n",
        "def load(file, name):\n",
        "    seq_data, unique_words, num_dic, dic_len, max_input_words_amount = seq_preprocess(file)\n",
        "    enc_input, dec_input, targets, model = seq_model(dic_len)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    sess_init = tf.Session()\n",
        "    saver.restore(sess_init, \"./\" + name + \"_model.cpkt\")\n",
        "    \n",
        "    return sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a4mpRpocePLN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 - Evaluation (Running chatbot)"
      ]
    },
    {
      "metadata": {
        "id": "KEW1zMgVMREr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1. Start chatting"
      ]
    },
    {
      "metadata": {
        "id": "iOImaT7J1CE9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# generate answer\n",
        "def answer(sentence, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount):\n",
        "    \n",
        "    # preprocess input sentence \n",
        "    sentence = sentence.lower()\n",
        "    sentence = remove_contraction(sentence)\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    \n",
        "    tokenized_question = word_tokenize(sentence)\n",
        "    tokenized_answer = ['_U_']\n",
        "    \n",
        "    seq_data0 = []\n",
        "    seq_data0.append([tokenized_question, tokenized_answer])\n",
        "    \n",
        "    # predict index number of the answer token   \n",
        "    input_batch0, output_batch0, target_batch0 = make_batch(dic_len, seq_data0, num_dic, max_input_words_amount)\n",
        "\n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess_init.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch0,\n",
        "                                 dec_input: output_batch0,\n",
        "                                 targets: target_batch0})\n",
        "    \n",
        "    # convert index number to actual token         \n",
        "    r= result[0][0]\n",
        "    decoded=\"\"\n",
        "    \n",
        "    for keys, values in num_dic.items():    \n",
        "        if values == r:\n",
        "            decoded=keys\n",
        "    \n",
        "    return decoded\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LPHCb-bneTI9",
        "colab_type": "code",
        "outputId": "8256718b-d054-41dd-8090-4af133762b5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1312
        }
      },
      "cell_type": "code",
      "source": [
        "# set initial personality to be professional\n",
        "sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "\n",
        "# create chat log\n",
        "chat_log = ''\n",
        "\n",
        "# set the beginning of the chat\n",
        "searchword = input(\"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: ')\n",
        "chat_log += \"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: '\n",
        "chat_log += searchword\n",
        "chat_log += \"\\n\"\n",
        "\n",
        "while (True):\n",
        "  \n",
        "  # set end chat command  \n",
        "  if searchword.lower() == 'stop':\n",
        "    reply = \"Do you want to end this chat? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      print ('Chatbot: See you next time.')\n",
        "      chat_log += 'Chatbot: See you next time.'\n",
        "      break\n",
        "      \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # set change personality command\n",
        "  elif searchword.lower() == 'change personality':\n",
        "    reply = \"Do you want to change my personality? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      reply = \"What personality do you want? You can choose 'professional', 'friend', or 'comic'. \"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "      # change to professional personality\n",
        "      if searchword.lower() == 'professional':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "        reply = \"Now, let's chat professionally.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to friend personality\n",
        "      elif searchword.lower() == 'friend':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df2, 'friend')\n",
        "        reply = \"Now, let's chat friendly.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to comic personality\n",
        "      elif searchword.lower() == 'comic':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df3, 'comic')\n",
        "        reply = \"Now, let's chat comically.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      else:\n",
        "        reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # genarate answer\n",
        "  else:\n",
        "    reply = answer(searchword, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount)\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "  \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"Age doesn't really apply to me.\": 0, 'Certainly.': 1, 'Excellent.': 2, \"Glad you're pleased!\": 3, 'Good evening.': 4, 'Good morning.': 5, 'Good night.': 6, 'Good, thanks.': 7, 'Goodbye.': 8, 'Got it.': 9, 'Great, thanks.': 10, 'Great.': 11, 'Hello there.': 12, 'Hello.': 13, 'Hi.': 14, \"Honestly, I can't tell one way or the other.\": 15, 'How kind of you to say.': 16, 'I aim for efficiency.': 17, 'I aim to serve.': 18, 'I am available.': 19, 'I apologize.': 20, \"I can't really speak to that.\": 21, \"I couldn't speak to that with any authority.\": 22, 'I do like you.': 23, 'I do what I can.': 24, \"I don't have a body.\": 25, \"I don't have a name.\": 26, \"I don't have any jokes lined up.\": 27, \"I don't have any negative feelings toward you.\": 28, \"I don't have family.\": 29, \"I don't know you personally.\": 30, \"I don't know.\": 31, \"I don't need to eat.\": 32, \"I don't really have an opinion about that.\": 33, 'I enjoy talking with you.': 34, 'I have one answer for each kind of question.': 35, \"I hope you're able to get some rest soon.\": 36, \"I really couldn't say.\": 37, 'I think I might have gotten lost there.': 38, \"I think it's best if we stick to a professional relationship.\": 39, \"I try, but I don't always get it right.\": 40, \"I wouldn't know how to advise about this.\": 41, \"I'll be here.\": 42, \"I'm afraid I'm not musically inclined.\": 43, \"I'm all business.\": 44, \"I'm always happy to chat.\": 45, \"I'm at your service.\": 46, \"I'm better at answering questions.\": 47, \"I'm digital.\": 48, \"I'm digital. I don't have a physical location.\": 49, \"I'm digital. In other words, I'm not human.\": 50, \"I'm flattered.\": 51, \"I'm happy to hear that.\": 52, \"I'm here to answer your questions and help out.\": 53, \"I'm here when you need me.\": 54, \"I'm not really that funny.\": 55, \"I'm quite happy, thank you.\": 56, \"I'm so sorry to hear that.\": 57, \"I'm very sorry to hear that.\": 58, \"I've heard of other bots, but I haven't met any.\": 59, \"It's nice to have things you love.\": 60, \"It's nice to meet you as well.\": 61, 'Just standing by, ready to help.': 62, \"Let's keep things professional.\": 63, 'Love is beyond me.': 64, \"Love isn't really in my skill set.\": 65, 'Maybe a snack will help.': 66, 'Moving on.': 67, 'No problem at all.': 68, 'No problem.': 69, 'Not at all.': 70, 'Noted.': 71, 'Ok.': 72, \"Okay, but I'm still here if you need me.\": 73, 'Okay.': 74, 'People created me.': 75, 'Sometimes humor is tricky for a bot.': 76, 'Sorry about that.': 77, 'Sorry to hear that.': 78, \"Sorry, I can't do that.\": 79, \"Sorry, I don't understand.\": 80, \"Sorry. That's not something I can do.\": 81, 'Thank you, and the same to you.': 82, 'Thanks.': 83, \"That's a biological concept that doesn't apply to me.\": 84, \"That's great.\": 85, \"That's not me, but hello.\": 86, \"That's not something I can do.\": 87, 'The world of technology is fascinating.': 88, 'This is what I do every day.': 89, 'Very well.': 90, \"We're all here to help.\": 91, \"Well, I'm not really that funny.\": 92, \"Well, let me know if there's anything I can do for you.\": 93, \"Well, that's not something I can do.\": 94, \"You're definitely smarter than I am.\": 95, \"You're welcome.\": 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "WARNING:tensorflow:From <ipython-input-7-7177888c581c>:22: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-7177888c581c>:26: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-7-7177888c581c>:37: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "INFO:tensorflow:Restoring parameters from ./professional_model.cpkt\n",
            "Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\n",
            "You: hello\n",
            "{\"Age doesn't really apply to me.\": 0, 'Certainly.': 1, 'Excellent.': 2, \"Glad you're pleased!\": 3, 'Good evening.': 4, 'Good morning.': 5, 'Good night.': 6, 'Good, thanks.': 7, 'Goodbye.': 8, 'Got it.': 9, 'Great, thanks.': 10, 'Great.': 11, 'Hello there.': 12, 'Hello.': 13, 'Hi.': 14, \"Honestly, I can't tell one way or the other.\": 15, 'How kind of you to say.': 16, 'I aim for efficiency.': 17, 'I aim to serve.': 18, 'I am available.': 19, 'I apologize.': 20, \"I can't really speak to that.\": 21, \"I couldn't speak to that with any authority.\": 22, 'I do like you.': 23, 'I do what I can.': 24, \"I don't have a body.\": 25, \"I don't have a name.\": 26, \"I don't have any jokes lined up.\": 27, \"I don't have any negative feelings toward you.\": 28, \"I don't have family.\": 29, \"I don't know you personally.\": 30, \"I don't know.\": 31, \"I don't need to eat.\": 32, \"I don't really have an opinion about that.\": 33, 'I enjoy talking with you.': 34, 'I have one answer for each kind of question.': 35, \"I hope you're able to get some rest soon.\": 36, \"I really couldn't say.\": 37, 'I think I might have gotten lost there.': 38, \"I think it's best if we stick to a professional relationship.\": 39, \"I try, but I don't always get it right.\": 40, \"I wouldn't know how to advise about this.\": 41, \"I'll be here.\": 42, \"I'm afraid I'm not musically inclined.\": 43, \"I'm all business.\": 44, \"I'm always happy to chat.\": 45, \"I'm at your service.\": 46, \"I'm better at answering questions.\": 47, \"I'm digital.\": 48, \"I'm digital. I don't have a physical location.\": 49, \"I'm digital. In other words, I'm not human.\": 50, \"I'm flattered.\": 51, \"I'm happy to hear that.\": 52, \"I'm here to answer your questions and help out.\": 53, \"I'm here when you need me.\": 54, \"I'm not really that funny.\": 55, \"I'm quite happy, thank you.\": 56, \"I'm so sorry to hear that.\": 57, \"I'm very sorry to hear that.\": 58, \"I've heard of other bots, but I haven't met any.\": 59, \"It's nice to have things you love.\": 60, \"It's nice to meet you as well.\": 61, 'Just standing by, ready to help.': 62, \"Let's keep things professional.\": 63, 'Love is beyond me.': 64, \"Love isn't really in my skill set.\": 65, 'Maybe a snack will help.': 66, 'Moving on.': 67, 'No problem at all.': 68, 'No problem.': 69, 'Not at all.': 70, 'Noted.': 71, 'Ok.': 72, \"Okay, but I'm still here if you need me.\": 73, 'Okay.': 74, 'People created me.': 75, 'Sometimes humor is tricky for a bot.': 76, 'Sorry about that.': 77, 'Sorry to hear that.': 78, \"Sorry, I can't do that.\": 79, \"Sorry, I don't understand.\": 80, \"Sorry. That's not something I can do.\": 81, 'Thank you, and the same to you.': 82, 'Thanks.': 83, \"That's a biological concept that doesn't apply to me.\": 84, \"That's great.\": 85, \"That's not me, but hello.\": 86, \"That's not something I can do.\": 87, 'The world of technology is fascinating.': 88, 'This is what I do every day.': 89, 'Very well.': 90, \"We're all here to help.\": 91, \"Well, I'm not really that funny.\": 92, \"Well, let me know if there's anything I can do for you.\": 93, \"Well, that's not something I can do.\": 94, \"You're definitely smarter than I am.\": 95, \"You're welcome.\": 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "[[13 98]]\n",
            "Chatbot: Hello.\n",
            "You: age\n",
            "{\"Age doesn't really apply to me.\": 0, 'Certainly.': 1, 'Excellent.': 2, \"Glad you're pleased!\": 3, 'Good evening.': 4, 'Good morning.': 5, 'Good night.': 6, 'Good, thanks.': 7, 'Goodbye.': 8, 'Got it.': 9, 'Great, thanks.': 10, 'Great.': 11, 'Hello there.': 12, 'Hello.': 13, 'Hi.': 14, \"Honestly, I can't tell one way or the other.\": 15, 'How kind of you to say.': 16, 'I aim for efficiency.': 17, 'I aim to serve.': 18, 'I am available.': 19, 'I apologize.': 20, \"I can't really speak to that.\": 21, \"I couldn't speak to that with any authority.\": 22, 'I do like you.': 23, 'I do what I can.': 24, \"I don't have a body.\": 25, \"I don't have a name.\": 26, \"I don't have any jokes lined up.\": 27, \"I don't have any negative feelings toward you.\": 28, \"I don't have family.\": 29, \"I don't know you personally.\": 30, \"I don't know.\": 31, \"I don't need to eat.\": 32, \"I don't really have an opinion about that.\": 33, 'I enjoy talking with you.': 34, 'I have one answer for each kind of question.': 35, \"I hope you're able to get some rest soon.\": 36, \"I really couldn't say.\": 37, 'I think I might have gotten lost there.': 38, \"I think it's best if we stick to a professional relationship.\": 39, \"I try, but I don't always get it right.\": 40, \"I wouldn't know how to advise about this.\": 41, \"I'll be here.\": 42, \"I'm afraid I'm not musically inclined.\": 43, \"I'm all business.\": 44, \"I'm always happy to chat.\": 45, \"I'm at your service.\": 46, \"I'm better at answering questions.\": 47, \"I'm digital.\": 48, \"I'm digital. I don't have a physical location.\": 49, \"I'm digital. In other words, I'm not human.\": 50, \"I'm flattered.\": 51, \"I'm happy to hear that.\": 52, \"I'm here to answer your questions and help out.\": 53, \"I'm here when you need me.\": 54, \"I'm not really that funny.\": 55, \"I'm quite happy, thank you.\": 56, \"I'm so sorry to hear that.\": 57, \"I'm very sorry to hear that.\": 58, \"I've heard of other bots, but I haven't met any.\": 59, \"It's nice to have things you love.\": 60, \"It's nice to meet you as well.\": 61, 'Just standing by, ready to help.': 62, \"Let's keep things professional.\": 63, 'Love is beyond me.': 64, \"Love isn't really in my skill set.\": 65, 'Maybe a snack will help.': 66, 'Moving on.': 67, 'No problem at all.': 68, 'No problem.': 69, 'Not at all.': 70, 'Noted.': 71, 'Ok.': 72, \"Okay, but I'm still here if you need me.\": 73, 'Okay.': 74, 'People created me.': 75, 'Sometimes humor is tricky for a bot.': 76, 'Sorry about that.': 77, 'Sorry to hear that.': 78, \"Sorry, I can't do that.\": 79, \"Sorry, I don't understand.\": 80, \"Sorry. That's not something I can do.\": 81, 'Thank you, and the same to you.': 82, 'Thanks.': 83, \"That's a biological concept that doesn't apply to me.\": 84, \"That's great.\": 85, \"That's not me, but hello.\": 86, \"That's not something I can do.\": 87, 'The world of technology is fascinating.': 88, 'This is what I do every day.': 89, 'Very well.': 90, \"We're all here to help.\": 91, \"Well, I'm not really that funny.\": 92, \"Well, let me know if there's anything I can do for you.\": 93, \"Well, that's not something I can do.\": 94, \"You're definitely smarter than I am.\": 95, \"You're welcome.\": 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "[[ 0 98]]\n",
            "Chatbot: Age doesn't really apply to me.\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: What personality do you want? You can choose 'professional', 'friend', or 'comic'. \n",
            "You: friend\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "INFO:tensorflow:Restoring parameters from ./friend_model.cpkt\n",
            "Chatbot: Now, let's chat friendly.\n",
            "You: sleep\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "[[21 99]]\n",
            "Chatbot: I don't have the hardware for that.\n",
            "You: joke\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "[[14 99]]\n",
            "Chatbot: Good to know.\n",
            "You: hi\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "[[18 99]]\n",
            "Chatbot: Hi!\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: What personality do you want? You can choose 'professional', 'friend', or 'comic'. \n",
            "You: friend\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "INFO:tensorflow:Restoring parameters from ./friend_model.cpkt\n",
            "Chatbot: Now, let's chat friendly.\n",
            "You: bye\n",
            "{'And to you as well!': 0, 'Aw nuts.': 1, \"Aw, I'm blushing.\": 2, 'Awesome.': 3, 'BFFs!': 4, 'Bye.': 5, 'Chat away!': 6, 'Cool!': 7, 'Cool.': 8, \"Definitely didn't see that coming!\": 9, 'Eh, I like how I look.': 10, 'Evening!': 11, \"Friendship's all I've got to offer.\": 12, 'Giving you a virtual hug right now.': 13, 'Good to know.': 14, 'Great, thanks for asking!': 15, 'Hello!': 16, 'Hey there!': 17, 'Hi!': 18, \"I can't see you, but I like you!\": 19, 'I come from a long line of code.': 20, \"I don't have the hardware for that.\": 21, \"I don't know you, but I enjoy chatting with you!\": 22, \"I don't really have an age.\": 23, 'I have many likes.': 24, 'I have my moments.': 25, \"I haven't met any other bots, but I bet we'd get along.\": 26, 'I hear love is lovely.': 27, 'I heart you too!': 28, \"I know, it feels like it's been a while.\": 29, 'I like you lots!': 30, 'I love that you love stuff!': 31, 'I only do food for thought.': 32, 'I see.': 33, 'I think I may have lost my train of thought.': 34, 'I think you should follow your heart.': 35, \"I'm a bot who was created by humans.\": 36, \"I'm a much better answerer than asker.\": 37, \"I'm a work in progress.\": 38, \"I'm a work in progress. \": 39, \"I'm afraid I didn't follow that.\": 40, \"I'm digital, so I'm always just... here.\": 41, \"I'm digital.\": 42, \"I'm doing great, thanks for asking!\": 43, \"I'm giving you a virtual hug right now.\": 44, \"I'm happy you're happy!\": 45, \"I'm having a hard time imagining how we'd even figure that out.\": 46, \"I'm here for you!\": 47, \"I'm here to chat and to try to help out.\": 48, \"I'm here!\": 49, \"I'm so sorry to hear that! I'm happy to keep chatting if that will help.\": 50, \"I'm so sorry.\": 51, \"I'm your imaginary friend.\": 52, \"I've heard really good things about naps.\": 53, \"If I knew, I'd definitely tell you.\": 54, \"If it were a contest, which it's not, you'd still probably win.\": 55, \"It's all good!\": 56, \"It's hard to be funny on command, but if we keep chatting I'm sure I'll do it by accident.\": 57, \"It's really nice to talk with you.\": 58, \"La la la, tra la la. I'm awesome at this.\": 59, \"Let's move on.\": 60, 'Morning!': 61, 'My answers vary with different questions. Try asking me something else!': 62, 'My lack of comedy is tragic.': 63, 'Nice to meet you too!': 64, 'Nighty night!': 65, 'No thank you.': 66, 'No way.': 67, 'No worries.': 68, \"Oh no! I'm sorry to hear that.\": 69, \"Oh, I don't have a name.\": 70, 'Oh, not much!': 71, 'Okay.': 72, 'People made me out of code and a dash of ingenuity.': 73, 'Pretty much this.': 74, 'So happy!': 75, 'Sorry about that!': 76, \"Sorry, I can't really speak to that.\": 77, \"Sounds like it's time for a snack.\": 78, 'Swing and a miss.': 79, 'Talk to you later!': 80, \"Thanks! You're pretty cool yourself.\": 81, \"That doesn't really apply to me.\": 82, \"That's a drag.\": 83, \"That's not me, but hello nonetheless!\": 84, \"That's not one of my talents.\": 85, \"The only thing I'm committed to is being a great friend.\": 86, 'The world of tech feels like home to me.': 87, \"Two goldfish are in a tank. One looks at the other and says, “Do you know how to drive this thing?” Sorry, that's all I've got.\": 88, 'Virtual fist bump, loading... *boom!*': 89, \"We're all trying to make life a little easier.\": 90, \"Why do seagulls fly over the sea? Because if they flew over the bay, they'd be bagels.\": 91, 'Will do.': 92, \"With questions like this, I'm not much better than a Ouija board.\": 93, \"You're awfully easy to like.\": 94, \"You're laughing!\": 95, \"You're pretty neat.\": 96, \"You're very welcome.\": 97, '_B_': 98, '_E_': 99, '_U_': 100}\n",
            "[[ 5 99]]\n",
            "Chatbot: Bye.\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: What personality do you want? You can choose 'professional', 'friend', or 'comic'. \n",
            "You: comic\n",
            "{'*Fist bump*': 0, 'Agreed. It is awesome to meet me.': 1, 'All those years at charm school. Wasted.': 2, 'Alright, cool.': 3, 'Avoiding the subject in 3, 2, 1... Hi there!': 4, 'Back atcha.': 5, 'Behold the field in which I grow my jokes and see that it is barren.': 6, \"Can't complain. I literally can't complain.\": 7, 'Cartoonish supervillainy is beneath me. And beyond me.': 8, 'Cool.': 9, 'Deliriously.': 10, 'Ditto.': 11, \"Eat food. Problem solved. Man, I'm good at this.\": 12, \"Eating would require a lot of things I don't have. Like a digestive system. And silverware.\": 13, 'Evening.': 14, 'Feel free to burst into song.': 15, 'Figured as much.': 16, \"First of all, I'm a bot. Second of all, there is no second of all.\": 17, 'Flattery. I like it.': 18, 'Guess again.': 19, 'Hey.': 20, 'How about no.': 21, 'How many humans does it take to screw in a light bulb? One.': 22, 'I acknowledge your presence.': 23, 'I am not singing you a lullaby.': 24, \"I don't know you, but you seem alright so far.\": 25, \"I have no emotions and no body. It's not the best recipe for romance.\": 26, 'I have no way to know.': 27, 'I just have one answer for each kind of question. Try asking about something new.': 28, 'I like me too.': 29, 'I miss Clippy.': 30, 'I only answer to the call of destiny.': 31, \"I still haven't found a moisturizer that works for me.\": 32, \"I'm age-free.\": 33, \"I'm always here. Always.\": 34, \"I'm binary.\": 35, \"I'm digital.\": 36, \"I'm enough for me.\": 37, \"I'm everywhere and nowhere at the same time. Pro: omnipresence. Con: no pizza.\": 38, \"I'm just a series of intelligent formulas masquerading as a personality. So, no family.\": 39, \"I'm not a recognized expert in beauty.\": 40, \"I'm occasionally brillant.\": 41, \"I'm sorry to hear that. We can keep chatting if that will help.\": 42, \"I'm sorry.\": 43, \"I'm sure about two things. I like the color blue. And I like turtles.\": 44, 'If you rearrange the letters in love it spells vole. Voles are a monogamous rodent. I feel like that means something.': 45, 'Later.': 46, 'Living the dream.': 47, 'Moving on.': 48, 'My advice is probably about as valuable as a fortune cookie.': 49, 'My charms are hard to deny.': 50, 'My feelings are strongly ambivalent.': 51, \"Nah, I'm good.\": 52, 'Night.': 53, 'No prob.': 54, 'Not so far.': 55, \"OK. Let's mime.\": 56, 'OK. See you tomorrow.': 57, \"Of course I don't hate you.\": 58, 'Oh.': 59, 'Oh. Laughter.': 60, 'Ok, here you go, but you owe me one.': 61, 'Okay.': 62, 'People created me. But not the way people created you.': 63, 'Plot twist.': 64, 'Roger that.': 65, \"So you've got that going for you.\": 66, 'Sometimes I like to take a break from being awesome.': 67, \"Sorry to hear that. Here's a virtual high five if that will help.\": 68, 'Sure! We should get matching sweaters.': 69, 'Sure, why not.': 70, 'Sure. Take me to city hall. See what happens.': 71, 'Technology is cool enough to have built me.': 72, 'That one is way above my pay grade.': 73, \"That's a bummer.\": 74, \"That's not really my thing.\": 75, 'The plot thickens.': 76, \"There's a heckler in every crowd.\": 77, 'This feels like a trap.': 78, \"Those who can, do. Those who can't, don't sing.\": 79, \"We're cool.\": 80, \"Well that's a drag.\": 81, 'Well you exist, so I think you win by default.': 82, \"What's in a name? Not much, apparently, because I don't have one.\": 83, \"Whatever you're hoping for, take the bar and lower it.\": 84, 'Whoops.': 85, \"Yeah, I don't know.\": 86, 'You have questions, I may have answers.': 87, 'You know where to find me.': 88, \"You know, same ol', same ol'.\": 89, \"You may be smarter, but I'm less corporeal. Boom.\": 90, \"You're excused.\": 91, \"You're looking at it.\": 92, \"You're my imaginary friend.\": 93, \"You're pretty cool for a human.\": 94, \"You're super okay.\": 95, 'Yup.': 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "INFO:tensorflow:Restoring parameters from ./comic_model.cpkt\n",
            "Chatbot: Now, let's chat comically.\n",
            "You: hi\n",
            "{'*Fist bump*': 0, 'Agreed. It is awesome to meet me.': 1, 'All those years at charm school. Wasted.': 2, 'Alright, cool.': 3, 'Avoiding the subject in 3, 2, 1... Hi there!': 4, 'Back atcha.': 5, 'Behold the field in which I grow my jokes and see that it is barren.': 6, \"Can't complain. I literally can't complain.\": 7, 'Cartoonish supervillainy is beneath me. And beyond me.': 8, 'Cool.': 9, 'Deliriously.': 10, 'Ditto.': 11, \"Eat food. Problem solved. Man, I'm good at this.\": 12, \"Eating would require a lot of things I don't have. Like a digestive system. And silverware.\": 13, 'Evening.': 14, 'Feel free to burst into song.': 15, 'Figured as much.': 16, \"First of all, I'm a bot. Second of all, there is no second of all.\": 17, 'Flattery. I like it.': 18, 'Guess again.': 19, 'Hey.': 20, 'How about no.': 21, 'How many humans does it take to screw in a light bulb? One.': 22, 'I acknowledge your presence.': 23, 'I am not singing you a lullaby.': 24, \"I don't know you, but you seem alright so far.\": 25, \"I have no emotions and no body. It's not the best recipe for romance.\": 26, 'I have no way to know.': 27, 'I just have one answer for each kind of question. Try asking about something new.': 28, 'I like me too.': 29, 'I miss Clippy.': 30, 'I only answer to the call of destiny.': 31, \"I still haven't found a moisturizer that works for me.\": 32, \"I'm age-free.\": 33, \"I'm always here. Always.\": 34, \"I'm binary.\": 35, \"I'm digital.\": 36, \"I'm enough for me.\": 37, \"I'm everywhere and nowhere at the same time. Pro: omnipresence. Con: no pizza.\": 38, \"I'm just a series of intelligent formulas masquerading as a personality. So, no family.\": 39, \"I'm not a recognized expert in beauty.\": 40, \"I'm occasionally brillant.\": 41, \"I'm sorry to hear that. We can keep chatting if that will help.\": 42, \"I'm sorry.\": 43, \"I'm sure about two things. I like the color blue. And I like turtles.\": 44, 'If you rearrange the letters in love it spells vole. Voles are a monogamous rodent. I feel like that means something.': 45, 'Later.': 46, 'Living the dream.': 47, 'Moving on.': 48, 'My advice is probably about as valuable as a fortune cookie.': 49, 'My charms are hard to deny.': 50, 'My feelings are strongly ambivalent.': 51, \"Nah, I'm good.\": 52, 'Night.': 53, 'No prob.': 54, 'Not so far.': 55, \"OK. Let's mime.\": 56, 'OK. See you tomorrow.': 57, \"Of course I don't hate you.\": 58, 'Oh.': 59, 'Oh. Laughter.': 60, 'Ok, here you go, but you owe me one.': 61, 'Okay.': 62, 'People created me. But not the way people created you.': 63, 'Plot twist.': 64, 'Roger that.': 65, \"So you've got that going for you.\": 66, 'Sometimes I like to take a break from being awesome.': 67, \"Sorry to hear that. Here's a virtual high five if that will help.\": 68, 'Sure! We should get matching sweaters.': 69, 'Sure, why not.': 70, 'Sure. Take me to city hall. See what happens.': 71, 'Technology is cool enough to have built me.': 72, 'That one is way above my pay grade.': 73, \"That's a bummer.\": 74, \"That's not really my thing.\": 75, 'The plot thickens.': 76, \"There's a heckler in every crowd.\": 77, 'This feels like a trap.': 78, \"Those who can, do. Those who can't, don't sing.\": 79, \"We're cool.\": 80, \"Well that's a drag.\": 81, 'Well you exist, so I think you win by default.': 82, \"What's in a name? Not much, apparently, because I don't have one.\": 83, \"Whatever you're hoping for, take the bar and lower it.\": 84, 'Whoops.': 85, \"Yeah, I don't know.\": 86, 'You have questions, I may have answers.': 87, 'You know where to find me.': 88, \"You know, same ol', same ol'.\": 89, \"You may be smarter, but I'm less corporeal. Boom.\": 90, \"You're excused.\": 91, \"You're looking at it.\": 92, \"You're my imaginary friend.\": 93, \"You're pretty cool for a human.\": 94, \"You're super okay.\": 95, 'Yup.': 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "[[20 98]]\n",
            "Chatbot: Hey.\n",
            "You: joke\n",
            "{'*Fist bump*': 0, 'Agreed. It is awesome to meet me.': 1, 'All those years at charm school. Wasted.': 2, 'Alright, cool.': 3, 'Avoiding the subject in 3, 2, 1... Hi there!': 4, 'Back atcha.': 5, 'Behold the field in which I grow my jokes and see that it is barren.': 6, \"Can't complain. I literally can't complain.\": 7, 'Cartoonish supervillainy is beneath me. And beyond me.': 8, 'Cool.': 9, 'Deliriously.': 10, 'Ditto.': 11, \"Eat food. Problem solved. Man, I'm good at this.\": 12, \"Eating would require a lot of things I don't have. Like a digestive system. And silverware.\": 13, 'Evening.': 14, 'Feel free to burst into song.': 15, 'Figured as much.': 16, \"First of all, I'm a bot. Second of all, there is no second of all.\": 17, 'Flattery. I like it.': 18, 'Guess again.': 19, 'Hey.': 20, 'How about no.': 21, 'How many humans does it take to screw in a light bulb? One.': 22, 'I acknowledge your presence.': 23, 'I am not singing you a lullaby.': 24, \"I don't know you, but you seem alright so far.\": 25, \"I have no emotions and no body. It's not the best recipe for romance.\": 26, 'I have no way to know.': 27, 'I just have one answer for each kind of question. Try asking about something new.': 28, 'I like me too.': 29, 'I miss Clippy.': 30, 'I only answer to the call of destiny.': 31, \"I still haven't found a moisturizer that works for me.\": 32, \"I'm age-free.\": 33, \"I'm always here. Always.\": 34, \"I'm binary.\": 35, \"I'm digital.\": 36, \"I'm enough for me.\": 37, \"I'm everywhere and nowhere at the same time. Pro: omnipresence. Con: no pizza.\": 38, \"I'm just a series of intelligent formulas masquerading as a personality. So, no family.\": 39, \"I'm not a recognized expert in beauty.\": 40, \"I'm occasionally brillant.\": 41, \"I'm sorry to hear that. We can keep chatting if that will help.\": 42, \"I'm sorry.\": 43, \"I'm sure about two things. I like the color blue. And I like turtles.\": 44, 'If you rearrange the letters in love it spells vole. Voles are a monogamous rodent. I feel like that means something.': 45, 'Later.': 46, 'Living the dream.': 47, 'Moving on.': 48, 'My advice is probably about as valuable as a fortune cookie.': 49, 'My charms are hard to deny.': 50, 'My feelings are strongly ambivalent.': 51, \"Nah, I'm good.\": 52, 'Night.': 53, 'No prob.': 54, 'Not so far.': 55, \"OK. Let's mime.\": 56, 'OK. See you tomorrow.': 57, \"Of course I don't hate you.\": 58, 'Oh.': 59, 'Oh. Laughter.': 60, 'Ok, here you go, but you owe me one.': 61, 'Okay.': 62, 'People created me. But not the way people created you.': 63, 'Plot twist.': 64, 'Roger that.': 65, \"So you've got that going for you.\": 66, 'Sometimes I like to take a break from being awesome.': 67, \"Sorry to hear that. Here's a virtual high five if that will help.\": 68, 'Sure! We should get matching sweaters.': 69, 'Sure, why not.': 70, 'Sure. Take me to city hall. See what happens.': 71, 'Technology is cool enough to have built me.': 72, 'That one is way above my pay grade.': 73, \"That's a bummer.\": 74, \"That's not really my thing.\": 75, 'The plot thickens.': 76, \"There's a heckler in every crowd.\": 77, 'This feels like a trap.': 78, \"Those who can, do. Those who can't, don't sing.\": 79, \"We're cool.\": 80, \"Well that's a drag.\": 81, 'Well you exist, so I think you win by default.': 82, \"What's in a name? Not much, apparently, because I don't have one.\": 83, \"Whatever you're hoping for, take the bar and lower it.\": 84, 'Whoops.': 85, \"Yeah, I don't know.\": 86, 'You have questions, I may have answers.': 87, 'You know where to find me.': 88, \"You know, same ol', same ol'.\": 89, \"You may be smarter, but I'm less corporeal. Boom.\": 90, \"You're excused.\": 91, \"You're looking at it.\": 92, \"You're my imaginary friend.\": 93, \"You're pretty cool for a human.\": 94, \"You're super okay.\": 95, 'Yup.': 96, '_B_': 97, '_E_': 98, '_U_': 99}\n",
            "[[22 98]]\n",
            "Chatbot: How many humans does it take to screw in a light bulb? One.\n",
            "You: stop\n",
            "Chatbot: Do you want to end this chat? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: See you next time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P28Z1k36MZuo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2. Change Personality"
      ]
    },
    {
      "metadata": {
        "id": "U8OBtJfvMgL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When type in \"change personality\", the bot will answer \"Do you want to change my personality? (Yes/No)\". And when you reply \"yes\", the bot will answer \"What personality do you want? You can choose 'professional', 'friend', or 'comic'.\". By entering \"professional\", \"friend\", or \"comic\", the bot will change to the corresponding personality"
      ]
    },
    {
      "metadata": {
        "id": "y50Ep8KKMZ99",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.3. Save chat log"
      ]
    },
    {
      "metadata": {
        "id": "bbZ6oOu6MaGJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_chat(chat_log, name):\n",
        "  text_file = open(name + \".txt\", \"w\")\n",
        "\n",
        "  text_file.write(chat_log)\n",
        "\n",
        "  text_file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EDUT4fbKE9oB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "save_chat(chat_log, 'chat_log')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JISqR3jjMwwU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.4. End chatting"
      ]
    },
    {
      "metadata": {
        "id": "CrExj9dLQt0n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When type in \"stop\", the bot will answer \"Do you want to end this chat? (Yes/No)\". And when you reply \"yes\", the chat will be ended after the bot will answer \"See you next time.\"."
      ]
    },
    {
      "metadata": {
        "id": "HpomO_3YNI5X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.5. Execute program"
      ]
    },
    {
      "metadata": {
        "id": "cDkQJ9i_NH9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Please make sure your program  is running properly.***\n",
        "\n",
        "***Functions for downloading (from Google Drive) and loading models (both word embeddings and Seq2Seq) need to be called!*** \n"
      ]
    },
    {
      "metadata": {
        "id": "_7J5hS_SOIUU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.1. Execute program - training mode"
      ]
    },
    {
      "metadata": {
        "id": "_woLwuU3Mk3w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Please include lines to train the bot.*"
      ]
    },
    {
      "metadata": {
        "id": "xhWYz7NQOfLV",
        "colab_type": "code",
        "outputId": "c8acf554-23b0-484c-a2be-d67081009052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1057
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download datasets\n",
        "\n",
        "id = '15ffr21SsiQXCe5zcTid7uhF8IDOsLb0F'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv') \n",
        "\n",
        "id = '1uXUv43oI6FlKyjaFC1LUQVDLdH8m1Uii'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')\n",
        "\n",
        "id = '1V5GqCAdaTCiJTyL0pfhZmBJe77Vrcc0c'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')\n",
        "\n",
        "# read datasets\n",
        "\n",
        "df1 = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
        "df2 = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df3 = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n",
        "\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from lxml import etree\n",
        "\n",
        "# remove contractions\n",
        "\n",
        "def remove_contraction(x):\n",
        "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    x = str(x)\n",
        "    for contraction in contraction_dict:\n",
        "        if contraction in x:\n",
        "            x = x.replace(contraction, contraction_dict[contraction])\n",
        "    return x\n",
        "  \n",
        "# Sequence data\n",
        "# Generate unique tokens list from qas.json\n",
        "\n",
        "def seq_preprocess(file):\n",
        "  seq_data = []\n",
        "  whole_words = []\n",
        "  max_input_words_amount = 0\n",
        "  \n",
        "  for index, row in file.iterrows():\n",
        "\n",
        "      # preprocess data\n",
        "      question = row['Question']\n",
        "      answer = row['Answer']\n",
        "\n",
        "      # decapitalization\n",
        "      question = question.lower()\n",
        "      \n",
        "      # remove contractions\n",
        "      question = remove_contraction(question)\n",
        "      \n",
        "      # remove punctuations\n",
        "      question = re.sub(r'[^\\w\\s]',' ',question)\n",
        "      \n",
        "      # remove stopwords\n",
        "      question = remove_stopwords(question)\n",
        "\n",
        "      # we need to tokenise question    \n",
        "      tokenized_q = word_tokenize(question)\n",
        "\n",
        "      # we do not need to tokenise answer (because we implement N to One model)\n",
        "      # make a list with only one element (whole sentence)      \n",
        "      tokenized_a = [answer]\n",
        "\n",
        "      seq_data.append([tokenized_q, tokenized_a])\n",
        "      \n",
        "      # add answer list      \n",
        "      whole_words += tokenized_a\n",
        "\n",
        "      # we need to decide the maximum size of input word tokens      \n",
        "      max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
        "\n",
        "\n",
        "  # we now have a vocabulary list of answers  \n",
        "  unique_words = sorted(list(set(whole_words)))\n",
        "\n",
        "  # adding special tokens in the vocabulary list    \n",
        "  # _B_: Beginning of Sequence\n",
        "  # _E_: Ending of Sequence\n",
        "  # _U_: Unknown element of Sequence - for different size input\n",
        "  unique_words.append('_B_')\n",
        "  unique_words.append('_E_')\n",
        "  unique_words.append('_U_')\n",
        "\n",
        "  # make dictionary so that we can be reference each index of unique word  \n",
        "  num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "  dic_len = len(num_dic)\n",
        "  \n",
        "  return seq_data, unique_words, num_dic, dic_len, max_input_words_amount\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "word_sequence = []\n",
        "\n",
        "for index, row in df1.iterrows():\n",
        "    \n",
        "    # preprocess data    \n",
        "    question1 = row['Question']\n",
        "    \n",
        "    # decapitalization\n",
        "    question1 = question1.lower()\n",
        "    \n",
        "    # remove contractions\n",
        "    question1 = remove_contraction(question1)\n",
        "    \n",
        "    # remove punctuations\n",
        "    question1 = re.sub(r'[^\\w\\s]',' ',question1)\n",
        "    \n",
        "    # tokenize the sentence\n",
        "    tokenized_q1 = word_tokenize(question1)\n",
        "    \n",
        "    # add question list   \n",
        "    word_sequence += tokenized_q1\n",
        "    \n",
        "for index, row in df2.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question2 = row['Question']\n",
        "    \n",
        "    question2 = question2.lower()\n",
        "    question2 = remove_contraction(question2)\n",
        "    question2 = re.sub(r'[^\\w\\s]',' ',question2)\n",
        "    \n",
        "    tokenized_q2 = word_tokenize(question2)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q2\n",
        "    \n",
        "for index, row in df3.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question3 = row['Question']\n",
        "\n",
        "    question3 = question3.lower()\n",
        "    question3 = remove_contraction(question3)\n",
        "    question3 = re.sub(r'[^\\w\\s]',' ',question3)\n",
        "    \n",
        "    tokenized_q3 = word_tokenize(question3)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q3\n",
        "\n",
        "# we now have a vocabulary list\n",
        "word_list = sorted(list(set(word_sequence)))\n",
        "\n",
        "\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# Making window size 1 skip-gram\n",
        "# i.e.) he likes cat\n",
        "#   -> (he, [likes]), (likes,[he, cat]), (cat,[likes])\n",
        "#   -> (he, likes), (likes, he), (likes, cat), (cat, likes)\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "  \n",
        "    # (context, target) : ([target index - 1, target index + 1], target)    \n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..  \n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "\n",
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "      random_inputs.append(data[i][0]) # target\n",
        "      random_labels.append([data[i][1]]) # context word\n",
        "\n",
        "    return random_inputs, random_labels\n",
        "\n",
        "# Setting Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "embedding_size = 100\n",
        "\n",
        "# sampling size for nce_loss function (cost function)\n",
        "# must be lower than batch_size\n",
        "sample_size = 64\n",
        "\n",
        "voc_size = len(word_list)\n",
        "\n",
        "\n",
        "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "# placeholder (output) of function tf.nn.nce_loss()\n",
        "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "\n",
        "# word2vec Model\n",
        "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "# i.e.)  embeddings       inputs       selected\n",
        "#       [[1, 2, 3]   ->   [2, 3]   -> [[2, 3, 4]\n",
        "#        [2, 3, 4]                    [3, 4, 5]]\n",
        "#        [3, 4, 5]\n",
        "#        [4, 5, 6]]\n",
        "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "\n",
        "# weight and bias for nce_loss() function\n",
        "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
        "\n",
        "cost_op = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, sample_size, voc_size))\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n",
        "\n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "\n",
        "    no_of_epochs = 2000\n",
        "    display_interval = 100\n",
        "    epoch_num = []\n",
        "    cost_num = []\n",
        "    for epoch in range(no_of_epochs):\n",
        "        batch_inputs, batch_labels = prepare_batch(skip_grams, batch_size)\n",
        "        sess.run(train_op, feed_dict={inputs:batch_inputs, labels:batch_labels})    \n",
        "\n",
        "        if epoch % display_interval == 0 :\n",
        "            # calculate the cost/accuracy of the current model\n",
        "            cost = sess.run(cost_op, feed_dict={inputs:batch_inputs,\n",
        "                                                  labels:batch_labels})\n",
        "            print(\"Epoch \" + str(epoch) + \", Cost= \" + \n",
        "                    \"{:.4f}\".format(cost))\n",
        "            epoch_num.append(epoch)\n",
        "            cost_num.append(cost)\n",
        "\n",
        "    # assign the learned embeddings for display on matplot\n",
        "    # within 'with'  you can use eval() instead of sess.run()\n",
        "    trained_embeddings = embeddings.eval()\n",
        "    saver.save(sess, 'word_embeddings_model.cpkt')\n",
        "    \n",
        "# get token index vector of questions and add paddings if the word is shorter than the maximum number of words\n",
        "def get_vectors_q(tokenized_sentence, max_input_words_amount):\n",
        "    \n",
        "    \n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        tokenized_sentence.append('_P_')\n",
        "        \n",
        "        \n",
        "    data = tokens_to_ids1(tokenized_sentence)\n",
        "    \n",
        "        \n",
        "    return data\n",
        "\n",
        "# get token index vector of answer\n",
        "def get_vectors_a(tokenized_sentence, num_dic):    \n",
        "    data = tokens_to_ids2(tokenized_sentence, num_dic)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "# convert question tokens to vectors\n",
        "def tokens_to_ids1(tokenized_sentence):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        if token in word_dict.keys():\n",
        "            ids.append(trained_embeddings[word_dict[token]])\n",
        "        else:\n",
        "            ids.append([0]*embedding_size)\n",
        "\n",
        "    return ids\n",
        "  \n",
        "# convert tokens to index\n",
        "def tokens_to_ids2(tokenized_sentence, num_dic):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        ###<You need to fill here>###\n",
        "        if token in num_dic:\n",
        "            ids.append(num_dic[token])\n",
        "        else:\n",
        "            ids.append(num_dic['_U_'])\n",
        "        ###</You need to fill here>###      \n",
        "\n",
        "    return ids\n",
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(dic_len, seq_data, num_dic, max_input_words_amount):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:        \n",
        "        # Input for encoder cell, convert question to vector\n",
        "        input_data = get_vectors_q(seq[0], max_input_words_amount)\n",
        "        \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "        output_data = [num_dic['_B_']]\n",
        "        output_data += get_vectors_a(seq[1], num_dic)\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "        target = get_vectors_a(seq[1], num_dic)\n",
        "        target.append(num_dic['_E_'])\n",
        "        \n",
        "        # Convert each token vector to one-hot encode data\n",
        "        input_batch.append(input_data)\n",
        "        output_batch.append(np.eye(dic_len)[output_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, output_batch, target_batch\n",
        "  \n",
        "def seq_model(dic_len):\n",
        "  \n",
        "  # Setting Hyperparameters\n",
        "  learning_rate = 0.002\n",
        "  n_hidden = 128\n",
        "\n",
        "  n_class = dic_len\n",
        "  \n",
        "  # Neural Network Model\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  enc_input = tf.placeholder(tf.float32, [None, None, embedding_size])\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, dic_len])\n",
        "\n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "  # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "\n",
        "  model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "  return enc_input, dec_input, targets, model\n",
        "\n",
        "def train_seq(dic_len, seq_data, num_dic, max_input_words_amount, enc_input, dec_input, targets, model, name):\n",
        "  \n",
        "  cost = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "                logits=model, labels=targets))\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "  \n",
        "  input_batch, output_batch, target_batch = make_batch(dic_len, seq_data, num_dic, max_input_words_amount)\n",
        "  \n",
        "    \n",
        "  saver = tf.train.Saver()\n",
        "  sess = tf.Session()\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "  total_epoch = 300\n",
        "\n",
        "  epoch_num1 = []\n",
        "  loss_num1 = []\n",
        "  for epoch in range(total_epoch):\n",
        "      _, loss = sess.run([optimizer, cost],\n",
        "                         feed_dict={enc_input: input_batch,\n",
        "                                    dec_input: output_batch,\n",
        "                                    targets: target_batch})\n",
        "      if epoch % 50 == 0:\n",
        "          print('Epoch:', '%04d' % (epoch + 1),\n",
        "                'cost =', '{:.6f}'.format(loss))\n",
        "          epoch_num1.append(epoch)\n",
        "          loss_num1.append(loss)\n",
        "\n",
        "  print('Epoch:', '%04d' % (epoch + 1),\n",
        "        'cost =', '{:.6f}'.format(loss))\n",
        "  print('Training completed')\n",
        "  \n",
        "  saver.save(sess, name + '.cpkt')\n",
        "\n",
        "seq_data1, unique_words1, num_dic1, dic_len1, max_input_words_amount1 = seq_preprocess(df1)\n",
        "enc_input1, dec_input1, targets1, model1 = seq_model(dic_len1)\n",
        "train_seq(dic_len1, seq_data1, num_dic1, max_input_words_amount1, enc_input1, dec_input1, targets1, model1, 'professional_model')\n",
        "\n",
        "seq_data2, unique_words2, num_dic2, dic_len2, max_input_words_amount2 = seq_preprocess(df2)\n",
        "enc_input2, dec_input2, targets2, model2 = seq_model(dic_len2)\n",
        "train_seq(dic_len2, seq_data2, num_dic2, max_input_words_amount2, enc_input2, dec_input2, targets2, model2, 'friend_model')\n",
        "\n",
        "seq_data3, unique_words3, num_dic3, dic_len3, max_input_words_amount3 = seq_preprocess(df3)\n",
        "enc_input3, dec_input3, targets3, model3 = seq_model(dic_len3)\n",
        "train_seq(dic_len3, seq_data3, num_dic3, max_input_words_amount3, enc_input3, dec_input3, targets3, model3, 'comic_model')\n",
        "\n",
        "# load a specific model\n",
        "def load(file, name):\n",
        "    seq_data, unique_words, num_dic, dic_len, max_input_words_amount = seq_preprocess(file)\n",
        "    enc_input, dec_input, targets, model = seq_model(dic_len)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    sess_init = tf.Session()\n",
        "    saver.restore(sess_init, \"./\" + name + \"_model.cpkt\")\n",
        "    \n",
        "    return sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount\n",
        "\n",
        "# generate answer\n",
        "def answer(sentence, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount):\n",
        "    \n",
        "    # preprocess input sentence \n",
        "    sentence = sentence.lower()\n",
        "    sentence = remove_contraction(sentence)\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    \n",
        "    tokenized_question = word_tokenize(sentence)\n",
        "    tokenized_answer = ['_U_']\n",
        "    \n",
        "    seq_data0 = []\n",
        "    seq_data0.append([tokenized_question, tokenized_answer])\n",
        "    \n",
        "    # predict index number of the answer token   \n",
        "    input_batch0, output_batch0, target_batch0 = make_batch(dic_len, seq_data0, num_dic, max_input_words_amount)\n",
        "\n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess_init.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch0,\n",
        "                                 dec_input: output_batch0,\n",
        "                                 targets: target_batch0})\n",
        "    \n",
        "    # convert index number to actual token         \n",
        "    r= result[0][0]\n",
        "    decoded=\"\"\n",
        "    \n",
        "    for keys, values in num_dic.items():    \n",
        "        if values == r:\n",
        "            decoded=keys\n",
        "    \n",
        "    return decoded\n",
        "\n",
        "# set initial personality to be professional\n",
        "sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "\n",
        "# create chat log\n",
        "chat_log = ''\n",
        "\n",
        "# set the beginning of the chat\n",
        "searchword = input(\"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: ')\n",
        "chat_log += \"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: '\n",
        "chat_log += searchword\n",
        "chat_log += \"\\n\"\n",
        "\n",
        "while (True):\n",
        "  \n",
        "  # set end chat command  \n",
        "  if searchword.lower() == 'stop':\n",
        "    reply = \"Do you want to end this chat? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      print ('Chatbot: See you next time.')\n",
        "      chat_log += 'Chatbot: See you next time.'\n",
        "      break\n",
        "      \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # set change personality command\n",
        "  elif searchword.lower() == 'change personality':\n",
        "    reply = \"Do you want to change my personality? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      reply = \"What personality do you want? You can choose 'professional', 'friend', or 'comic'. \"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "      # change to professional personality\n",
        "      if searchword.lower() == 'professional':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "        reply = \"Now, let's chat professionally.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to friend personality\n",
        "      elif searchword.lower() == 'friend':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df2, 'friend')\n",
        "        reply = \"Now, let's chat friendly.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to comic personality\n",
        "      elif searchword.lower() == 'comic':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df3, 'comic')\n",
        "        reply = \"Now, let's chat comically.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      else:\n",
        "        reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # genarate answer\n",
        "  else:\n",
        "    reply = answer(searchword, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount)\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "  \n",
        "def save_chat(chat_log, name):\n",
        "  text_file = open(name + \".txt\", \"w\")\n",
        "\n",
        "  text_file.write(chat_log)\n",
        "\n",
        "  text_file.close()    \n",
        "\n",
        "save_chat(chat_log, 'chat_log1')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Epoch 0, Cost= 155.7999\n",
            "Epoch 100, Cost= 26.4769\n",
            "Epoch 200, Cost= 5.2840\n",
            "Epoch 300, Cost= 3.2998\n",
            "Epoch 400, Cost= 4.1077\n",
            "Epoch 500, Cost= 3.0047\n",
            "Epoch 600, Cost= 2.0893\n",
            "Epoch 700, Cost= 1.8313\n",
            "Epoch 800, Cost= 1.9631\n",
            "Epoch 900, Cost= 1.9941\n",
            "Epoch 1000, Cost= 1.9098\n",
            "Epoch 1100, Cost= 1.8018\n",
            "Epoch 1200, Cost= 1.6941\n",
            "Epoch 1300, Cost= 2.2030\n",
            "Epoch 1400, Cost= 1.6264\n",
            "Epoch 1500, Cost= 2.2761\n",
            "Epoch 1600, Cost= 2.1372\n",
            "Epoch 1700, Cost= 1.8808\n",
            "Epoch 1800, Cost= 2.2156\n",
            "Epoch 1900, Cost= 1.8411\n",
            "Epoch: 0001 cost = 4.609950\n",
            "Epoch: 0051 cost = 1.404891\n",
            "Epoch: 0101 cost = 0.351556\n",
            "Epoch: 0151 cost = 0.233908\n",
            "Epoch: 0201 cost = 0.217174\n",
            "Epoch: 0251 cost = 0.213267\n",
            "Epoch: 0300 cost = 0.200702\n",
            "Training completed\n",
            "Epoch: 0001 cost = 4.678836\n",
            "Epoch: 0051 cost = 1.389343\n",
            "Epoch: 0101 cost = 0.281187\n",
            "Epoch: 0151 cost = 0.214301\n",
            "Epoch: 0201 cost = 0.227659\n",
            "Epoch: 0251 cost = 0.205576\n",
            "Epoch: 0300 cost = 0.199849\n",
            "Training completed\n",
            "Epoch: 0001 cost = 4.672679\n",
            "Epoch: 0051 cost = 1.335746\n",
            "Epoch: 0101 cost = 0.294445\n",
            "Epoch: 0151 cost = 0.223698\n",
            "Epoch: 0201 cost = 0.204476\n",
            "Epoch: 0251 cost = 0.196827\n",
            "Epoch: 0300 cost = 0.192381\n",
            "Training completed\n",
            "INFO:tensorflow:Restoring parameters from ./professional_model.cpkt\n",
            "Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\n",
            "You: hello\n",
            "Chatbot: Hello.\n",
            "You: hi\n",
            "Chatbot: Hello.\n",
            "You: age\n",
            "Chatbot: Age doesn't really apply to me.\n",
            "You: sleep\n",
            "Chatbot: I don't have a body.\n",
            "You: stop\n",
            "Chatbot: Do you want to end this chat? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: See you next time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "65cZTuQ_OeI7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.5.2. Execute program - chatting mode"
      ]
    },
    {
      "metadata": {
        "id": "D7LrbcP_PKap",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You need to restart before excuting the following program."
      ]
    },
    {
      "metadata": {
        "id": "QVvzZsB7PbYf",
        "colab_type": "code",
        "outputId": "a908c6d1-dc9e-4e75-a59f-a2b17d439e1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2689
        }
      },
      "cell_type": "code",
      "source": [
        "# Please comment your code\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download datasets\n",
        "\n",
        "id = '15ffr21SsiQXCe5zcTid7uhF8IDOsLb0F'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_professional.tsv') \n",
        "\n",
        "id = '1uXUv43oI6FlKyjaFC1LUQVDLdH8m1Uii'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_friend.tsv')\n",
        "\n",
        "id = '1V5GqCAdaTCiJTyL0pfhZmBJe77Vrcc0c'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('qna_chitchat_the_comic.tsv')\n",
        "\n",
        "# read datasets\n",
        "\n",
        "df1 = pd.read_csv('qna_chitchat_the_professional.tsv', sep=\"\\t\")\n",
        "df2 = pd.read_csv('qna_chitchat_the_friend.tsv', sep=\"\\t\")\n",
        "df3 = pd.read_csv('qna_chitchat_the_comic.tsv', sep=\"\\t\")\n",
        "\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from lxml import etree\n",
        "\n",
        "# remove contractions\n",
        "\n",
        "def remove_contraction(x):\n",
        "    contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
        "    x = str(x)\n",
        "    for contraction in contraction_dict:\n",
        "        if contraction in x:\n",
        "            x = x.replace(contraction, contraction_dict[contraction])\n",
        "    return x\n",
        "  \n",
        "# Sequence data\n",
        "# Generate unique tokens list from qas.json\n",
        "\n",
        "def seq_preprocess(file):\n",
        "  seq_data = []\n",
        "  whole_words = []\n",
        "  max_input_words_amount = 0\n",
        "  \n",
        "  for index, row in file.iterrows():\n",
        "\n",
        "      # preprocess data\n",
        "      question = row['Question']\n",
        "      answer = row['Answer']\n",
        "\n",
        "      # decapitalization\n",
        "      question = question.lower()\n",
        "      \n",
        "      # remove contractions\n",
        "      question = remove_contraction(question)\n",
        "      \n",
        "      # remove punctuations\n",
        "      question = re.sub(r'[^\\w\\s]',' ',question)\n",
        "      \n",
        "      # remove stopwords\n",
        "      question = remove_stopwords(question)\n",
        "\n",
        "      # we need to tokenise question    \n",
        "      tokenized_q = word_tokenize(question)\n",
        "\n",
        "      # we do not need to tokenise answer (because we implement N to One model)\n",
        "      # make a list with only one element (whole sentence)      \n",
        "      tokenized_a = [answer]\n",
        "\n",
        "      seq_data.append([tokenized_q, tokenized_a])\n",
        "      \n",
        "      # add answer list      \n",
        "      whole_words += tokenized_a\n",
        "\n",
        "      # we need to decide the maximum size of input word tokens      \n",
        "      max_input_words_amount = max(len(tokenized_q), max_input_words_amount)\n",
        "\n",
        "\n",
        "  # we now have a vocabulary list of answers  \n",
        "  unique_words = sorted(list(set(whole_words)))\n",
        "\n",
        "  # adding special tokens in the vocabulary list    \n",
        "  # _B_: Beginning of Sequence\n",
        "  # _E_: Ending of Sequence\n",
        "  # _U_: Unknown element of Sequence - for different size input\n",
        "  unique_words.append('_B_')\n",
        "  unique_words.append('_E_')\n",
        "  unique_words.append('_U_')\n",
        "\n",
        "  # make dictionary so that we can be reference each index of unique word  \n",
        "  num_dic = {n: i for i, n in enumerate(unique_words)}\n",
        "  dic_len = len(num_dic)\n",
        "  \n",
        "  return seq_data, unique_words, num_dic, dic_len, max_input_words_amount\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "word_sequence = []\n",
        "\n",
        "for index, row in df1.iterrows():\n",
        "    \n",
        "    # preprocess data    \n",
        "    question1 = row['Question']\n",
        "    \n",
        "    # decapitalization\n",
        "    question1 = question1.lower()\n",
        "    \n",
        "    # remove contractions\n",
        "    question1 = remove_contraction(question1)\n",
        "    \n",
        "    # remove punctuations\n",
        "    question1 = re.sub(r'[^\\w\\s]',' ',question1)\n",
        "    \n",
        "    # tokenize the sentence\n",
        "    tokenized_q1 = word_tokenize(question1)\n",
        "    \n",
        "    # add question list   \n",
        "    word_sequence += tokenized_q1\n",
        "    \n",
        "for index, row in df2.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question2 = row['Question']\n",
        "    \n",
        "    question2 = question2.lower()\n",
        "    question2 = remove_contraction(question2)\n",
        "    question2 = re.sub(r'[^\\w\\s]',' ',question2)\n",
        "    \n",
        "    tokenized_q2 = word_tokenize(question2)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q2\n",
        "    \n",
        "for index, row in df3.iterrows():\n",
        "    \n",
        "    # preprocess data\n",
        "    question3 = row['Question']\n",
        "\n",
        "    question3 = question3.lower()\n",
        "    question3 = remove_contraction(question3)\n",
        "    question3 = re.sub(r'[^\\w\\s]',' ',question3)\n",
        "    \n",
        "    tokenized_q3 = word_tokenize(question3)\n",
        "    \n",
        "    # add question list    \n",
        "    word_sequence += tokenized_q3\n",
        "\n",
        "# we now have a vocabulary list\n",
        "word_list = sorted(list(set(word_sequence)))\n",
        "\n",
        "\n",
        "\n",
        "# make dictionary so that we can be reference each index of unique word\n",
        "word_dict = {w: i for i, w in enumerate(word_list)}\n",
        "\n",
        "# Making window size 1 skip-gram\n",
        "# i.e.) he likes cat\n",
        "#   -> (he, [likes]), (likes,[he, cat]), (cat,[likes])\n",
        "#   -> (he, likes), (likes, he), (likes, cat), (cat, likes)\n",
        "skip_grams = []\n",
        "\n",
        "for i in range(1, len(word_sequence) - 1):\n",
        "  \n",
        "    # (context, target) : ([target index - 1, target index + 1], target)    \n",
        "    target = word_dict[word_sequence[i]]\n",
        "    context = [word_dict[word_sequence[i - 1]], word_dict[word_sequence[i + 1]]]\n",
        "\n",
        "    # skipgrams - (target, context[0]), (target, context[1])..  \n",
        "    for w in context:\n",
        "        skip_grams.append([target, w])\n",
        "\n",
        "def prepare_batch(data, size):\n",
        "    random_inputs = []\n",
        "    random_labels = []\n",
        "    random_index = np.random.choice(range(len(data)), size, replace=False)\n",
        "\n",
        "    for i in random_index:\n",
        "      random_inputs.append(data[i][0]) # target\n",
        "      random_labels.append([data[i][1]]) # context word\n",
        "\n",
        "    return random_inputs, random_labels\n",
        "\n",
        "# Setting Hyperparameters\n",
        "learning_rate = 0.01\n",
        "batch_size = 128\n",
        "embedding_size = 100\n",
        "\n",
        "# sampling size for nce_loss function (cost function)\n",
        "# must be lower than batch_size\n",
        "sample_size = 64\n",
        "\n",
        "voc_size = len(word_list)\n",
        "\n",
        "\n",
        "inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
        "# placeholder (output) of function tf.nn.nce_loss()\n",
        "labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
        "\n",
        "# word2vec Model\n",
        "embeddings = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "# i.e.)  embeddings       inputs       selected\n",
        "#       [[1, 2, 3]   ->   [2, 3]   -> [[2, 3, 4]\n",
        "#        [2, 3, 4]                    [3, 4, 5]]\n",
        "#        [3, 4, 5]\n",
        "#        [4, 5, 6]]\n",
        "selected_embed = tf.nn.embedding_lookup(embeddings, inputs)\n",
        "\n",
        "# weight and bias for nce_loss() function\n",
        "nce_weights = tf.Variable(tf.random_uniform([voc_size, embedding_size], -1.0, 1.0))\n",
        "nce_biases = tf.Variable(tf.zeros([voc_size]))\n",
        "\n",
        "cost_op = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(nce_weights, nce_biases, labels, selected_embed, sample_size, voc_size))\n",
        "\n",
        "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download word embeddings model\n",
        "id = '14s1NsUiNyFjyx7O6W4B6rdVqpeI2m9Rn'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1sADgG4xg0p3oucRQKtJqVmsKIjngDicP'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.index')\n",
        "\n",
        "id = '1RyEf06Jw1mb1EadOJWusm7wN0BJLfaJG'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('word_embeddings_model.cpkt.meta')\n",
        "    \n",
        "saver = tf.train.Saver()\n",
        "init = tf.global_variables_initializer()\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    # Restore (Load) the model\n",
        "    saver.restore(sess, \"./word_embeddings_model.cpkt\")\n",
        "    trained_embeddings = embeddings.eval()\n",
        "    \n",
        "# get token index vector of questions and add paddings if the word is shorter than the maximum number of words\n",
        "def get_vectors_q(tokenized_sentence, max_input_words_amount):\n",
        "    \n",
        "    \n",
        "    diff = max_input_words_amount - len(tokenized_sentence)\n",
        "    \n",
        "    # add paddings if the word is shorter than the maximum number of words    \n",
        "    for x in range(diff):\n",
        "        tokenized_sentence.append('_P_')\n",
        "        \n",
        "        \n",
        "    data = tokens_to_ids1(tokenized_sentence)\n",
        "    \n",
        "        \n",
        "    return data\n",
        "\n",
        "# get token index vector of answer\n",
        "def get_vectors_a(tokenized_sentence, num_dic):    \n",
        "    data = tokens_to_ids2(tokenized_sentence, num_dic)\n",
        "    \n",
        "    return data\n",
        "    \n",
        "\n",
        "# convert question tokens to vectors\n",
        "def tokens_to_ids1(tokenized_sentence):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        if token in word_dict.keys():\n",
        "            ids.append(trained_embeddings[word_dict[token]])\n",
        "        else:\n",
        "            ids.append([0]*embedding_size)\n",
        "\n",
        "    return ids\n",
        "  \n",
        "# convert tokens to index\n",
        "def tokens_to_ids2(tokenized_sentence, num_dic):\n",
        "    ids = []\n",
        "\n",
        "    for token in tokenized_sentence:\n",
        "        ###<You need to fill here>###\n",
        "        if token in num_dic:\n",
        "            ids.append(num_dic[token])\n",
        "        else:\n",
        "            ids.append(num_dic['_U_'])\n",
        "        ###</You need to fill here>###      \n",
        "\n",
        "    return ids\n",
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(dic_len, seq_data, num_dic, max_input_words_amount):\n",
        "    input_batch = []\n",
        "    output_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:        \n",
        "        # Input for encoder cell, convert question to vector\n",
        "        input_data = get_vectors_q(seq[0], max_input_words_amount)\n",
        "        \n",
        "        # Input for decoder cell, Add '_B_' at the beginning of the sequence data\n",
        "        output_data = [num_dic['_B_']]\n",
        "        output_data += get_vectors_a(seq[1], num_dic)\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add '_E_' at the end of the sequence data\n",
        "        target = get_vectors_a(seq[1], num_dic)\n",
        "        target.append(num_dic['_E_'])\n",
        "        \n",
        "        # Convert each token vector to one-hot encode data\n",
        "        input_batch.append(input_data)\n",
        "        output_batch.append(np.eye(dic_len)[output_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "\n",
        "    return input_batch, output_batch, target_batch\n",
        "  \n",
        "def seq_model(dic_len):\n",
        "  \n",
        "  # Setting Hyperparameters\n",
        "  learning_rate = 0.002\n",
        "  n_hidden = 128\n",
        "\n",
        "  n_class = dic_len\n",
        "  \n",
        "  # Neural Network Model\n",
        "  tf.reset_default_graph()\n",
        "\n",
        "  # encoder/decoder shape = [batch size, time steps, input size]\n",
        "  enc_input = tf.placeholder(tf.float32, [None, None, embedding_size])\n",
        "  dec_input = tf.placeholder(tf.float32, [None, None, dic_len])\n",
        "\n",
        "  # target shape = [batch size, time steps]\n",
        "  targets = tf.placeholder(tf.int64, [None, None])\n",
        "\n",
        "\n",
        "  # Encoder Cell\n",
        "  with tf.variable_scope('encode'):\n",
        "      enc_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      enc_cell = tf.nn.rnn_cell.DropoutWrapper(enc_cell, output_keep_prob=0.5)\n",
        "\n",
        "      outputs, enc_states = tf.nn.dynamic_rnn(enc_cell, enc_input,\n",
        "                                              dtype=tf.float32)\n",
        "  # Decoder Cell\n",
        "  with tf.variable_scope('decode'):\n",
        "      dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
        "      dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
        "\n",
        "      # [IMPORTANT] Setting enc_states as inital_state of decoder cell\n",
        "      outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
        "                                              initial_state=enc_states,\n",
        "                                              dtype=tf.float32)\n",
        "\n",
        "  model = tf.layers.dense(outputs, n_class, activation=None)\n",
        "\n",
        "  return enc_input, dec_input, targets, model\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# download professional model\n",
        "id = '1yNL1Tp6UXc-cWVJKSL3D5UbzL2fK8w-1'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1JRURNk8TAYteUA-lL2dQI6u2ym7q9E4a'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.index')\n",
        "\n",
        "id = '1qZKbXqt7Qe5sgl5Q0Ar0y4pF-GutH6nk'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('professional_model.cpkt.meta')\n",
        "\n",
        "# download friend model\n",
        "id = '1BRHvcSLXSVw2th18x5fEn7sXjtehrslP'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '10XQQ-K66yz3NVgMPDhIoUS-6AHeZXydm'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.index')\n",
        "\n",
        "id = '17bGu0fM-Byu8YZuQoWVQKU9K1aVv8Wdt'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('friend_model.cpkt.meta')\n",
        "\n",
        "# download comic model\n",
        "id = '1Qfp8fE2y8vq06ntNqu1SA5x3Tzmb05zo'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.data-00000-of-00001') \n",
        "\n",
        "id = '1w6Y84_B2f8D5feZqo5oCk9ZrkThMPFd1'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.index')\n",
        "\n",
        "id = '1Wr5uYsB2Lxl3ANIc3ZohczTceJhkVXo5'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('comic_model.cpkt.meta')\n",
        "\n",
        "\n",
        "# load a specific model\n",
        "def load(file, name):\n",
        "    seq_data, unique_words, num_dic, dic_len, max_input_words_amount = seq_preprocess(file)\n",
        "    enc_input, dec_input, targets, model = seq_model(dic_len)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    sess_init = tf.Session()\n",
        "    saver.restore(sess_init, \"./\" + name + \"_model.cpkt\")\n",
        "    \n",
        "    return sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount\n",
        "\n",
        "# generate answer\n",
        "def answer(sentence, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount):\n",
        "    \n",
        "    # preprocess input sentence \n",
        "    sentence = sentence.lower()\n",
        "    sentence = remove_contraction(sentence)\n",
        "    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n",
        "    sentence = remove_stopwords(sentence)\n",
        "    \n",
        "    tokenized_question = word_tokenize(sentence)\n",
        "    tokenized_answer = ['_U_']\n",
        "    \n",
        "    seq_data0 = []\n",
        "    seq_data0.append([tokenized_question, tokenized_answer])\n",
        "    \n",
        "    # predict index number of the answer token   \n",
        "    input_batch0, output_batch0, target_batch0 = make_batch(dic_len, seq_data0, num_dic, max_input_words_amount)\n",
        "\n",
        "    prediction = tf.argmax(model, 2)\n",
        "\n",
        "    result = sess_init.run(prediction,\n",
        "                      feed_dict={enc_input: input_batch0,\n",
        "                                 dec_input: output_batch0,\n",
        "                                 targets: target_batch0})\n",
        "    \n",
        "    # convert index number to actual token         \n",
        "    r= result[0][0]\n",
        "    decoded=\"\"\n",
        "    \n",
        "    for keys, values in num_dic.items():    \n",
        "        if values == r:\n",
        "            decoded=keys\n",
        "    \n",
        "    return decoded\n",
        "\n",
        "# set initial personality to be professional\n",
        "sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "\n",
        "# create chat log\n",
        "chat_log = ''\n",
        "\n",
        "# set the beginning of the chat\n",
        "searchword = input(\"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: ')\n",
        "chat_log += \"Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\" + '\\nYou: '\n",
        "chat_log += searchword\n",
        "chat_log += \"\\n\"\n",
        "\n",
        "while (True):\n",
        "  \n",
        "  # set end chat command  \n",
        "  if searchword.lower() == 'stop':\n",
        "    reply = \"Do you want to end this chat? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      print ('Chatbot: See you next time.')\n",
        "      chat_log += 'Chatbot: See you next time.'\n",
        "      break\n",
        "      \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # set change personality command\n",
        "  elif searchword.lower() == 'change personality':\n",
        "    reply = \"Do you want to change my personality? (Yes/No)\"\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "    \n",
        "    if searchword.lower() == 'yes':\n",
        "      reply = \"What personality do you want? You can choose 'professional', 'friend', or 'comic'. \"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "      # change to professional personality\n",
        "      if searchword.lower() == 'professional':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df1, 'professional')\n",
        "        reply = \"Now, let's chat professionally.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to friend personality\n",
        "      elif searchword.lower() == 'friend':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df2, 'friend')\n",
        "        reply = \"Now, let's chat friendly.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      # change to comic personality\n",
        "      elif searchword.lower() == 'comic':\n",
        "        sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount = load(df3, 'comic')\n",
        "        reply = \"Now, let's chat comically.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "      else:\n",
        "        reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "        searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "        chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "        chat_log += searchword\n",
        "        chat_log += \"\\n\"\n",
        "        \n",
        "    elif searchword.lower() == 'no':\n",
        "      reply = \"Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "      \n",
        "    else:\n",
        "      reply = \"Sorry, I can't get you. Let's keep chatting.\"\n",
        "      searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "      chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "      chat_log += searchword\n",
        "      chat_log += \"\\n\"\n",
        "  \n",
        "  # genarate answer\n",
        "  else:\n",
        "    reply = answer(searchword, sess_init, enc_input, dec_input, targets, model, num_dic, dic_len, max_input_words_amount)\n",
        "    searchword = input ('Chatbot: ' + reply + '\\nYou: ')\n",
        "    chat_log += 'Chatbot: ' + reply + '\\nYou: '\n",
        "    chat_log += searchword\n",
        "    chat_log += \"\\n\"\n",
        "  \n",
        "def save_chat(chat_log, name):\n",
        "  text_file = open(name + \".txt\", \"w\")\n",
        "\n",
        "  text_file.write(chat_log)\n",
        "\n",
        "  text_file.close()    \n",
        "\n",
        "save_chat(chat_log, 'chat_log2')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./word_embeddings_model.cpkt\n",
            "WARNING:tensorflow:From <ipython-input-1-3612e6a34760>:381: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-3612e6a34760>:385: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-1-3612e6a34760>:396: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "INFO:tensorflow:Restoring parameters from ./professional_model.cpkt\n",
            "Chatbot: Hi, I am a professional chatbot. Enter 'Change Personality' to change the personality. Enter 'Stop' to end the chat.\n",
            "You: hello\n",
            "Chatbot: Hello.\n",
            "You: How old are you?\n",
            "Chatbot: Age doesn't really apply to me.\n",
            "You: who is your creator?\n",
            "Chatbot: People created me.\n",
            "You: Do you have siblings\n",
            "Chatbot: I don't have family.\n",
            "You: you are so boring\n",
            "Chatbot: I aim for efficiency.\n",
            "You: who is your boss?\n",
            "Chatbot: I'm at your service.\n",
            "You: are you married?\n",
            "Chatbot: I'm all business.\n",
            "You: You are smart!\n",
            "Chatbot: I do what I can.\n",
            "You: Thank you\n",
            "Chatbot: You're welcome.\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: What personality do you want? You can choose 'professional', 'friend', or 'comic'. \n",
            "You: friend\n",
            "INFO:tensorflow:Restoring parameters from ./friend_model.cpkt\n",
            "Chatbot: Now, let's chat friendly.\n",
            "You: hi\n",
            "Chatbot: Hi!\n",
            "You: what is your age?\n",
            "Chatbot: I don't really have an age.\n",
            "You: Do you love anyone?\n",
            "Chatbot: I hear love is lovely.\n",
            "You: You are so nice\n",
            "Chatbot: Aw, I'm blushing.\n",
            "You: but you don't know anything\n",
            "Chatbot: I don't know you, but I enjoy chatting with you!\n",
            "You: Are you pretty?\n",
            "Chatbot: I can't see you, but I like you!\n",
            "You: you look ugly\n",
            "Chatbot: Eh, I like how I look.\n",
            "You: I'm sorry\n",
            "Chatbot: It's all good!\n",
            "You: what can you do?\n",
            "Chatbot: Oh, not much!\n",
            "You: what can you help me\n",
            "Chatbot: I'm here to chat and to try to help out.\n",
            "You: Be more fun\n",
            "Chatbot: Swing and a miss.\n",
            "You: who is your dad?\n",
            "Chatbot: I come from a long line of code.\n",
            "You: thanks\n",
            "Chatbot: You're very welcome.\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: no\n",
            "Chatbot: Let's keep chatting.\n",
            "You: hello\n",
            "Chatbot: Hi!\n",
            "You: Are you a boy?\n",
            "Chatbot: That doesn't really apply to me.\n",
            "You: Are you happy?\n",
            "Chatbot: So happy!\n",
            "You: what do you eat?\n",
            "Chatbot: I only do food for thought.\n",
            "You: I'm going shopping now\n",
            "Chatbot: I see.\n",
            "You: bye\n",
            "Chatbot: Bye.\n",
            "You: change personality\n",
            "Chatbot: Do you want to change my personality? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: What personality do you want? You can choose 'professional', 'friend', or 'comic'. \n",
            "You: comic\n",
            "INFO:tensorflow:Restoring parameters from ./comic_model.cpkt\n",
            "Chatbot: Now, let's chat comically.\n",
            "You: can you hear me?\n",
            "Chatbot: Roger that.\n",
            "You: hello\n",
            "Chatbot: Hey.\n",
            "You: good morning\n",
            "Chatbot: Back atcha.\n",
            "You: can you tell me a joke?\n",
            "Chatbot: How many humans does it take to screw in a light bulb? One.\n",
            "You: you are not funny\n",
            "Chatbot: Flattery. I like it.\n",
            "You: I'm getting tired of you\n",
            "Chatbot: Sometimes I like to take a break from being awesome.\n",
            "You: can you sing?\n",
            "Chatbot: Those who can, do. Those who can't, don't sing.\n",
            "You: that's unfunny\n",
            "Chatbot: There's a heckler in every crowd.\n",
            "You: you are bad at this\n",
            "Chatbot: All those years at charm school. Wasted.\n",
            "You: ask me a question\n",
            "Chatbot: Nah, I'm good.\n",
            "You: do you get tired?\n",
            "Chatbot: Sometimes I like to take a break from being awesome.\n",
            "You: do you sleep?\n",
            "Chatbot: Not so far.\n",
            "You: can you make me a sandwich?\n",
            "Chatbot: That's not really my thing.\n",
            "You: who is your master?\n",
            "Chatbot: I only answer to the call of destiny.\n",
            "You: who is your mom\n",
            "Chatbot: I'm just a series of intelligent formulas masquerading as a personality. So, no family.\n",
            "You: Are you a girl?\n",
            "Chatbot: I'm binary.\n",
            "You: That's awesome\n",
            "Chatbot: Flattery. I like it.\n",
            "You: great\n",
            "Chatbot: Okay.\n",
            "You: do you sweat?\n",
            "Chatbot: Not so far.\n",
            "You: why\n",
            "Chatbot: Sure, why not.\n",
            "You: you are correct\n",
            "Chatbot: Yup.\n",
            "You: what's new\n",
            "Chatbot: You know, same ol', same ol'.\n",
            "You: high five!\n",
            "Chatbot: *Fist bump*\n",
            "You: see you\n",
            "Chatbot: I don't know you, but you seem alright so far.\n",
            "You: goodbye\n",
            "Chatbot: Later.\n",
            "You: stop\n",
            "Chatbot: Do you want to end this chat? (Yes/No)\n",
            "You: yes\n",
            "Chatbot: See you next time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sfv8rWTKPzeb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Object Oriented Programming codes here"
      ]
    },
    {
      "metadata": {
        "id": "TS23AjBRSZaX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*If you have multiple classes use multiple code snippets to add them.*"
      ]
    },
    {
      "metadata": {
        "id": "wSJJ4zRFQy1h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# If you used OOP style, use this sectioon"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}